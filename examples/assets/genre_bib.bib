@misc{gu2025surveyllmasajudge,
      title={A Survey on LLM-as-a-Judge}, 
      author={Jiawei Gu and Xuhui Jiang and Zhichao Shi and Hexiang Tan and Xuehao Zhai and Chengjin Xu and Wei Li and Yinghan Shen and Shengjie Ma and Honghao Liu and Saizhuo Wang and Kun Zhang and Yuanzhuo Wang and Wen Gao and Lionel Ni and Jian Guo},
      year={2025},
      eprint={2411.15594},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.15594}, 
}

@misc{zheng2023judgingllmasajudgemtbenchchatbot,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05685}, 
}

@incollection{aaronsNicoleKraussInheriting2017,
  title = {Nicole {{Krauss}}: {{Inheriting}} the {{Burden}} of {{Holocaust Trauma}}},
  shorttitle = {Nicole {{Krauss}}},
  booktitle = {Third-{{Generation Holocaust Representation}}},
  author = {Aarons, Victoria and Berger, Alan L.},
  year = {2017},
  series = {Trauma, {{History}}, and {{Memory}}},
  eprint = {j.ctt22727kb.8},
  eprinttype = {jstor},
  pages = {147--170},
  publisher = {Northwestern University Press},
  urldate = {2021-08-31},
  abstract = {A grandchild of Holocaust survivors and the author of three novels, \emph{Man Walks into a Room}  (2002), \emph{The History of Love}  (2005), and \emph{Great House}  (2010), Nicole Krauss admits that while the Holocaust is a manifest presence in her work, she cannot write her ancestors' stories the way survivors or their children have written about the Shoah. In a recent interview, she takes issue with being labeled a Holocaust writer and maintains that she has ``written very little about the Holocaust in terms of actual events.''{$^1$} Chronologically separated and shielded from the horrors of the historical realities of the Holocaust},
  isbn = {978-0-8101-3410-2}
}

@misc{abassyLLMDetectAIveToolFineGrained2024,
  title = {{{LLM-DetectAIve}}: A {{Tool}} for {{Fine-Grained Machine-Generated Text Detection}}},
  shorttitle = {{{LLM-DetectAIve}}},
  author = {Abassy, Mervat and Elozeiri, Kareem and Aziz, Alexander and Ta, Minh Ngoc and Tomar, Raj Vardhan and Adhikari, Bimarsha and Ahmed, Saad El Dine and Wang, Yuxia and Afzal, Osama Mohammed and Xie, Zhuohan and Mansurov, Jonibek and Artemova, Ekaterina and Mikhailov, Vladislav and Xing, Rui and Geng, Jiahui and Iqbal, Hasan and Mujahid, Zain Muhammad and Mahmoud, Tarek and Tsvigun, Akim and Aji, Alham Fikri and Shelmanov, Artem and Habash, Nizar and Gurevych, Iryna and Nakov, Preslav},
  year = {2024},
  month = oct,
  number = {arXiv:2408.04284},
  eprint = {2408.04284},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.04284},
  urldate = {2025-01-23},
  abstract = {The ease of access to large language models (LLMs) has enabled a widespread of machine-generated texts, and now it is often hard to tell whether a piece of text was human-written or machine-generated. This raises concerns about potential misuse, particularly within educational and academic domains. Thus, it is important to develop practical systems that can automate the process. Here, we present one such system, LLM-DetectAIve, designed for fine-grained detection. Unlike most previous work on machine-generated text detection, which focused on binary classification, LLM-DetectAIve supports four categories: (i) human-written, (ii) machine-generated, (iii) machine-written, then machine-humanized, and (iv) human-written, then machine-polished. Category (iii) aims to detect attempts to obfuscate the fact that a text was machine-generated, while category (iv) looks for cases where the LLM was used to polish a human-written text, which is typically acceptable in academic writing, but not in education. Our experiments show that LLM-DetectAIve can effectively identify the above four categories, which makes it a potentially useful tool in education, academia, and other domains. LLM-DetectAIve is publicly accessible at https://github.com/mbzuai-nlp/LLM-DetectAIve. The video describing our system is available at https://youtu.be/E8eT\_bE7k8c.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\5M4ADTK7\\Abassy et al. - 2024 - LLM-DetectAIve a Tool for Fine-Grained Machine-Generated Text Detection.pdf;C\:\\Users\\spide\\Zotero\\storage\\6RJ7WBSA\\2408.html}
}

@article{abbouFeminisation2021,
  title = {F{\'e}minisation},
  author = {Abbou, Julie and Candea, Maria},
  year = {2021},
  month = sep,
  journal = {Langage et soci{\'e}t{\'e}},
  number = {HS1},
  pages = {141--145},
  issn = {0181-4095},
  doi = {10.3917/ls.hs01.0142},
  urldate = {2023-01-19}
}

@article{abbouLangueEstelleToujours2019,
  title = {La Langue Est-Elle Toujours Un Lieu de Lutte F{\'e}ministe? {{De}} La Contrefa{\c c}on S{\'e}miotique {\`a} La Lib{\'e}ralisation},
  author = {Abbou, Julie},
  year = {2019},
  journal = {Recherches f{\'e}ministes},
  volume = {32},
  number = {2},
  pages = {235--258},
  urldate = {2023-01-19}
}

@article{abbouQuiPeurLecriture2018,
  title = {Qui a Peur de l'{\'e}criture Inclusive ? {{Entre}} D{\'e}lire Eschatologique et Peur d'{\'e}masculation},
  shorttitle = {Qui a Peur de l'{\'e}criture Inclusive ?},
  author = {Abbou, Julie and Arnold, Aron and Candea, Maria and Marignier, No{\'e}mie},
  year = {2018},
  month = may,
  journal = {Semen},
  number = {44},
  issn = {0761-2990, 1957-780X},
  doi = {10.4000/semen.10800},
  urldate = {2023-03-26}
}

@article{abeilleLaccordProximiteDeterminant2018,
  title = {L'accord de Proximit{\'e} Du D{\'e}terminant En Fran{\c c}ais},
  author = {Abeill{\'e}, Anne and An, Aixiu and Shira{\"i}shi, Aoi},
  year = {2018},
  month = oct,
  journal = {Discours},
  number = {22},
  issn = {1963-1723},
  doi = {10.4000/discours.9542},
  urldate = {2023-01-19},
  abstract = {Contemporary French is often considered to have lost closest conjunct agreement, contrary to other Romance languages (Corbett, 1991), hence recent debates about French feminine visibility (Michel, 2016; Viennot, 2017). We consider the coordination of determiners and nouns in the Noun Phrase with diferent number or gender in large written corpora (Frantext, FrWaC). We show that closest conjunct agreement is alive and even favoured in some cases. We conclude by some remarks on gender agreement and word order.}
}

@article{abercrombieAlexaGoogleSiri2021,
  title = {Alexa, {{Google}}, {{Siri}}: {{What}} Are {{Your Pronouns}}? {{Gender}} and {{Anthropomorphism}} in the {{Design}} and {{Perception}} of {{Conversational Assistants}}},
  shorttitle = {Alexa, {{Google}}, {{Siri}}},
  author = {Abercrombie, Gavin and Curry, Amanda Cercas and Pandya, Mugdha and Rieser, Verena},
  year = {2021},
  month = jun,
  eprint = {2106.02578},
  primaryclass = {cs},
  pages = {10},
  urldate = {2022-08-10},
  abstract = {Technology companies have produced varied responses to concerns about the effects of the design of their conversational AI systems. Some have claimed that their voice assistants are in fact not gendered or human-like---despite design features suggesting the contrary. We compare these claims to user perceptions by analysing the pronouns they use when referring to AI assistants. We also examine systems' responses and the extent to which they generate output which is gendered and anthropomorphic. We find that, while some companies appear to be addressing the ethical concerns raised, in some cases, their claims do not seem to hold true. In particular, our results show that system outputs are ambiguous as to the humanness of the systems, and that users tend to personify and gender them as a result.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\RDWHQQLV\Alexa, Google, Siri What are Your Pronouns Gender and Anthropomorphism in the Design and Perception of Conversational Assistants (Abercrombie et al., 2021).pdf}
}

@misc{academiefrancaiseDeclarationLAcademieFrancaise2017,
  title = {D{\'e}claration de l'{{Acad{\'e}mie}} Fran{\c c}aise Sur l'{\'e}criture Dite "Inclusive"},
  author = {{Acad{\'e}mie fran{\c c}aise}},
  year = {2017},
  month = oct,
  urldate = {2023-02-03}
}

@article{adamVariationsEnonciativesAspects1995,
  title = {{Variations {\'e}nonciatives. Aspects de la gen{\`e}se du style de l'{\'E}tranger}},
  author = {Adam, Jean-Michel and No{\"e}l, Mireille},
  year = {1995},
  journal = {Langages},
  volume = {29},
  number = {118},
  pages = {64--84},
  issn = {0458-726X},
  doi = {10.3406/lgge.1995.1715},
  urldate = {2022-08-09},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\2P5XXIEC\Variations énonciatives. Aspects de la genèse du style de l'Étranger (Adam and Noël, 1995).pdf}
}

@inproceedings{agarwalPEFTDebiasCapturingDebiasing2023,
  title = {{{PEFTDebias}} : {{Capturing}} Debiasing Information Using {{PEFTs}}},
  shorttitle = {{{PEFTDebias}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Agarwal, Sumit and Veerubhotla, Aditya and Bansal, Srijan},
  year = {2023},
  pages = {1992--2000},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.122},
  urldate = {2024-12-17},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\Y8UPQY4J\Agarwal et al. - 2023 - PEFTDebias  Capturing debiasing information using PEFTs.pdf}
}

@misc{alhafniArabicParallelGender2021,
  title = {The {{Arabic Parallel Gender Corpus}} 2.0: {{Extensions}} and {{Analyses}}},
  shorttitle = {The {{Arabic Parallel Gender Corpus}} 2.0},
  author = {Alhafni, Bashar and Habash, Nizar and Bouamor, Houda},
  year = {2021},
  month = oct,
  eprint = {2110.09216},
  urldate = {2023-02-24},
  abstract = {Gender bias in natural language processing (NLP) applications, particularly machine translation, has been receiving increasing attention. Much of the research on this issue has focused on mitigating gender bias in English NLP models and systems. Addressing the problem in poorly resourced, and/or morphologically rich languages has lagged behind, largely due to the lack of datasets and resources. In this paper, we introduce a new corpus for gender identification and rewriting in contexts involving one or two target users (I and/or You) -- first and second grammatical persons with independent grammatical gender preferences. We focus on Arabic, a gender-marking morphologically rich language. The corpus has multiple parallel components: four combinations of 1st and 2nd person in feminine and masculine grammatical genders, as well as English, and English to Arabic machine translation output. This corpus expands on Habash et al. (2019)'s Arabic Parallel Gender Corpus (APGC v1.0) by adding second person targets as well as increasing the total number of sentences over 6.5 times, reaching over 590K words. Our new dataset will aid the research and development of gender identification, controlled text generation, and post-editing rewrite systems that could be used to personalize NLP applications and provide users with the correct outputs based on their grammatical gender preferences. We make the Arabic Parallel Gender Corpus (APGC v2.0) publicly available.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2110.09216},
  keywords = {Computer Science - Computation and Language}
}

@misc{alhafniSharedTaskGender2022,
  title = {The {{Shared Task}} on {{Gender Rewriting}}},
  author = {Alhafni, Bashar and Habash, Nizar and Bouamor, Houda and Obeid, Ossama and Alrowili, Sultan and Alzeer, Daliyah and Alshanqiti, Khawlah M. and ElBakry, Ahmed and ElNokrashy, Muhammad and Gabr, Mohamed and Issam, Abderrahmane and Qaddoumi, Abdelrahim and {Vijay-Shanker}, K. and Zyate, Mahmoud},
  year = {2022},
  eprint = {2210.12410},
  doi = {10.48550/arXiv.2210.12410},
  urldate = {2022-11-23},
  abstract = {In this paper, we present the results and findings of the Shared Task on Gender Rewriting, which was organized as part of the Seventh Arabic Natural Language Processing Workshop. The task of gender rewriting refers to generating alternatives of a given sentence to match different target user gender contexts (e.g., female speaker with a male listener, a male speaker with a male listener, etc.). This requires changing the grammatical gender (masculine or feminine) of certain words referring to the users. In this task, we focus on Arabic, a gender-marking morphologically rich language. A total of five teams from four countries participated in the shared task.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2210.12410},
  keywords = {gender rewriting,lu}
}

@inproceedings{alhafniUserCentricGenderRewriting2022,
  title = {User-{{Centric Gender Rewriting}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Alhafni, Bashar and Habash, Nizar and Bouamor, Houda},
  year = {2022},
  pages = {618--631},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.46},
  urldate = {2022-10-16},
  abstract = {In this paper, we define the task of gender rewriting in contexts involving two users (I and/or You) -- first and second grammatical persons with independent grammatical gender preferences. We focus on Arabic, a gender-marking morphologically rich language. We develop a multi-step system that combines the positive aspects of both rule-based and neural rewriting models. Our results successfully demonstrate the viability of this approach on a recently created corpus for Arabic gender rewriting, achieving 88.42 M2 F0.5 on a blind test set. Our proposed system improves over previous work on the first-person-only version of this task, by 3.05 absolute increase in M2 F0.5. We demonstrate a use case of our gender rewriting system by using it to post-edit the output of a commercial MT system to provide personalized outputs based on the users' grammatical gender preferences. We make our code, data, and pretrained models publicly available.},
  keywords = {gender rewriting,lu}
}

@inproceedings{alihosseiniJointlyMeasuringDiversity2019,
  title = {Jointly {{Measuring Diversity}} and {{Quality}} in {{Text Generation Models}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Methods}} for {{Optimizing}} and {{Evaluating Neural Language Generation}}},
  author = {Alihosseini, Danial and Montahaei, Ehsan and Soleymani Baghshah, Mahdieh},
  year = {2019},
  pages = {90--98},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/W19-2311},
  urldate = {2024-03-18},
  abstract = {Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglect their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generators density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.}
}

@article{allassonniere-tangTypologyClassifiersGender2019,
  title = {A Typology of Classifiers and Gender: {{From}} Description to Computation},
  author = {{Allassonni{\`e}re-Tang}, Marc},
  year = {2019},
  pages = {79},
  abstract = {Tang, M. 2019. A typology of classifiers and gender. From description to computation. Studia Linguistica Upsaliensia 23. 78 pp. Uppsala: Acta Universitatis Upsaliensis. ISBN 978-91-513-0507-3.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\6QG5SYB3\Allassonnière-Tang - A typology of classifiers and gender From description to computation.pdf}
}

@article{alpheratzFrancaisInclusifConceptualisation2018,
  title = {{Fran{\c c}ais inclusif : conceptualisation et analyse linguistique}},
  shorttitle = {{Fran{\c c}ais inclusif}},
  author = {{Alpheratz}},
  editor = {Neveu, F. and Harmegnies, B. and Hriba, L. and Pr{\'e}vost, S.},
  year = {2018},
  journal = {SHS Web of Conferences},
  volume = {46},
  pages = {13003},
  issn = {2261-2424},
  doi = {10.1051/shsconf/20184613003},
  urldate = {2022-08-12},
  abstract = {Inclusive French : Conceptualization And Linguistic Analysis. This research paper summarises the conceptualization of ``inclusive French'' as a set of variations of standard French based on gender and used by several social groups which have in common a gender consciousness and/or politics. Specific to the 21st Century, this phenomenon has been producing observables that linguistics can see as an object of study. Its description and its conceptualization forced us to define the philosophical context and the epistemic community that contributed to its emergence and to design the concepts and the metalinguistic terms for its analysis. ``The diaethical variation, the epicenization, the inclusive writing'' and ``the gender hyperonymization'' are linguistic processes and resources that allow to study these new usages. Some of them can fall under a grammatical category which has disappeared in French : the neutral gender. We will see what are the prospects of its reactivation and how, perhaps, this grammatical category can satisfy a need that standard French doesn't manage to fulfil. Finally, we will introduce our linguistic experimentation on the neutral gender that will allow us to validate or invalidate the hypothesis of its existence and its usefulness.},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\CYS3GJY6\Français inclusif  conceptualisation et analyse linguistique (Alpheratz, 2018).pdf}
}

@inproceedings{alrowiliGenerativeApproachGender2022,
  title = {Generative {{Approach}} for {{Gender Rewriting Task}} with {{ArabicT5}}},
  booktitle = {Proceedings of the {{The Seventh Arabic Natural Language Processing Workshop}} ({{WANLP}})},
  author = {Alrowili, Sultan and {Vijay-Shanker}, K},
  year = {2022},
  pages = {491--495},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  urldate = {2023-02-21},
  abstract = {Addressing the correct gender in generative tasks (e.g., Machine Translation) has been an overlooked issue in the Arabic NLP. However, the recent introduction of the Arabic Parallel Gender Corpus (APGC) dataset has established new baselines for the Arabic Gender Rewriting task. To address the Gender Rewriting task, we first pre-train our new Seq2Seq ArabicT5 model on a 17GB of Arabic Corpora. Then, we continue pre-training our ArabicT5 model on the APGC dataset using a newly proposed method. Our evaluation shows that our ArabicT5 model, when trained on the APGC dataset, achieved competitive results against existing state-of-the-art methods. In addition, our ArabicT5 model shows better results on the APGC dataset compared to other Arabic and multilingual T5 models.}
}

@misc{alvesTowerOpenMultilingual2024,
  title = {Tower: {{An Open Multilingual Large Language Model}} for {{Translation-Related Tasks}}},
  shorttitle = {Tower},
  author = {Alves, Duarte M. and Pombal, Jos{\'e} and Guerreiro, Nuno M. and Martins, Pedro H. and Alves, Jo{\~a}o and Farajian, Amin and Peters, Ben and Rei, Ricardo and Fernandes, Patrick and Agrawal, Sweta and Colombo, Pierre and de Souza, Jos{\'e} G. C. and Martins, Andr{\'e} F. T.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.17733},
  eprint = {2402.17733},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.17733},
  urldate = {2025-01-09},
  abstract = {While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task. In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows. We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct. Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed LLMs. To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations, including ours, on our benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\NS77XLLI\\Alves et al. - 2024 - Tower An Open Multilingual Large Language Model for Translation-Related Tasks.pdf;C\:\\Users\\spide\\Zotero\\storage\\ZGEMCI2E\\2402.html}
}

@misc{amatriainPromptDesignEngineering2024,
  title = {Prompt {{Design}} and {{Engineering}}: {{Introduction}} and {{Advanced Methods}}},
  shorttitle = {Prompt {{Design}} and {{Engineering}}},
  author = {Amatriain, Xavier},
  year = {2024},
  month = feb,
  eprint = {2401.14423},
  urldate = {2024-03-18},
  abstract = {Prompt design and engineering has rapidly become essential for maximizing the potential of large language models. In this paper, we introduce core concepts, advanced techniques like Chain-ofThought and Reflection, and the principles behind building LLM-based agents. Finally, we provide a survey of tools for prompt engineers.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2401.14423},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering}
}

@article{andersonCasePivotAsia2017,
  title = {The {{Case}} of the {{Pivot}} to {{Asia}}: {{System Effects}} and the {{Origins}} of {{Strategy}}: {{THE PIVOT TO ASIA}}},
  shorttitle = {The {{Case}} of the {{Pivot}} to {{Asia}}},
  author = {Anderson, Nicholas D. and Cha, Victor D.},
  year = {2017},
  month = dec,
  journal = {Political Science Quarterly},
  volume = {132},
  number = {4},
  pages = {23},
  issn = {00323195},
  doi = {10.1002/polq.12703},
  urldate = {2021-12-17},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\CBMFDM96\The Case of the Pivot to Asia System Effects and the Origins of Strategy THE PIVOT TO ASIA (Anderson and Cha, 2017).pdf}
}

@book{anscombre2009topoï,
  title = {Des Topo{\"i} {\`a} La Th{\'e}orie Des St{\'e}r{\'e}otypes En Passant Par La Polyphonie et l'argumentation Dans La Langue: Hommages {\`a} {{Jean-Claude Anscombre}}},
  author = {Anscombre, J.C. and Leeman, D.},
  year = {2009},
  series = {Laboratoire Langages, Litt{\'e}ratures, Soci{\'e}t{\'e}s. {{Collection}} Langages},
  publisher = {Universit{\'e} de Savoie},
  isbn = {978-2-915797-47-3}
}

@misc{anthropicClaude3Model2024,
  title = {The {{Claude}} 3 {{Model Family}}: {{Opus}}, {{Sonnet}}, {{Haiku}}},
  author = {{Anthropic}},
  year = {2024},
  month = mar,
  urldate = {2024-03-18}
}

@article{arigneFigureToutIntegre2011,
  title = {La Figure Du Tout Int{\'e}gr{\'e} et Les Noms Discrets Collectifs},
  author = {Arigne, Viviane},
  year = {2011},
  journal = {Anglophonia},
  number = {15 (30)},
  pages = {59--99},
  issn = {1278-3331, 2427-0466},
  doi = {10.4000/anglophonia.389},
  urldate = {2022-11-13},
  abstract = {This article assesses the role of the model of the integrated whole in the linguistics of collective nouns and, more particularly, collective discrete nouns. The cognitive dimension of the integrated whole whose prototype is given by the discrete noun body is twofold and concerns both metalinguistic and linguistic referential constructions. A number of features are isolated in the prototypical integrated whole, such as meronomical features, tri-dimensional object-reference, tokenreference as well as position in the hierarchy of integrated wholes. Collective discrete nouns are analysed along the same lines as less prototypical integrated wholes. Isomorphism between a discrete whole and similarly discrete homeomerous parts accounts for the fascination collective discrete nouns hold for scholars, be it in English or French linguistics. The features pertaining to prototypical or collective integrated wholes help explain how collective mass-nouns are very frequently excluded from the class of collective nouns and how a number of collective discrete nouns are hardly ever mentioned in linguistic or grammatical works. Lastly, the noun body is analysed as a semantically derived collective noun: the prototypical integrated whole is used as a collective, less prototypical, integrated whole to unite and interlock the most independent discrete entities, human animates.},
  keywords = {ling. angl.,lu,noms collectifs}
}

@article{armandrenaudQuelquesRemarquesStyle2022,
  title = {{Quelques remarques sur le style de l'Etranger}},
  author = {{Armand Renaud}},
  year = {2022},
  pages = {8},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\CMVBRTMY\Quelques remarques sur le style de l'Etranger (Armand Renaud, 2022).pdf}
}

@article{arnolde.wayneHermanMelvillesTypee,
  title = {Herman {{Melvilles Typee}}: {{The Freedoms}} of the {{Savage}}},
  author = {{Arnold E. Wayne}},
  pages = {31},
  langid = {english},
  keywords = {melville},
  file = {C:\Users\spide\Documents\COURS S4\- APPROFONDISSEMENT\Littérature\PDF Recherches\Herman Melvilles Typee The Freedoms of the Savage (Arnold E. Wayne, ).pdf}
}

@misc{aroraAskMeAnything2022,
  title = {Ask {{Me Anything}}: {{A Simple Strategy}} for {{Prompting Language Models}}},
  shorttitle = {Ask {{Me Anything}}},
  author = {Arora, Simran and Narayan, Avanika and Chen, Mayee F. and Orr, Laurel and Guha, Neel and Bhatia, Kush and Chami, Ines and Sala, Frederic and R{\'e}, Christopher},
  year = {2022},
  month = nov,
  eprint = {2210.02441},
  urldate = {2024-03-18},
  abstract = {Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly perfect prompt for a task. To mitigate the high degree of effort involved in prompting, we instead ask whether collecting multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING PROMPTING (AMA). We first develop an understanding of the effective prompt formats, finding question-answering (QA) prompts, which encourage open-ended generation (``Who went to the park?'') tend to outperform those that restrict the model outputs (``John went to the park. Output True or False''). Our approach recursively uses the LLM to transform task inputs to the effective QA format. We apply these prompts to collect several noisy votes for the input's true label. We find that these prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions. We evaluate AMA across open-source model families (EleutherAI, BLOOM, OPT, and T0) and sizes (125M-175B parameters), demonstrating an average performance lift of 10.2\% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code for reproducing the results here: https://github.com/HazyResearch/ama\_prompting.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2210.02441},
  keywords = {Computer Science - Computation and Language}
}

@article{arstilaTimeSlowsAccidents2012,
  title = {Time {{Slows Down}} during {{Accidents}}},
  author = {Arstila, Valtteri},
  year = {2012},
  journal = {Frontiers in Psychology},
  volume = {3},
  pages = {10},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2012.00196},
  urldate = {2022-11-05},
  abstract = {The experienced speed of the passage of time is not constant as time can seem to fly or slow down depending on the circumstances we are in. Anecdotally accidents and other frightening events are extreme examples of the latter; people who have survived accidents often report altered phenomenology including how everything appeared to happen in slow motion. While the experienced phenomenology has been investigated, there are no explanations about how one can have these experiences. Instead, the only recently discussed explanation suggests that the anecdotal phenomenology is due to memory effects and hence not really experienced during the accidents. The purpose of this article is (i) to reintroduce the currently forgotten comprehensively altered phenomenology that some people experience during the accidents, (ii) to explain why the recent experiments fail to address the issue at hand, and (iii) to suggest a new framework to explain what happens when people report having experiences of time slowing down in these cases. According to the suggested framework, our cognitive processes become rapidly enhanced. As a result, the relation between the temporal properties of events in the external world and in internal states becomes distorted with the consequence of external world appearing to slow down. That is, the presented solution is a realist one in a sense that it maintains that sometimes people really do have experiences of time slowing down.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\SG3K8GC5\Time Slows Down during Accidents (Arstila, 2012).pdf}
}

@misc{arzaghiUnderstandingIntrinsicSocioeconomic2024,
  title = {Understanding {{Intrinsic Socioeconomic Biases}} in {{Large Language Models}}},
  author = {Arzaghi, Mina and Carichon, Florian and Farnadi, Golnoosh},
  year = {2024},
  month = may,
  number = {arXiv:2405.18662},
  eprint = {2405.18662},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.18662},
  urldate = {2024-12-17},
  abstract = {Large Language Models (LLMs) are increasingly integrated into critical decision-making processes, such as loan approvals and visa applications, where inherent biases can lead to discriminatory outcomes. In this paper, we examine the nuanced relationship between demographic attributes and socioeconomic biases in LLMs, a crucial yet understudied area of fairness in LLMs. We introduce a novel dataset of one million English sentences to systematically quantify socioeconomic biases across various demographic groups. Our findings reveal pervasive socioeconomic biases in both established models such as GPT-2 and state-of-the-art models like Llama 2 and Falcon. We demonstrate that these biases are significantly amplified when considering intersectionality, with LLMs exhibiting a remarkable capacity to extract multiple demographic attributes from names and then correlate them with specific socioeconomic biases. This research highlights the urgent necessity for proactive and robust bias mitigation techniques to safeguard against discriminatory outcomes when deploying these powerful models in critical real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {C\:\\Users\\spide\\Zotero\\storage\\SUZ9QWP6\\Arzaghi et al. - 2024 - Understanding Intrinsic Socioeconomic Biases in Large Language Models.pdf;C\:\\Users\\spide\\Zotero\\storage\\I5RJBDI8\\2405.html}
}

@article{attalLagrammaticaliteSyntaxeSaintSimon2000,
  title = {{L'agrammaticalit{\'e} et la syntaxe de Saint-Simon}},
  author = {Attal, Pierre},
  year = {2000},
  journal = {L Information Grammaticale},
  volume = {84},
  number = {1},
  pages = {6},
  issn = {0222-9838},
  doi = {10.3406/igram.2000.3594},
  urldate = {2022-08-18},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\H4BS9SC6\L'agrammaticalité et la syntaxe de Saint-Simon (Attal, 2000).pdf}
}

@article{bachaLocutionsGrandMonde2007,
  title = {{Les locutions \emph{grand monde} et \emph{grand-chose} : Un cas de grammaticalisation ?}},
  shorttitle = {{Les locutions \emph{grand monde} et \emph{grand-chose}}},
  author = {Bacha, Jacqueline},
  year = {2007},
  journal = {L'Information Grammaticale},
  volume = {113},
  number = {1},
  pages = {5},
  issn = {0222-9838},
  doi = {10.3406/igram.2007.3881},
  urldate = {2022-07-24},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\M43YKE3N\Les locutions grand monde et grand-chose  Un cas de grammaticalisation  (Bacha, 2007).pdf}
}

@inproceedings{barberisQuandTesSuper2010,
  title = {{<< Quand t'es super bobo >>{\dots} La deuxi{\`e}me personne g{\'e}n{\'e}rique dans le fran{\c c}ais parisien des jeunes}},
  booktitle = {{2{\`e}me Congr{\`e}s Mondial de Linguistique Fran{\c c}aise}},
  author = {Barb{\'e}ris, Jeanne-Marie},
  year = {2010},
  pages = {19},
  publisher = {EDP Sciences},
  address = {La Nouvelle-Orl{\'e}ans, Etats-Unis},
  doi = {10.1051/cmlf/2010258},
  urldate = {2022-11-03},
  isbn = {978-2-7598-0534-1 978-2-7598-0533-4},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\5XNQHXR8\« Quand t’es super bobo »… La deuxième personne générique dans le français parisien des jeunes (Barbéris, 2010).pdf}
}

@article{BattlingWomen,
  title = {Battling {{Women}}},
  pages = {13},
  keywords = {civi. women},
  annotation = {WOMEN},
  file = {C:\Users\spide\Zotero\storage\I43CQ2LU\SS5 Battling Women.pdf}
}

@article{bejanWhyDaysSeem2019,
  title = {Why the {{Days Seem Shorter}} as {{We Get Older}}},
  author = {Bejan, Adrian},
  year = {2019},
  month = may,
  journal = {European Review},
  volume = {27},
  number = {02},
  pages = {8},
  issn = {1062-7987, 1474-0575},
  doi = {10.1017/S1062798718000741},
  urldate = {2022-07-24},
  abstract = {Why does it feel that the time passes faster as we get older? What is the physical basis for the impression that some days are slower than others? Why do we tend to focus on the unusual (the surprise), not on the ever present? This article unveils the physics basis for these common observations. The reason is that the measurable `clock time' is not the same as the time perceived by the human mind. The `mind time' is a sequence of images, i.e. reflections of nature that are fed by stimuli from sensory organs. The rate at which changes in mental images are perceived decreases with age, because of several physical features that change with age: saccades frequency, body size, pathways degradation, etc. The misalignment between mental-image time and clock time serves to unite the voluminous observations of this phenomenon in the literature with the constructal law of evolution of flow architecture, as physics.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\HDSGEHHE\Why the Days Seem Shorter as We Get Older (Bejan, 2019).pdf}
}

@inproceedings{bellaTacklingLanguageModelling2024,
  title = {Tackling {{Language Modelling Bias}} in {{Support}} of {{Linguistic Diversity}}},
  booktitle = {The 2024 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bella, G{\'a}bor and Helm, Paula and Koch, Gertraud and Giunchiglia, Fausto},
  year = {2024},
  month = jun,
  pages = {562--572},
  publisher = {ACM},
  address = {Rio de Janeiro Brazil},
  doi = {10.1145/3630106.3658925},
  urldate = {2024-12-17},
  isbn = {979-8-4007-0450-5},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\JAKHCAN6\Bella et al. - 2024 - Tackling Language Modelling Bias in Support of Linguistic Diversity.pdf}
}

@article{benningerMeuteLoupsBrassee2001,
  title = {Une Meute de Loups / Une Brass{\'e}e de Questions : Collection, Quantification et M{\'e}taphore},
  shorttitle = {Une Meute de Loups / Une Brass{\'e}e de Questions},
  author = {Benninger, C{\'e}line},
  year = {2001},
  journal = {Langue fran{\c c}aise},
  volume = {129},
  number = {1},
  pages = {21--34},
  issn = {0023-8368},
  doi = {10.3406/lfr.2001.1015},
  urldate = {2023-02-05},
  keywords = {a lire}
}

@article{benzimanLawLawSuccess2005,
  title = {Law, and the {{Law}} of {{Success}}: {{Reevaluating}} "{{Death}} of a {{Salesman}}'s" {{Treatment}} of the {{American Dream}}},
  author = {Benziman, Galia},
  year = 2005,
  journal = {South Atlantic Modern Language Association},
  volume = {70},
  number = {2},
  pages = {22},
  keywords = {miller},
  file = {C:\Users\spide\Documents\PDF recherches\Law, and the Law of Success Reevaluating Death of a Salesman's Treatment of the American Dream (Benziman, 2005).pdf}
}

@article{bercovitchAPoliticsAmbiguityScarlet1988,
  title = {The {{A-Politics}} of {{Ambiguity}} in {{The Scarlet Letter}}},
  author = {Bercovitch, Sacvan},
  year = {1988},
  journal = {New Literary History},
  volume = {19},
  number = {3},
  eprint = {469093},
  eprinttype = {jstor},
  pages = {15},
  issn = {0028-6087},
  doi = {10.2307/469093},
  urldate = {2021-08-21},
  keywords = {hawthorne},
  annotation = {SCARLET LETTER},
  file = {C:\Users\spide\Zotero\storage\9G7RV39U\The A-Politics of Ambiguity in The Scarlet Letter (Bercovitch, 1988).pdf}
}

@book{berlinBasicColorTerms1969,
  title = {Basic {{Color Terms}}: {{Their Universality}} and {{Evolution}}},
  author = {Berlin, Brent and Kay, Paul},
  year = {1969},
  publisher = {University of California Press},
  isbn = {1-57586-162-3}
}

@article{bernardMAKINGRELIGIOUSPOLICY1998,
  title = {{{THE MAKING OF RELIGIOUS POLICY}}, 1533--1546: {{HENRY VIII AND THE SEARCH FOR THE MIDDLE WAY}}},
  shorttitle = {{{THE MAKING OF RELIGIOUS POLICY}}, 1533--1546},
  author = {Bernard, G. W.},
  year = {1998},
  month = jun,
  journal = {The Historical Journal},
  volume = {41},
  number = {2},
  pages = {30},
  issn = {0018-246X, 1469-5103},
  doi = {10.1017/S0018246X98007778},
  urldate = {2022-04-20},
  abstract = {Too often r eligious policy in Henr ViII's reign after the break wit seen as fluctuating and inconsistent as he was influenced first bj! one group of mini and then bj! another. Here it is aigued bj! contrast that Henr VIII was verj much th in the making of religious policy and that his polic), which he pursued skilfully an best characterized as a search for the middle wa).},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\PQ5DSI4Q\THE MAKING OF RELIGIOUS POLICY, 1533–1546 HENRY VIII AND THE SEARCH FOR THE MIDDLE WAY (Bernard, 1998).pdf}
}

@article{bernhardAudelaNormesIdentifier2024,
  title = {{Au-del{\`a} des normes : identifier et documenter les langues minoris{\'e}es pour le traitement automatique des langues}},
  shorttitle = {{Au-del{\`a} des normes}},
  author = {Bernhard, Delphine and {Vergez-Couret}, Marianne and Dupuy, Est{\`e}le},
  year = {2024},
  month = dec,
  journal = {Cahiers du plurilinguisme europ{\'e}en},
  number = {16},
  issn = {2970-989X},
  doi = {10.57086/cpe.1710},
  urldate = {2024-12-19},
  abstract = {Cet article propose une r{\'e}flexion sur les d{\'e}fis de la documentation des langues minoris{\'e}es dans l'espace num{\'e}rique {\`a} partir des travaux r{\'e}alis{\'e}s dans le cadre du projet DIVITAL. Les premiers travaux du projet ont concern{\'e} la collecte de corpus et leur documentation par des m{\'e}tadonn{\'e}es {\`a} grain fin. Ces travaux ont mis en {\'e}vidence deux d{\'e}fis majeurs~: (i)~l'identification des langues et de leurs variantes, dans le cadre des normes de codification des noms de langues, et (ii)~la cr{\'e}ation de nouvelles ressources en lien avec les pratiques actuelles de ces langues.           ,              This article looks at the challenges of documenting minority languages in the digital environment, based on work carried out as part of the DIVITAL project. The project's initial work involved collecting corpora and documenting them using fine-grained metadata. This work has highlighted two major challenges: (i)~the identification of languages and their variants, within the framework of standards for the codification of language names, and (ii)~the creation of new resources linked to the current practices of these languages.           ,              Dieser Artikel stellt {\"U}berlegungen zu den Herausforderungen der Dokumentation von Minderheitensprachen im digitalen Raum an, ausgehend von den Arbeiten, die im Rahmen des DIVITAL-Projekts durchgef{\"u}hrt wurden. Die ersten Arbeiten des Projekts betrafen die Sammlung von Korpora und ihre Dokumentation durch feink{\"o}rnige Metadaten. Diese Arbeiten haben zwei gro{\ss}e Herausforderungen aufgezeigt: (i)~die Identifizierung der Sprachen und ihrer Varianten im Rahmen der Normen f{\"u}r die Kodierung von Sprachnamen und (ii)~die Schaffung neuer Ressourcen in Verbindung mit der aktuellen Praxis dieser Sprachen.},
  copyright = {https://creativecommons.org/licenses/by-sa/4.0},
  langid = {french}
}

@article{berryFictionalEncountersFictions1998,
  title = {Fictional {{Encounters}} with {{Fiction}}'s {{Others}}},
  author = {Berry, Ellen},
  editor = {Schwab, Gabriele},
  year = {1998},
  journal = {NOVEL: A Forum on Fiction},
  volume = {31},
  number = {2},
  eprint = {1346209},
  eprinttype = {jstor},
  pages = {278--280},
  publisher = {Duke University Press},
  issn = {0029-5132},
  doi = {10.2307/1346209},
  urldate = {2022-02-05}
}

@article{bertholdPortentousSomethingsMelvilles2021,
  title = {"{{Portentous Somethings}}": {{Melville}}'s {{Typee}} and the {{Language}} of {{Captivity}}},
  author = {Berthold, Michael C},
  year = {2021},
  journal = {THE NEW ENGLAND QUARTERLY},
  pages = {20},
  langid = {english},
  keywords = {melville},
  file = {C:\Users\spide\Zotero\storage\4NVDFLFZ\Portentous Somethings Melville's Typee and the Language of Captivity (Berthold, 2021).pdf}
}

@phdthesis{bjorkRelativizingLinguisticRelativity2008,
  title = {Relativizing {{Linguistic Relativity}}. {{Investigating Underlying Assumptions}} about {{Language}} in the {{Neo-Whorfian Literature}}},
  author = {Bj{\"o}rk, Ingrid},
  year = {2008},
  urldate = {2023-03-25},
  school = {Uppsala Universitet}
}

@article{blevinsLexicalistAnalysisGerundive,
  title = {A {{Lexicalist}} Analysis of Gerundive {{Nominals}} in {{English}}},
  author = {Blevins, James P},
  pages = {40},
  abstract = {Gerundive nominals in English are true categorial hybrids, combining a verbal complementation pattern with a characteristically nominal distribution and a clausal interpretation. Initial generative descriptions of gerundives attempt to resolve this conflict by assimilating these constructions to unambiguously nominal or clausal categories, or by assigning the discordant properties to different derivational levels. The alternative articulated here follows Schachter 1976 and Jackendoff 1977 in associating these conflicting properties with different projections in a phrase structure analysis. Gerundives are analyzed as nominal phrases headed by present participles, which, as verbal categories, maintain a predominantly verbal argument structure and interpretation. Following Pullum 1991, the clash between the category of a gerundive phrase and its lexical head is not simply stipulated, but rather attributed to the default projection of constitutive 'head' features, as proposed in extended phrase structure systems. The present account however contrasts with Pullum 1991 in proposing a uniform treatment of 'POSS-ing' and 'ACC-ing' gerundives. Both constructions are assigned analyses, which preserve bar-level succession, and in which NP specifiers, whether possessive or accusative, function as true syntactic subjects. The formal difference between POSS-ing and ACC-ing gerundives is then keyed to whether they are sanctioned by single or multiple phrase structure rules.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\KMR3WZX9\A Lexicalist analysis of gerundive Nominals in English (Blevins, ).pdf}
}

@misc{blodgettRacialDisparityNatural2017,
  title = {Racial {{Disparity}} in {{Natural Language Processing}}: {{A Case Study}} of {{Social Media African-American English}}},
  shorttitle = {Racial {{Disparity}} in {{Natural Language Processing}}},
  author = {Blodgett, Su Lin and O'Connor, Brendan},
  year = {2017},
  month = jun,
  number = {arXiv:1707.00061},
  eprint = {1707.00061},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.00061},
  urldate = {2024-12-17},
  abstract = {We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {C\:\\Users\\spide\\Zotero\\storage\\47UE5ZNV\\Blodgett and O'Connor - 2017 - Racial Disparity in Natural Language Processing A Case Study of Social Media African-American Engli.pdf;C\:\\Users\\spide\\Zotero\\storage\\SHH5PTPL\\1707.html}
}

@misc{boisnardThealtresProjectAnnotations2024,
  title = {{Thealtres project annotations: Comparing theater in Alsatian with the dramatic traditions at its source. Version 1.0.}},
  shorttitle = {{Thealtres project annotations}},
  author = {Boisnard, Fanny and Schneider, Alexia and Pablo Ruiz Fabo and Nugues, Lara and Krautter, Benjamin},
  year = {2024},
  month = jan,
  publisher = {[object Object]},
  doi = {10.5281/ZENODO.10520030},
  urldate = {2024-04-28},
  abstract = {Thealtres project data produced in 2023: Alsatian, German and French popular theater annotations for character social variables and plays settings, and plays' bibliographic metadata. As described in the README, many of the Alsatian data come from our earlier MeThAL project. German and French data are all new.},
  copyright = {Creative Commons Attribution 4.0 International},
  langid = {french},
  keywords = {Alsatian theater,Computational Literary Studies,French theater,German theater,metadata,Posse,Schwank,vaudeville}
}

@article{bojanowski2016enriching,
  title = {Enriching Word Vectors with Subword Information},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2016},
  journal = {arXiv preprint arXiv:1607.04606},
  eprint = {1607.04606},
  archiveprefix = {arXiv}
}

@inproceedings{bolukbasiManComputerProgrammer2016,
  title = {Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings},
  booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
  year = {2016},
  series = {{{NIPS}}'16},
  pages = {4356--4364},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
  isbn = {978-1-5108-3881-9}
}

@article{bostromReversalTestEliminating2006,
  title = {The {{Reversal Test}}: {{Eliminating Status Quo Bias}} in {{Applied Ethics}}},
  shorttitle = {The {{Reversal Test}}},
  author = {Bostrom, Nick and Ord, Toby},
  year = {2006},
  month = jul,
  journal = {Ethics},
  volume = {116},
  number = {4},
  pages = {656--679},
  issn = {0014-1704, 1539-297X},
  doi = {10.1086/505233},
  urldate = {2022-11-24},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\EXTSELC9\The Reversal Test Eliminating Status Quo Bias in Applied Ethics (Bostrom and Ord, 2006).pdf}
}

@article{boughanmiCollectifsHumains2016,
  title = {Sur Les Collectifs Humains},
  author = {Boughanmi, Jihene},
  editor = {Neveu, F. and Bergounioux, G. and C{\^o}t{\'e}, M.-H. and Fournier, J.-M. and Hriba, L. and Pr{\'e}vost, S.},
  year = {2016},
  journal = {SHS Web of Conferences},
  volume = {27},
  issn = {2261-2424},
  doi = {10.1051/shsconf/20162712004},
  urldate = {2023-02-16},
  abstract = {About collective human nouns. Several types of referential entities are classified under the label collective : entities that refer to inanimates as well as entities that refer to animates. In this contribution, we will be focusing on collective human nouns. Our approach aims at presenting the general characteristics as well as the distinctive features of this class of nouns. The general characteristics through the presentation of a typology and their peculiartities through the study of a human collective noun in particular : people.}
}

@inproceedings{boughorbelImprovingLanguageModels2024,
  title = {Improving {{Language Models Trained}} on {{Translated Data}} with {{Continual Pre-Training}} and {{Dictionary Learning Analysis}}},
  booktitle = {Proceedings of {{The Second Arabic Natural Language Processing Conference}}},
  author = {Boughorbel, Sabri and Parvez, Md Rizwan and Hawasly, Majd},
  year = {2024},
  pages = {73--88},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.arabicnlp-1.7},
  urldate = {2024-11-11},
  abstract = {Training LLMs in low resources languages usually utilizes machine translation (MT) data augmentation from English language. However, translation brings a number of challenges: there are large costs attached to translating and curating huge amounts of content with high-end machine translation solutions; the translated content carries over cultural biases; and if the translation is not faithful and accurate, the quality of the data degrades causing issues in the trained model. In this work, we investigate the role of translation and synthetic data in training language models. We translate TinyStories, a dataset of 2.2M short stories for 3-4 year old children, from English to Arabic using the open NLLB3B MT model. We train a number of story generation models of size 1M-33M parameters using this data. We identify a number of quality and task-specific issues in the resulting models. To rectify these issues, we further pre-train the models with a small dataset of synthesized high-quality stories generated by a capable LLM in Arabic, representing 1\% of the original training data. We show, using GPT-4 as a judge and dictionary learning analysis from mechanistic interpretability, that the suggested approach is a practical means to resolve some of the translation pitfalls. We illustrate the improvement through case studies of linguistic and cultural bias issues.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\IRXJIMP7\Boughorbel et al. - 2024 - Improving Language Models Trained on Translated Da.pdf}
}

@article{boyleUnreliableNarrationGreat1969,
  title = {Unreliable {{Narration}} in "{{The Great Gatsby}}"},
  author = {Boyle, Thomas E.},
  year = {1969},
  journal = {The Bulletin of the Rocky Mountain Modern Language Association},
  volume = {23},
  number = {1},
  eprint = {1346578},
  eprinttype = {jstor},
  pages = {6},
  issn = {0035-7626},
  doi = {10.2307/1346578},
  urldate = {2021-08-22},
  keywords = {fitzgerald},
  annotation = {GATSBY},
  file = {C:\Users\spide\Zotero\storage\3MFHGKZ7\Unreliable Narration in The Great Gatsby (Boyle, 1969).pdf}
}

@misc{brandlHowConservativeAre2022,
  title = {How {{Conservative}} Are {{Language Models}}? {{Adapting}} to the {{Introduction}} of {{Gender-Neutral Pronouns}}},
  shorttitle = {How {{Conservative}} Are {{Language Models}}?},
  author = {Brandl, Stephanie and Cui, Ruixiang and S{\o}gaard, Anders},
  year = {2022},
  month = may,
  number = {arXiv:2204.10281},
  eprint = {2204.10281},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-10-07},
  abstract = {Gender-neutral pronouns have recently been introduced in many languages to a) include non-binary people and b) as a generic singular. Recent results from psycholinguistics suggest that gender-neutral pronouns (in Swedish) are not associated with human processing difficulties. This, we show, is in sharp contrast with automated processing. We show that gender-neutral pronouns in Danish, English, and Swedish are associated with higher perplexity, more dispersed attention patterns, and worse downstream performance. We argue that such conservativity in language models may limit widespread adoption of genderneutral pronouns and must therefore be resolved.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\XJ2E4N66\How Conservative are Language Models Adapting to the Introduction of Gender-Neutral Pronouns (Brandl et al., 2022).pdf}
}

@article{brandsGeorgeBushGulf2004,
  title = {George {{Bush}} and the {{Gulf War}} of 1991},
  author = {Brands, H. W.},
  year = {2004},
  month = mar,
  journal = {Presidential Studies Quarterly},
  volume = {34},
  number = {1},
  pages = {25},
  issn = {0360-4918, 1741-5705},
  doi = {10.1111/j.1741-5705.2004.00038.x},
  urldate = {2021-12-17},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\CP8FMWAG\George Bush and the Gulf War of 1991 (Brands, 2004).pdf}
}

@article{brandsWorldWordRise1998,
  title = {The {{World}} in a {{Word}}: {{The Rise}} and {{Fall}} of {{D{\'e}tente}}},
  shorttitle = {The {{World}} in a {{Word}}},
  author = {Brands, H. W.},
  year = {1998},
  journal = {Rhetoric and Public Affairs},
  volume = {1},
  number = {1},
  eprint = {41939430},
  eprinttype = {jstor},
  pages = {15},
  issn = {1094-8392},
  urldate = {2021-12-17},
  keywords = {civi for. pol.},
  annotation = {CIVI FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\K5INIZII\The World in a Word (Brands, 1998).pdf}
}

@article{braunCognitiveEffectsMasculine2005,
  title = {Cognitive {{Effects}} of {{Masculine Generics}} in {{German}}: {{An Overview}} of {{Empirical Findings}}},
  shorttitle = {Cognitive {{Effects}} of {{Masculine Generics}} in {{German}}},
  author = {Braun, Friederike and Sczesny, Sabine and Stahlberg, Dagmar},
  year = {2005},
  month = jan,
  journal = {Communications},
  volume = {30},
  number = {1},
  pages = {1--21},
  issn = {0341-2059, 1613-4087},
  doi = {10.1515/comm.2005.30.1.1},
  urldate = {2024-03-23},
  abstract = {This article presents a series of experiments which were conducted among native speakers of German to determine the influence of different types of German generics on the cognitive inclusion of women. Results indicate that the inclusion of women is higher with `non-sexist' alternatives than with masculine generics, a tendency which was consistent across different studies. The different alternatives, however, showed different effects which also varied depending on the context. These results are discussed with regard to their practical consequences in situations such as nominating women and men for awards or political offices.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\3LECFRTN\Cognitive Effects of Masculine Generics in German An Overview of Empirical Findings (Braun et al., 2005).pdf}
}

@article{brennerPowerMetaphorExplaining2010,
  title = {The {{Power}} of {{Metaphor}}: {{Explaining U}}.{{S}}. {{Policy}} toward {{Cuba}}},
  shorttitle = {The {{Power}} of {{Metaphor}}},
  author = {{brenner}, philip},
  year = {2010},
  month = apr,
  journal = {Diplomatic History},
  volume = {34},
  number = {2},
  pages = {8},
  issn = {01452096, 14677709},
  doi = {10.1111/j.1467-7709.2009.00857.x},
  urldate = {2021-12-17},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\JRSXQ9HU\The Power of Metaphor Explaining U.S. Policy toward Cuba (brenner, 2010).pdf}
}

@article{brinkleyBERNATHLECTURERising1996,
  title = {{{BERNATH LECTURE The Rising Stock}} of {{Jimmy Carter}}: {{The}} "{{Hands}} on" {{Legacy}} of {{Our Thirty-ninth President}}},
  shorttitle = {{{BERNATH LECTURE The Rising Stock}} of {{Jimmy Carter}}},
  author = {Brinkley, Douglas},
  year = {1996},
  month = oct,
  journal = {Diplomatic History},
  volume = {20},
  number = {4},
  pages = {25},
  issn = {0145-2096, 1467-7709},
  doi = {10.1111/j.1467-7709.1996.tb00285.x},
  urldate = {2021-12-17},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\3WUMA2MJ\BERNATH LECTURE The Rising Stock of Jimmy Carter The Hands on Legacy of Our Thirty-ninth President (Brinkley, 1996).pdf}
}

@article{brinkleyDemocraticEnlargementClinton1997,
  title = {Democratic {{Enlargement}}: {{The Clinton Doctrine}}},
  shorttitle = {Democratic {{Enlargement}}},
  author = {Brinkley, Douglas},
  year = {1997},
  journal = {Foreign Policy},
  number = {106},
  eprint = {1149177},
  eprinttype = {jstor},
  pages = {16},
  issn = {0015-7228},
  doi = {10.2307/1149177},
  urldate = {2021-12-13},
  keywords = {civi for. pol.},
  annotation = {CIVI FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\3D95M6T6\Democratic Enlargement (Brinkley, 1997).pdf}
}

@misc{brooksRiseAIGeneratedContent2024,
  title = {The {{Rise}} of {{AI-Generated Content}} in {{Wikipedia}}},
  author = {Brooks, Creston and Eggert, Samuel and Peskoff, Denis},
  year = {2024},
  month = oct,
  number = {arXiv:2410.08044},
  eprint = {2410.08044},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.08044},
  urldate = {2025-01-23},
  abstract = {The rise of AI-generated content in popular information sources raises significant concerns about accountability, accuracy, and bias amplification. Beyond directly impacting consumers, the widespread presence of this content poses questions for the long-term viability of training language models on vast internet sweeps. We use GPTZero, a proprietary AI detector, and Binoculars, an open-source alternative, to establish lower bounds on the presence of AI-generated content in recently created Wikipedia pages. Both detectors reveal a marked increase in AI-generated content in recent pages compared to those from before the release of GPT-3.5. With thresholds calibrated to achieve a 1\% false positive rate on pre-GPT-3.5 articles, detectors flag over 5\% of newly created English Wikipedia articles as AI-generated, with lower percentages for German, French, and Italian articles. Flagged Wikipedia articles are typically of lower quality and are often self-promotional or partial towards a specific viewpoint on controversial topics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\UY2S882S\\Brooks et al. - 2024 - The Rise of AI-Generated Content in Wikipedia.pdf;C\:\\Users\\spide\\Zotero\\storage\\9WF6ZLBZ\\2410.html}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models Are Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  eprint = {2005.14165},
  urldate = {2024-03-18},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -- something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2005.14165},
  keywords = {Computer Science - Computation and Language}
}

@article{burnettPoliticalDimensionsGender2021,
  title = {Political Dimensions of Gender Inclusive Writing in {{Parisian}} Universities},
  author = {Burnett, Heather and Pozniak, C{\'e}line},
  year = {2021},
  journal = {Journal of Sociolinguistics},
  volume = {25},
  number = {5},
  pages = {808--831},
  issn = {1467-9841},
  doi = {10.1111/josl.12489},
  urldate = {2025-01-24},
  abstract = {{\'E}criture inclusive (EI) has long been the topic of public debates in France. These debates have become more intense in recent years, often focusing on the higher education system and culminating in the formulation of three separate laws banning it for public administration. In this paper, we investigate the foundations of these conflicts through a large quantitative corpus study of the (non)use of EI in Parisian undergraduate brochures. Our results suggest that Parisian university professors use EI not only to ensure gender neutral reference but also as a tool to construct their political identities. We show that both the use of EI and its particular forms are conditioned by how brochure writers position themselves on non gender-related-related issues within the French university's political landscape, which explains how conflicts surrounding a linguistic practice have become understood as conflicts about larger issues in French society. Our paper thus provides new information to be taken into account in the formulation and promotion of nonsexist language policies and sheds light on how feminist linguistic activism and its opposition are deeply intertwined with other kinds of social activism in present-day France.},
  copyright = {{\copyright} 2021 The Authors. Journal of Sociolinguistics published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {France,French,inclusive writing,political ideology,prestige,quantitative study},
  file = {C\:\\Users\\spide\\Zotero\\storage\\FFP9NYRQ\\Burnett and Pozniak - 2021 - Political dimensions of gender inclusive writing in Parisian universities.pdf;C\:\\Users\\spide\\Zotero\\storage\\AB9VJUS3\\josl.html}
}

@article{burrGenderLanguagePolitics2003,
  title = {Gender and {{Language Politics}} in {{France}}},
  author = {Burr, Elisabeth},
  year = {2003},
  journal = {Gender Across Languages},
  volume = {3},
  pages = {119--139},
  keywords = {lu,pol. ling.}
}

@misc{caiLocatingMitigatingGender2024,
  title = {Locating and {{Mitigating Gender Bias}} in {{Large Language Models}}},
  author = {Cai, Yuchen and Cao, Ding and Guo, Rongxi and Wen, Yaqin and Liu, Guiquan and Chen, Enhong},
  year = {2024},
  month = mar,
  number = {arXiv:2403.14409},
  eprint = {2403.14409},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.14409},
  urldate = {2024-12-17},
  abstract = {Large language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\5T4D2XUX\\Cai et al. - 2024 - Locating and Mitigating Gender Bias in Large Language Models.pdf;C\:\\Users\\spide\\Zotero\\storage\\W5QGFG92\\2403.html}
}

@incollection{cameronLakoffContextSocial1998,
  title = {Lakoff in {{Context}}: {{The Social}} and {{Linguistic Functions}} of {{Tag Questions}}},
  booktitle = {Women's {{Language}}: {{Critical Approaches}}},
  author = {Cameron, Deborah and McAlinden, Fiona and O'Leary, Kathy},
  year = {1998},
  pages = {75--93},
  urldate = {2023-01-19},
  isbn = {978-1-315-84679-8}
}

@article{campoHesPlantingGarden2004,
  title = {"{{He}}'s {{Planting}} the {{Garden}}!": {{The}} Garden as Unifying Symbol in "{{Death}} of a {{Salesman}}"},
  author = {Campo, Carlos},
  year = {2004},
  month = dec,
  journal = {Penn State University Press},
  volume = {10},
  pages = {2},
  langid = {english},
  keywords = {miller},
  file = {C:\Users\spide\Documents\PDF recherches\He's Planting the Garden! The garden as unifying symbol in Death of a Salesman (Campo, 2004).pdf}
}

@article{cappeauManqueIndefinisOu2008,
  title = {{Il manque des ind{\'e}finis ! ou comment l'oral nous oblige {\`a} revoir la description des ind{\'e}finis:}},
  shorttitle = {{Il manque des ind{\'e}finis ! ou comment l'oral nous oblige {\`a} revoir la description des ind{\'e}finis}},
  author = {Cappeau, Paul},
  year = {2008},
  month = oct,
  journal = {Le fran{\c c}ais aujourd'hui},
  volume = {n{$^\circ$} 162},
  number = {3},
  pages = {73--83},
  issn = {0184-7732},
  doi = {10.3917/lfa.162.0073},
  urldate = {2022-11-25},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\WUJ3TUUL\Il manque des indéfinis (Cappeau, 2008).pdf}
}

@article{caseyGenderTroubleTwelfth1997,
  title = {Gender {{Trouble}} in {{Twelfth Night}}},
  author = {Casey, Charles},
  year = {1997},
  month = may,
  journal = {Theatre Journal},
  volume = {49},
  pages = {121--141},
  langid = {english},
  keywords = {shakespeare},
  file = {C:\Users\spide\Documents\PDF recherches\Gender Trouble in Twelfth Night (, ).pdf}
}

@article{cassPanderedWhispersNarrative1980,
  title = {"{{Pandered}} in {{Whispers}}": {{Narrative Reliability}} in "{{The Great Gatsby}}"},
  shorttitle = {"{{Pandered}} in {{Whispers}}"},
  author = {Cass, Colin S.},
  year = {1980},
  journal = {College Literature},
  volume = {7},
  number = {2},
  eprint = {25111322},
  eprinttype = {jstor},
  pages = {11},
  issn = {0093-3139},
  urldate = {2021-08-22},
  keywords = {fitzgerald},
  annotation = {GATSBY},
  file = {C:\Users\spide\Zotero\storage\74BD884R\Pandered in Whispers (Cass, 1980).pdf}
}

@article{chavalariasMinuitMoinsDix,
  title = {{Minuit moins dix {\`a} l'horloge de Poutine}},
  author = {Chavalarias, David},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\KSKG9NVX\Chavalarias - Minuit moins dix à l'horloge de Poutine.pdf}
}

@article{chenAutomaticExtractionSubordinate2021,
  title = {Automatic Extraction of Subordinate Clauses and Its Application in Second Language Acquisition Research},
  author = {Chen, Xiaobin and Alexopoulou, Theodora and Tsimpli, Ianthi},
  year = {2021},
  month = apr,
  journal = {Behavior Research Methods},
  volume = {53},
  number = {2},
  pages = {15},
  issn = {1554-3528},
  doi = {10.3758/s13428-020-01456-7},
  urldate = {2022-08-27},
  abstract = {Clause subordination is an important linguistic phenomenon that is relevant to research in psycholinguistics, cognitive and behavioral sciences, language acquisition, and computational information retrieval. The paper presents a comprehensive tool called AutoSubClause, which is specifically designed for extracting subordinate clause (SC) information from natural English production. Using dependency parsing, AutoSubClause is able to extract not only information characterizing the three main types of SCs---complement, adverbial, and relative clauses---but also information regarding the internal structure of different clause types and their semantic and structural relations with elements of the main clause. Robustness testing of the system and its underlying dependency parser Stanford CoreNLP showed satisfactory results. To demonstrate the usefulness of AutoSubClause, we used it to analyze a large-scale learner corpus and investigate the effects of first language (L1) on the acquisition of subordination in second language (L2) English. Our analysis shows that learners from an L1 that is typologically different from the L2 in clause subordination tend to have different developmental trajectories from those whose L1 is typologically similar to the L2. Furthermore, the developmental patterns for different types of SCs also vary. This finding suggests the need to approach clausal subordination as a multi-componential construct rather than a unitary one, as is the case in most previous research. Finally, we demonstrate how NLP technology can support research questions that rely on linguistic analysis across various disciplines and help gain new insights with the increasing opportunities for up-scaled analysis.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\RKXET89E\Automatic extraction of subordinate clauses and its application in second language acquisition research (Chen et al., 2021).pdf}
}

@misc{chenChatGPTsOneyearAnniversary2024,
  title = {{{ChatGPT}}'s {{One-year Anniversary}}: {{Are Open-Source Large Language Models Catching}} Up?},
  shorttitle = {{{ChatGPT}}'s {{One-year Anniversary}}},
  author = {Chen, Hailin and Jiao, Fangkai and Li, Xingxuan and Qin, Chengwei and Ravaut, Mathieu and Zhao, Ruochen and Xiong, Caiming and Joty, Shafiq},
  year = {2024},
  month = jan,
  number = {arXiv:2311.16989},
  eprint = {2311.16989},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.16989},
  urldate = {2024-12-17},
  abstract = {Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closedsource LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\spide\Zotero\storage\UMQYQVB2\Chen et al. - 2024 - ChatGPT's One-year Anniversary Are Open-Source Large Language Models Catching up.pdf}
}

@inproceedings{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  eprint = {1603.02754},
  primaryclass = {cs},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  urldate = {2025-01-09},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\spide\\Zotero\\storage\\D9CJLFWC\\Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf;C\:\\Users\\spide\\Zotero\\storage\\SG8U2RJW\\1603.html}
}

@incollection{chervelLaccordProximiteGrammaire2019,
  title = {L'accord de Proximit{\'e} et La Grammaire},
  booktitle = {Le F{\'e}minin et Le Masculin Dans La Langue. {{L}}'{\'e}criture Inclusive En Question},
  author = {Chervel, Andr{\'e}},
  year = {2019},
  pages = {95--114},
  publisher = {ESF},
  abstract = {Comment les femmes et les hommes sont-ils d{\'e}sign{\'e}s dans les langues ? Le sont-ils de fa{\c c}on {\'e}quitable ? L'{\'e}criture inclusive enflamme les d{\'e}bats et met en lumi{\`e}re de nombreuses questions sur la langue fran{\c c}aise : a-t-elle {\'e}t{\'e} volontairement amend{\'e}e pour exclure les femmes ? Qu'est-ce que le genre ? Y a-t-il un neutre en fran{\c c}ais ? Pourquoi l'accord au masculin est-il dominant ? L'{\'e}cole doit-elle s'emparer du sujet ? Dans cet essai, des linguistes se saisissent de la question pour replacer l'{\'e}tude et l'histoire de la langue au coeur de la r{\'e}flexion. Ils proposent de prendre le temps d'examiner ce champ de discussion pour en comprendre les enjeux et y voir plus clair. Des apports pr{\'e}cieux sur les langues anglaise, allemande, cor{\'e}enne et arabe ouvrent d'autres perspectives.},
  isbn = {978-2-7101-3894-5}
}

@article{chevalierApprochesLinguistiquesGenre2013,
  title = {{Approches linguistiques du genre (gender)}},
  author = {Chevalier, Yannick},
  year = {2013},
  journal = {La Cl{\'e} des Langues},
  urldate = {2024-12-01},
  langid = {french},
  keywords = {read},
  file = {C:\Users\spide\Zotero\storage\JY78T5X4\Chevalier - Approches linguistiques du genre (gender).pdf}
}

@article{choDALLEvalProbingReasoning2022,
  title = {{{DALL-Eval}}: {{Probing}} the {{Reasoning Skills}} and {{Social Biases}} of {{Text-to-Image Generative Transformers}}},
  shorttitle = {{{DALL-Eval}}},
  author = {Cho, Jaemin and Zala, Abhay and Bansal, Mohit},
  year = {2022},
  month = feb,
  pages = {12},
  urldate = {2022-08-25},
  abstract = {Generating images from textual descriptions has gained a lot of attention. Recently, DALL-E, a multimodal transformer language model, and its variants have shown high-quality text-to-image generation capabilities with a simple architecture and training objective, powered by large-scale training data and computation. However, despite the interesting image generation results, there has not been a detailed analysis on how to evaluate such models. In this work, we investigate the reasoning capabilities and social biases of such text-to-image generative transformers in detail. First, we measure four visual reasoning skills: object recognition, object counting, color recognition, and spatial relation understanding. For this, we propose PaintSkills, a diagnostic dataset and evaluation toolkit that measures these four visual reasoning skills. Second, we measure the text alignment and quality of the generated images based on pretrained image captioning, image-text retrieval, and image classification models. Third, we assess social biases in the models. For this, we suggest evaluation of gender and racial biases of text-to-image generation models based on a pretrained image-text retrieval model and human evaluation. In our experiments, we show that recent text-to-image models perform better in recognizing and counting objects than recognizing colors and understanding spatial relations, while there exists a large gap between model performances and oracle accuracy on all skills. Next, we demonstrate that recent text-to-image models learn specific gender/racial biases from web image-text pairs. We also show that our automatic evaluations of visual reasoning skills and gender bias are highly correlated with human judgments. We hope our work will help guide future progress in improving text-to-image models on visual reasoning skills and social biases. Code and data at: https://github.com/j-min/DallEval},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\44P6RYHG\DALL-Eval Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers (Cho et al., 2022).pdf}
}

@article{chomskynoamRemarksNominalization1970,
  title = {Remarks on {{Nominalization}}},
  author = {{Chomsky, Noam}},
  year = {1970},
  pages = {56},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\G24PJT8F\Chomsky Remarks on Nominalization (1970; rev 1975) OCR.pdf}
}

@misc{chuttarsingInflecteur2021,
  title = {Inflecteur},
  author = {Chuttarsing, Adrien},
  year = {2021}
}

@article{cihonCorporateGovernanceArtificial2021,
  title = {Corporate {{Governance}} of {{Artificial Intelligence}} in the {{Public Interest}}},
  author = {Cihon, Peter and Schuett, Jonas and Baum, Seth D.},
  year = {2021},
  month = jul,
  journal = {Information},
  volume = {12},
  number = {7},
  pages = {275},
  issn = {2078-2489},
  doi = {10.3390/info12070275},
  urldate = {2024-12-17},
  abstract = {Corporations play a major role in artificial intelligence (AI) research, development, and deployment, with profound consequences for society. This paper surveys opportunities to improve how corporations govern their AI activities so as to better advance the public interest. The paper focuses on the roles of and opportunities for a wide range of actors inside the corporation---managers, workers, and investors---and outside the corporation---corporate partners and competitors, industry consortia, nonprofit organizations, the public, the media, and governments. Whereas prior work on multistakeholder AI governance has proposed dedicated institutions to bring together diverse actors and stakeholders, this paper explores the opportunities they have even in the absence of dedicated multistakeholder institutions. The paper illustrates these opportunities with many cases, including the participation of Google in the U.S. Department of Defense Project Maven; the publication of potentially harmful AI research by OpenAI, with input from the Partnership on AI; and the sale of facial recognition technology to law enforcement by corporations including Amazon, IBM, and Microsoft. These and other cases demonstrate the wide range of mechanisms to advance AI corporate governance in the public interest, especially when diverse actors work together.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\YMSL4UK3\Cihon et al. - 2021 - Corporate Governance of Artificial Intelligence in the Public Interest.pdf}
}

@article{CirculaireBlanquerContre,
  title = {Circulaire {{Blanquer}} Contre l'{\'e}criture Dite Inclusive},
  pages = {10}
}

@misc{CirculaireBlanquerContrea,
  title = {Circulaire {{Blanquer}} Contre l'{\'e}criture Dite Inclusive}
}

@article{clarkecaywoodEthicsPersonalSelling,
  title = {Ethics and {{Personal Selling}}: {{Death}} of a {{Salesman}} as an {{Ethical Primer}}},
  author = {{Clarke Caywood}},
  journal = {Journal of Personal Selling \& Sales Management},
  volume = {6},
  number = {2},
  pages = {9},
  langid = {english},
  keywords = {miller},
  file = {C:\Users\spide\Zotero\storage\LJF8R6GX\Ethics and Personal Selling Death of a Salesman as an Ethical Primer (Clarke Caywood, ).pdf}
}

@article{ColdWarVietnam,
  title = {Cold {{War}} and {{Vietnam}}},
  pages = {4},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\TCV9SZZ8\L3 Cold War and Vietnam.pdf}
}

@misc{colvinPydantic2025,
  title = {Pydantic},
  author = {Colvin, Samuel and Jolibois, Eric and Ramezani, Hasan and Garcia Badaracco, Adrian and Dorsey, Terrence and Montague, David and Matveenko, Serge and Trylesinski, Marcelo and Runkle, Sydney and Hewitt, David and Hall, Alex and Plot, Victorien},
  year = {2025},
  month = jan,
  urldate = {2025-01-09},
  abstract = {Data validation using Python type hints},
  copyright = {MIT}
}

@article{combaz-champlaineRectificationsOrthographiques19902020,
  title = {{Les rectifications orthographiques de 1990 comme r{\'e}v{\'e}lateurs du rapport des enseignants {\`a} l'orthographe}},
  author = {{Combaz-Champlaine}, Catherine},
  year = {2020},
  month = jan,
  journal = {Glottopol},
  number = {33},
  issn = {1769-7425},
  doi = {10.4000/glottopol.559},
  urldate = {2022-12-07},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\JHWH2U3C\Les rectifications orthographiques de 1990 comme révélateurs du rapport des enseignants à l’orthographe (Combaz-Champlaine, 2020).pdf}
}

@article{ComparativeStudyHerman2020,
  title = {A {{Comparative Study}} between {{Herman Melville}}'s {{Typee}} and {{Laila Lalami}}'s},
  year = {2020},
  month = apr,
  volume = {9},
  number = {2},
  pages = {10},
  issn = {2278-4012},
  abstract = {The present paper compares and contrasts between Herman Melville's Typee (1846) and Laila Lalami's The Moor's Account (2014). As historical novels, both texts challenge the metanarratives given voice to the disfranchised Mustapha and Tommo to narrate the adventures they undertake. For the sake of conducting the study, the paper employs textual, analytical, and comparative methods to highlight the affinities between both texts including themes of the connection with nature, the theme of survival, and the theme of challenging the grand narratives as a case in point. At the same time, the study shows that both novels differ in other aspects as the representation of women, narrative techniques, and the ending.},
  langid = {english},
  keywords = {melville},
  file = {C:\Users\spide\Zotero\storage\E29DWYYV\A Comparative Study between Herman Melville’s Typee and Laila Lalami’s (, 2020).pdf}
}

@article{CongressCIAGuatemala1954,
  title = {Congress, the {{CIA}}, and {{Guatemala}}, 1954 --- {{Central Intelligence Agency}}},
  year = {1954},
  pages = {6},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\JRARX79C\Congress, the CIA, and Guatemala, 1954 — Central Intelligence Agency (, 1954).pdf}
}

@article{contini-moravaFunctionsNominalClassification2013,
  title = {Functions of Nominal Classification},
  author = {{Contini-Morava}, Ellen and Kilarski, Marcin},
  year = {2013},
  month = nov,
  journal = {Language Sciences},
  volume = {40},
  pages = {263--299},
  issn = {03880001},
  doi = {10.1016/j.langsci.2013.03.002},
  urldate = {2024-12-03},
  abstract = {Nominal classification systems are generally categorized on the basis of morphosyntactic criteria. However, the functional motivations for these phenomena do not coincide directly with their morphosyntactic properties: some functions are shared by diverse systems, and each morphosyntactic type may serve diverse communicative functions. We provide a functional typology for nominal classification, including both noun class and classifier systems. We focus on two types of functions: semantic, i.e., the use of classification markers to expand the referential power of the lexicon, and discourse/pragmatic, i.e., the use of classification markers to establish and manipulate the status of discourse referents. We identify functions that are shared by formally diverse systems as well as functions that depend on means of expression. We also review psycholinguistic evidence for the role of nominal classification in language comprehension and production.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\UKZWUNN5\Contini-Morava and Kilarski - 2013 - Functions of nominal classification.pdf}
}

@book{corbettExpressionGender2014,
  title = {The Expression of Gender},
  editor = {Corbett, Greville G.},
  year = {2014},
  series = {The Expression of Cognitive Categories},
  number = {6},
  publisher = {De Gruyter},
  address = {Berlin},
  isbn = {978-3-11-030660-6},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\NLEYJ3TY\Corbett - 2014 - The expression of gender.pdf}
}

@book{corbettGender1991,
  title = {Gender},
  author = {Corbett, Greville G.},
  year = {1991},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  abstract = {Gender is a fascinating category, central and pervasive in some languages and totally absent in others. In this new, overall account of gender systems, over 200 languages are discussed, from English and Russian to Archi and Chichewa. More detailed analysis of individual languages provides clear illustrations of specific types of systems. Gender distinction is often based on sex; sometimes this is only one criterion and the gender of nouns depends on other factors (thus "house" is masculine in Russian, feminine in French and neuter in Tamil). On occasion there are equivalent distinctions such as human/non-human, animate/inanimate, where sex is irrelevant.},
  isbn = {978-0-521-32939-2}
}

@book{corbettGender1991a,
  title = {Gender},
  author = {Corbett, Greville G.},
  year = {1991},
  month = apr,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9781139166119},
  urldate = {2024-11-21},
  abstract = {Gender is a fascinating category, central and pervasive in some languages and totally absent in others. In this new, comprehensive account of gender systems, over 200 languages are discussed, from English and Russian to Archi and Chichewa. Detailed analysis of individual languages provides clear illustrations of specific types of system. Gender distinction is often based on sex; sometimes this is only one criterion and the gender of nouns depends on other factors (thus 'house' is masculine in Russian, feminine in French and neuter in Tamil). Some languages have comparable distinctions such as human/non-human, animate/inanimate, where sex is irrelevant. No other textbook surveys gender across this range of languages. Gender will be invaluable both for class use and as a reference resource for students and researchers in linguistics.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-0-521-32939-2 978-0-521-33845-5 978-1-139-16611-9},
  file = {C:\Users\spide\Zotero\storage\EX7V2ZUF\Corbett - 1991 - Gender.pdf}
}

@article{cormackMigrationPoliticsNarrative2006,
  title = {Migration and the {{Politics}} of {{Narrative Form}}: {{Realism}} and the {{Postcolonial Subject}} in {{Brick Lane}}},
  shorttitle = {Migration and the {{Politics}} of {{Narrative Form}}},
  author = {Cormack, Alistair},
  year = {2006},
  journal = {Contemporary Literature},
  volume = {47},
  number = {4},
  pages = {28},
  issn = {1548-9949},
  doi = {10.1353/cli.2007.0014},
  urldate = {2022-04-20},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\KTK99SWM\Migration and the Politics of Narrative Form Realism and the Postcolonial Subject in Brick Lane (Cormack, 2006).pdf}
}

@article{costa-jussaAnalysisGenderBias2019,
  title = {An Analysis of Gender Bias Studies in Natural Language Processing},
  author = {{Costa-juss{\`a}}, Marta R.},
  year = {2019},
  month = nov,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {11},
  pages = {11},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0105-5},
  urldate = {2022-08-10},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\UPAWKMJM\An analysis of gender bias studies in natural language processing (Costa-jussà, 2019).pdf}
}

@article{cottPassionlessnessInterpretationVictorian2020,
  title = {Passionlessness: {{An Interpretation}} of {{Victorian Sexual Ideology}}, 1790-1850},
  author = {Cott, Nancy F},
  year = {2020},
  pages = {19},
  langid = {english},
  keywords = {civi. women},
  annotation = {WOMEN},
  file = {C:\Users\spide\Zotero\storage\Y7Y2IHKW\Passionlessness An Interpretation of Victorian Sexual Ideology, 1790-1850 (Cott, 2020).pdf}
}

@misc{cunninghamSparseAutoencodersFind2023,
  title = {Sparse {{Autoencoders Find Highly Interpretable Features}} in {{Language Models}}},
  author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  year = {2023},
  month = oct,
  number = {arXiv:2309.08600},
  eprint = {2309.08600},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-11},
  abstract = {One of the roadblocks to a better understanding of neural networks' internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, humanunderstandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is superposition, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task (Wang et al., 2022) to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\spide\Zotero\storage\QX3LUQAW\Cunningham et al. - 2023 - Sparse Autoencoders Find Highly Interpretable Feat.pdf}
}

@book{curzanGenderShiftsHistory2003,
  title = {Gender {{Shifts}} in the {{History}} of {{English}}},
  author = {Curzan, Anne},
  year = {2003},
  publisher = {Cambridge University Press},
  isbn = {0 521 82007 3},
  langid = {english}
}

@misc{danilevskySurveyStateExplainable2020,
  title = {A {{Survey}} of the {{State}} of {{Explainable AI}} for {{Natural Language Processing}}},
  author = {Danilevsky, Marina and Qian, Kun and Aharonov, Ranit and Katsis, Yannis and Kawas, Ban and Sen, Prithviraj},
  year = {2020},
  month = oct,
  number = {arXiv:2010.00711},
  eprint = {2010.00711},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.00711},
  urldate = {2024-12-17},
  abstract = {Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\spide\Zotero\storage\PIVUZHLX\Danilevsky et al. - 2020 - A Survey of the State of Explainable AI for Natural Language Processing.pdf}
}

@article{DeclineParliamentaryGovernment2022,
  title = {The {{Decline}} of {{Parliamentary Government}} under {{Elizabeth I}} and the {{Early Stuarts}}},
  year = {2022},
  pages = {18},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\FU7AQ7NQ\The Decline of Parliamentary Government under Elizabeth I and the Early Stuarts (, 2022).pdf}
}

@misc{deepseek-aiDeepSeekV3TechnicalReport2024,
  title = {{{DeepSeek-V3 Technical Report}}},
  author = {{DeepSeek-AI} and Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Zhang, Haowei and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J. L. and Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi and Wang, Jiawei and Chen, Jin and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Song, Junxiao and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Xu, Lei and Xia, Leyi and Zhao, Liang and Wang, Litong and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and Zhang, Peng and Wang, Qiancheng and Zhu, Qihao and Chen, Qinyu and Du, Qiushi and Chen, R. J. and Jin, R. L. and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Xu, Runxin and Zhang, Ruoyu and Chen, Ruyi and Li, S. S. and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu, Shaoqing and Ye, Shengfeng and Ye, Shengfeng and Ma, Shirong and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Wang, T. and Yun, Tao and Pei, Tian and Sun, Tianyu and Xiao, W. L. and Zeng, Wangding and Zhao, Wanjia and An, Wei and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Li, X. Q. and Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and Zhang, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun, Xiaowen and Wang, Xiaoxiang and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yu, Xingkai and Song, Xinnan and Shan, Xinxia and Zhou, Xinyi and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhu, Y. X. and Zhang, Yang and Xu, Yanhong and Xu, Yanhong and Huang, Yanping and Li, Yao and Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui and Yu, Yi and Zheng, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Tang, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Wu, Yu and Ou, Yuan and Zhu, Yuchen and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Zha, Yukun and Xiong, Yunfan and Ma, Yunxian and Yan, Yuting and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Wu, Z. F. and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Huang, Zhen and Zhang, Zhen and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Gou, Zhibin and Ma, Zhicheng and Yan, Zhigang and Shao, Zhihong and Xu, Zhipeng and Wu, Zhiyu and Zhang, Zhongyu and Li, Zhuoshu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Gao, Ziyi and Pan, Zizheng},
  year = {2024},
  month = dec,
  number = {arXiv:2412.19437},
  eprint = {2412.19437},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.19437},
  urldate = {2025-01-24},
  abstract = {We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\P7XC96PV\\DeepSeek-AI et al. - 2024 - DeepSeek-V3 Technical Report.pdf;C\:\\Users\\spide\\Zotero\\storage\\ZJEZAKQD\\2412.html}
}

@article{desousaArtificialIntelligenceSpeedy2022,
  title = {Artificial Intelligence and Speedy Trial in the Judiciary: {{Myth}}, Reality or Need? {{A}} Case Study in the {{Brazilian Supreme Court}} ({{STF}})},
  shorttitle = {Artificial Intelligence and Speedy Trial in the Judiciary},
  author = {De Sousa, Weslei Gomes and Fidelis, Rafael Antunes and De Souza Bermejo, Paulo Henrique and Da Silva Gon{\c c}alo, Ana Gersica and De Souza Melo, Bruno},
  year = {2022},
  month = jan,
  journal = {Government Information Quarterly},
  volume = {39},
  number = {1},
  pages = {101660},
  issn = {0740624X},
  doi = {10.1016/j.giq.2021.101660},
  urldate = {2024-12-17},
  langid = {english}
}

@article{devinneyTheoriesGenderNLP2022,
  title = {Theories of "{{Gender}}" in {{NLP Bias Research}}},
  author = {Devinney, Hannah and Bj{\"o}rklund, Jenny and Bj{\"o}rklund, Henrik},
  year = {2022},
  month = may,
  eprint = {2205.02526},
  primaryclass = {cs},
  pages = {26},
  urldate = {2022-08-10},
  abstract = {The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how ``gender'' is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time. We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define ``bias.'' Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\2A6RKCM8\Theories of Gender in NLP Bias Research (Devinney et al., 2022).pdf}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-19},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\spide\Zotero\storage\HSH3WV85\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@misc{dhingraQueerPeopleAre2023,
  title = {Queer {{People}} Are {{People First}}: {{Deconstructing Sexual Identity Stereotypes}} in {{Large Language Models}}},
  shorttitle = {Queer {{People}} Are {{People First}}},
  author = {Dhingra, Harnoor and Jayashanker, Preetiha and Moghe, Sayali and Strubell, Emma},
  year = {2023},
  month = jun,
  number = {arXiv:2307.00101},
  eprint = {2307.00101},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.00101},
  urldate = {2024-12-17},
  abstract = {Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\KLERKLZJ\\Dhingra et al. - 2023 - Queer People are People First Deconstructing Sexual Identity Stereotypes in Large Language Models.pdf;C\:\\Users\\spide\\Zotero\\storage\\JUQRW3YJ\\2307.html}
}

@misc{dollEvaluatingGenderBias2024,
  title = {Evaluating {{Gender Bias}} in {{Large Language Models}}},
  author = {D{\"o}ll, Michael and D{\"o}hring, Markus and M{\"u}ller, Andreas},
  year = {2024},
  month = nov,
  number = {arXiv:2411.09826},
  eprint = {2411.09826},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-19},
  abstract = {Gender bias in artificial intelligence has become an important issue, particularly in the context of language models used in communication-oriented applications. This study examines the extent to which Large Language Models (LLMs) exhibit gender bias in pronoun selection in occupational contexts. The analysis evaluates the models GPT-4, GPT-4o, PaLM 2 Text Bison and Gemini 1.0 Pro using a self-generated dataset. The jobs considered include a range of occupations, from those with a significant male presence to those with a notable female concentration, as well as jobs with a relatively equal gender distribution. Three different sentence processing methods were used to assess potential gender bias: masked tokens, unmasked sentences, and sentence completion. In addition, the LLMs suggested names of individuals in specific occupations, which were then examined for gender distribution. The results show a positive correlation between the models' pronoun choices and the gender distribution present in U.S. labor force data. Female pronouns were more often associated with female-dominated occupations, while male pronouns were more often associated with male-dominated occupations. Sentence completion showed the strongest correlation with actual gender distribution, while name generation resulted in a more balanced `politically correct' gender distribution, albeit with notable variations in predominantly male or female occupations. Overall, the prompting method had a greater impact on gender distribution than the model selection itself, highlighting the complexity of addressing gender bias in LLMs. The findings highlight the importance of prompting in gender mapping.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\spide\Zotero\storage\YE3WMZH5\Döll et al. - 2024 - Evaluating Gender Bias in Large Language Models.pdf}
}

@phdthesis{doyenNeutralisationAutomatiqueGenre2024,
  title = {{Neutralisation automatique du genre {\`a} travers l'utilisation de noms collectifs en fran{\c c}ais}},
  author = {Doyen, Enzo},
  year = {2024},
  langid = {french},
  school = {Universit{\'e} de Strasbourg}
}

@masterthesis{DreierNicoleElizabeth2018GiPa,
  title = {Gender in {{Proto-Indo-European}} and the Feminine Morphemes},
  author = {Dreier, Nicole Elizabeth},
  year = {2018},
  abstract = {The three-gender system seen in the core Indo-European languages is not the oldest gender system in Proto-Indo-European (PIE). There is evidence of an earlier animacy-based, two-gender system in PIE, which raises the question of how the third gender (i.e. the feminine) came to be. Its origins are made even more uncertain by the feminizing suffixes *-(e)h2-, *-ih2-, and *-i-hx-, as they show older functions, such as deriving collective and abstract nouns. This thesis outlines some of the many explanations scholars have offered for these questions over the last two centuries and ultimately argues that a combination of these and other factors may have been involved in this change to the PIE gender system.},
  langid = {english},
  school = {University of Georgia},
  file = {C:\Users\spide\Zotero\storage\QFTP6VXN\Dreier - GENDER IN PROTO-INDO-EUROPEAN AND THE FEMININE MORPHEMES.pdf}
}

@book{duboisDictionnaireLinguistique2001,
  title = {Dictionnaire de Linguistique},
  author = {Dubois, Jean and Giacomo, Math{\'e}e and Guespin, Louis and Marcellesi, Christiane and Marcellesi, Jean-Baptiste and M{\'e}vel, Jean-Pierre},
  year = {2001},
  publisher = {Larousse},
  isbn = {‎ 978-2035320476}
}

@article{ducelEvaluationAutomatiqueBiais2024,
  title = {{{\'E}valuation automatique des biais de genre dans des mod{\`e}les de langue auto-r{\'e}gressifs}},
  author = {Ducel, Fanny and N{\'e}v{\'e}ol, Aur{\'e}lie and Fort, Kar{\"e}n},
  year = {2024},
  journal = {TALN 2024},
  abstract = {Automatically Assessing Gender Biases in Autoregressive Language Models.},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\622DG34V\Ducel et al. - Évaluation automatique des biais de genre dans des.pdf}
}

@article{ducelRechercheBiaisDans2024,
  title = {{La recherche sur les biais dans les mod{\`e}les de langue est biais{\'e}e: {\'e}tat de l'art en abyme}},
  author = {Ducel, Fanny and N{\'e}v{\'e}ol, Aur{\'e}lie and Fort, Kar{\"e}n},
  year = {2024},
  journal = {Revue TAL : traitement automatique des langues},
  volume = {3},
  number = {64},
  abstract = {Fairness and independence from bias are emerging as major quality criteria for Natural Language Processing applications. It is therefore crucial to provide a better understanding and control of these biases. This survey paper presents a review of recent research addressing the study of bias in language models. We use queries to scientific articles search engines (mainly the ACL anthology) and snowballing to identify a wide range of articles. Our analysis reveals that bias research mainly addresses methods for defining, measuring and mitigating bias. We highlight biases inherent to research on stereotypical biases in language models and conclude by calling for greater linguistic, cultural and typological diversity, and for greater transparency regarding these potentially biasing elements.},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\LLEECRWY\Ducel et al. - La recherche sur les biais dans les modèles de lan.pdf}
}

@article{EastCityBrick2022,
  title = {East of the {{City}}: "{{Brick Lane}}", {{Capitalism}}, and the {{Global Metropolis}}},
  year = {2022},
  pages = {24},
  langid = {english},
  keywords = {ali},
  file = {C:\Users\spide\Zotero\storage\R6YRBJRT\East of the City Brick Lane, Capitalism, and the Global Metropolis (, 2022).pdf}
}

@book{eckertLanguageGender2003,
  title = {Language and {{Gender}}},
  author = {Eckert, Penelope and {McConnell-Ginet}, Sally},
  year = {2003},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511791147},
  abstract = {Language and Gender is a 2003 introduction to the study of the relation between gender and language use, written by two of the leading experts in the field. It covers the main topics, beginning with a clear discussion of gender and of the resources that the linguistic system offers for the construction of social meaning. The body of the book offers broad and deep coverage of the interaction between language and social life, ranging from nuances of pronunciation to conversational dynamics to the deployment of metaphor. The discussion is organized around the contributions language makes to situated social practice rather than around linguistic structures or gender analyses. At the same time, it introduces linguistic concepts in a way that is suitable for non-linguists. It is set to become the standard textbook for courses on language and gender.}
}

@book{eckertLanguageGender2003a,
  title = {Language and {{Gender}}},
  author = {Eckert, Penelope and {McConnell-Ginet}, Sally},
  year = {2003},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  urldate = {2023-03-19},
  abstract = {Language and Gender is a 2003 introduction to the study of the relation between gender and language use, written by two of the leading experts in the field. It covers the main topics, beginning with a clear discussion of gender and of the resources that the linguistic system offers for the construction of social meaning. The body of the book offers broad and deep coverage of the interaction between language and social life, ranging from nuances of pronunciation to conversational dynamics to the deployment of metaphor. The discussion is organized around the contributions language makes to situated social practice rather than around linguistic structures or gender analyses. At the same time, it introduces linguistic concepts in a way that is suitable for non-linguists. It is set to become the standard textbook for courses on language and gender.},
  isbn = {978-1-107-02905-7}
}

@misc{edouardphilippeJONo02722017,
  title = {{{JO}} Nº 0272},
  author = {{{\'E}douard Philippe}},
  year = {2017},
  month = nov,
  journal = {texte nº 4},
  urldate = {2023-02-03}
}

@incollection{elmigerFeminisationLangueFrancaise2011,
  title = {F{\'e}minisation de La Langue Fran{\c c}aise : Une Br{\`e}ve Histoire Des Positions Politiques et Du Positionnement Linguistique},
  booktitle = {Langage, Genre et Sexualit{\'e}},
  author = {Elmiger, Daniel},
  year = {2011},
  edition = {Alexandre Duch{\^e}ne et Claudine Mo{\"i}se},
  pages = {71--89},
  publisher = {Nota bene},
  address = {Qu{\'e}bec},
  urldate = {2023-01-19},
  abstract = {L'objectif de cet ouvrage est d'examiner les liens complexes entre langage, genre et sexualit{\'e}. En adoptant un positionnement r{\'e}solument critique, qui met l'accent sur les aspects historique, politique et social des pratiques langagi{\`e}res, cet ouvrage part de l'id{\'e}e que le genre et l'identit{\'e} sexuelle sont avant tout des constructions sociales et id{\'e}ologiques qui trouvent une forme de mat{\'e}rialit{\'e}, entre autres, dans les discours et les pratiques sociales. Nous croyons que ces processus langagiers participent {\`a} la formation des diff{\'e}rences et des in{\'e}galit{\'e}s - sans toutefois en {\^e}tre l'unique cause -, qu'ils contribuent {\`a} la r{\'e}ification des cat{\'e}gories au m{\^e}me titre qu'ils permettent d'en cr{\'e}er de nouvelles},
  isbn = {978-2-89518-366-2}
}

@misc{fabianoAIActLarge2024,
  title = {{{AI Act}} and {{Large Language Models}} ({{LLMs}}): {{When}} Critical Issues and Privacy Impact Require Human and Ethical Oversight},
  shorttitle = {{{AI Act}} and {{Large Language Models}} ({{LLMs}})},
  author = {Fabiano, Nicola},
  year = {2024},
  month = apr,
  number = {arXiv:2404.00600},
  eprint = {2404.00600},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.00600},
  urldate = {2024-12-17},
  abstract = {The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {C\:\\Users\\spide\\Zotero\\storage\\YBB2Z92H\\Fabiano - 2024 - AI Act and Large Language Models (LLMs) When critical issues and privacy impact require human and e.pdf;C\:\\Users\\spide\\Zotero\\storage\\BWVGNDTX\\2404.html}
}

@article{falenskaAssessingGenderBias2021,
  title = {Assessing {{Gender Bias}} in {{Wikipedia}}: {{Inequalities}} in {{Article Titles}}},
  shorttitle = {Assessing {{Gender Bias}} in {{Wikipedia}}},
  author = {Falenska, Agnieszka and {\c C}etino{\u g}lu, {\"O}zlem},
  year = {2021},
  journal = {Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing},
  pages = {10},
  doi = {10.18653/v1/2021.gebnlp-1.9},
  urldate = {2022-08-10},
  abstract = {Potential gender biases existing in Wikipedia's content can contribute to biased behaviors in a variety of downstream NLP systems. Yet, efforts in understanding what inequalities in portraying women and men occur in Wikipedia focused so far only on biographies, leaving open the question of how often such harmful patterns occur in other topics. In this paper, we investigate gender-related asymmetries in Wikipedia titles from all domains. We assess that for only half of gender-related articles, i.e., articles with words such as women or male in their titles, symmetrical counterparts describing the same concept for the other gender (and clearly stating it in their titles) exist. Among the remaining imbalanced cases, the vast majority of articles concern sports- and social-related issues. We provide insights on how such asymmetries can influence other Wikipedia components and propose steps towards reducing the frequency of observed patterns.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\DNYXRR8W\Assessing Gender Bias in Wikipedia Inequalities in Article Titles (Falenska and Çetinoğlu, 2021).pdf}
}

@misc{fanEnglishCentricMultilingualMachine2020,
  title = {Beyond {{English-Centric Multilingual Machine Translation}}},
  author = {Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and {El-Kishky}, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and Goyal, Naman and Birch, Tom and Liptchinsky, Vitaliy and Edunov, Sergey and Grave, Edouard and Auli, Michael and Joulin, Armand},
  year = {2020},
  month = oct,
  eprint = {2010.11125},
  urldate = {2024-03-18},
  abstract = {Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric by training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open source a training dataset that covers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems of WMT. We open-source our scripts so that others may reproduce the data, evaluation, and final M2M-100 model here.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2010.11125},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{feddenGenderClassifiersConcurrent2017,
  title = {Gender and Classifiers in Concurrent Systems: {{Refining}} the Typology of Nominal Classification},
  shorttitle = {Gender and Classifiers in Concurrent Systems},
  author = {Fedden, Sebastian and Corbett, Greville G.},
  year = {2017},
  month = apr,
  journal = {Glossa: a journal of general linguistics},
  volume = {2},
  number = {1},
  issn = {2397-1835},
  doi = {10.5334/gjgl.177},
  urldate = {2024-12-01},
  abstract = {Some languages have both gender and classifiers, contrary to what was once believed possible. We use these interesting languages as a unique window onto nominal classification. They provide the impetus for a new typology, based on the degree of orthogonality of the semantic systems and the degree of difference of the forms realizing them. This nine-way typology integrates traditional gender, traditional classifiers and -- importantly -- the many recently attested phenomena lying between. Besides progress specifically in understanding nominal classification, our approach provides clarity on the wider theoretical issue of single versus concurrent featural systems.},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\9FBPT7MV\Fedden and Corbett - 2017 - Gender and classifiers in concurrent systems Refining the typology of nominal classification.pdf}
}

@article{fieldHamartiaDeathSalesman1972,
  title = {Hamartia in {{Death}} of a {{Salesman}}},
  author = {Field, B. S.},
  year = {1972},
  month = jan,
  journal = {Twentieth Century Literature},
  volume = {18},
  number = {1},
  eprint = {440691},
  eprinttype = {jstor},
  pages = {7},
  issn = {0041462X},
  doi = {10.2307/440691},
  urldate = {2021-01-26},
  langid = {english},
  keywords = {miller},
  file = {C:\Users\spide\Zotero\storage\A8ZAHFZA\Hamartia in Death of a Salesman (Field, 1972).pdf}
}

@incollection{fishmanConversationalInsecurity1980,
  title = {Conversational {{Insecurity}}},
  booktitle = {Language},
  author = {Fishman, Pamela M.},
  year = {1980},
  pages = {127--132},
  publisher = {Elsevier},
  urldate = {2023-01-19},
  abstract = {Concrete ways that women talk are frequently explainedas a result of female "personality" and socialization. This paper offers an alternative social explanation for the depiction of women as "insecure", using data from tape recording of three male-female couples in their homes. Looking at the seemingly insecure behaviour of women in actual conversational settings, their activity can be demonstrated to be embedded in the necessary work involved in producing successful interactions.},
  isbn = {978-0-08-024696-3}
}

@article{flamentLentreeThemeRheme2006,
  title = {{L'entr{\'e}e th{\`e}me/rh{\`e}me du glossaire de Comenius}},
  author = {Flament, Dani{\`e}le},
  year = {2006},
  month = dec,
  journal = {Linx},
  number = {55},
  pages = {61--71},
  issn = {0246-8743, 2118-9692},
  doi = {10.4000/linx.389},
  urldate = {2022-09-01},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\YP72JY8A\L’entrée thèmerhème du glossaire de Comenius (Flament, 2006).pdf}
}

@article{flauxPronomsIndefinisFrancais2008,
  title = {{Les pronoms ind{\'e}finis en fran{\c c}ais : une classe {\`a} (re)d{\'e}finir}},
  shorttitle = {{Les pronoms ind{\'e}finis en fran{\c c}ais}},
  author = {Flaux, Nelly},
  year = {2008},
  month = jul,
  journal = {Travaux de linguistique},
  volume = {n{$^\circ$} 56},
  number = {1},
  pages = {7--46},
  issn = {0082-6049},
  doi = {10.3917/tl.056.0007},
  urldate = {2022-11-03},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\BV7Q2L4Q\Les pronoms indéfinis en français  une classe à (re)définir (Flaux, 2008).pdf}
}

@article{flauxProposNomsCollectifs1999,
  title = {{\`A} Propos Des Noms Collectifs},
  author = {Flaux, Nelly},
  year = {1999},
  month = jul,
  journal = {Revue de linguistique romane},
  number = {63},
  pages = {471--502},
  doi = {10.5169/SEALS-400007},
  urldate = {2023-02-05},
  keywords = {a lire}
}

@misc{ForstersSymbolismRoom,
  title = {Forster's {{Symbolism}}: "{{A Room}} with a {{View}}", {{Fourth Chapter}} on {{JSTOR}}},
  urldate = {2022-01-11},
  howpublished = {https://www-jstor-org.lama.univ-amu.fr/stable/30225594?Search=yes\&resultItemClick=true\&searchText=a+room+with+a+view\&searchUri=\%2Faction\%2FdoBasicSearch\%3FQuery\%3Da\%2Broom\%2Bwith\%2Ba\%2Bview\%26so\%3Drel\&ab\_segments=0\%2Fbasic\_search\_gsv2\%2Fcontrol\&refreqid=fastly-default\%3Aaffe28e17245ced7ebcabaf13b19641b\&seq=1\#metadata\_info\_tab\_contents},
  file = {C:\Users\spide\Zotero\storage\T6WCGFGV\30225594.html}
}

@incollection{foundalisEvolutionGenderIndoEuropean2019,
  title = {Evolution of {{Gender}} in {{Indo-European Languages}}},
  booktitle = {Proceedings of the {{Twenty-Fourth Annual Conference}} of the {{Cognitive Science Society}}},
  author = {Foundalis, Harry E.},
  editor = {Gray, Wayne D. and Schunn, Christian D.},
  year = {2019},
  month = apr,
  edition = {1},
  pages = {304--309},
  publisher = {Routledge},
  doi = {10.4324/9781315782379-89},
  urldate = {2024-11-21},
  abstract = {In a recent paper, Lera Boroditsky and Lauren A. Schmidt (2000) examined the degree to which the linguistic category of grammatical gender of nouns influences people's perception of the cognitive category of biological gender, or sex. Their conclusion was that English speakers' intuitions about the gender of certain nouns (animals) correlate with the gender assigned to those nouns in languages such as German and Spanish. More important, they found that people's ideas about the putative biological gender (sex) of objects are strongly influenced by the grammatical gender of those objects in their native language. In this study I sought to reproduce Boroditsky and Schmidt's results in order to show that the interpretation they supplied is unwarranted, and that the authors conflate the concepts of biological gender (sex) and ``formal gender'', which is employed by most Indo-European languages (as opposed to ``natural gender'', in English). I compare the intuitions of 20 American monolinguals with the statistics of formal gender as it appears in 14 Indo-European languages. Moreover, I discuss the possible origin and evolution of gender in such languages, and suggest an explanation for the relation between grammatical and biological gender.},
  isbn = {978-1-315-78237-9},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\K52MLBDK\4_5920058130531620414-Copy.pdf}
}

@misc{friedrichAdversarialLearningPrivacyPreserving2019,
  title = {Adversarial {{Learning}} of {{Privacy-Preserving Text Representations}} for {{De-Identification}} of {{Medical Records}}},
  author = {Friedrich, Max and K{\"o}hn, Arne and Wiedemann, Gregor and Biemann, Chris},
  year = {2019},
  month = jun,
  number = {arXiv:1906.05000},
  eprint = {1906.05000},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-12},
  abstract = {De-identification is the task of detecting protected health information (PHI) in medical text. It is a critical step in sanitizing electronic health records (EHRs) to be shared for research. Automatic de-identification classifiers can significantly speed up the sanitization process. However, obtaining a large and diverse dataset to train such a classifier that works well across many types of medical text poses a challenge as privacy laws prohibit the sharing of raw medical records. We introduce a method to create privacy-preserving shareable representations of medical text (i.e. they contain no PHI) that does not require expensive manual pseudonymization. These representations can be shared between organizations to create unified datasets for training de-identification models. Our representation allows training a simple LSTM-CRF de-identification model to an F1 score of 97.4\%, which is comparable to a strong baseline that exposes private information in its representation. A robust, widely available de-identification classifier based on our representation could potentially enable studies for which de-identification would otherwise be too costly.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\spide\Zotero\storage\P85JW3SK\Friedrich et al. - 2019 - Adversarial Learning of Privacy-Preserving Text Re.pdf}
}

@article{friedrichHowYourThesis2023,
  title = {``{{How}} Is Your Thesis Going?''--{{Ph}}.{{D}}. Students' Perspectives on Mental Health and Stress in Academia},
  shorttitle = {``{{How}} Is Your Thesis Going?},
  author = {Friedrich, Julian and Bareis, Anna and Bross, Moritz and B{\"u}rger, Zo{\'e} and Cort{\'e}s Rodr{\'i}guez, {\'A}lvaro and Effenberger, Nina and Kleinhansl, Markus and Kremer, Fabienne and Schr{\"o}der, Cornelius},
  editor = {Almhdawi, Khader Ahmad},
  year = {2023},
  month = jul,
  journal = {PLOS ONE},
  volume = {18},
  number = {7},
  pages = {e0288103},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0288103},
  urldate = {2023-08-12},
  abstract = {Mental health issues among Ph.D. students are prevalent and on the rise, with multiple studies showing that Ph.D. students are more likely to experience symptoms of mental health-related issues than the general population. However, the data is still sparse. This study aims to investigate the mental health of 589 Ph.D. students at a public university in Germany using a mixed quantitative and qualitative approach. We administered a web-based self-report questionnaire to gather data on the mental health status, investigated mental illnesses such as depression and anxiety, and potential areas for improvement of the mental health and well-being of Ph.D. students. Our results revealed that one-third of the participants were above the cut-off for depression and that factors such as perceived stress and self-doubt were prominent predictors of the mental health status of Ph.D. students. Additionally, we found job insecurity and low job satisfaction to be predictors of stress and anxiety. Many participants in our study reported working more than full-time while being employed part-time. Importantly, deficient supervision was found to have a negative effect on Ph.D. students' mental health. The study's results are in line with those of earlier investigations of mental health in academia, which likewise reveal significant levels of depression and anxiety among Ph.D. students. Overall, the findings provide a greater knowledge of the underlying reasons and potential interventions required for advancing the mental health problems experienced by Ph.D. students. The results of this research can guide the development of effective strategies to support the mental health of Ph.D. students.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\RLNGLQTI\“How is your thesis going”–Ph.D. students’ perspectives on mental health and stress in academia (Friedrich et al., 2023).pdf}
}

@article{gabrielNeutralisingLinguisticSexism2018,
  title = {Neutralising Linguistic Sexism: {{Promising}} but Cumbersome?},
  shorttitle = {Neutralising Linguistic Sexism},
  author = {Gabriel, Ute and Gygax, Pascal M. and Kuhn, Elisabeth A.},
  year = {2018},
  month = aug,
  journal = {Group Processes \& Intergroup Relations},
  volume = {21},
  number = {5},
  pages = {844--858},
  publisher = {SAGE Publications Ltd},
  issn = {1368-4302},
  doi = {10.1177/1368430218771742},
  urldate = {2024-08-28},
  abstract = {The generic use of grammatically (or lexically) gender-marked nouns and pronouns (GM) to refer to women and men in Indo-European languages has been criticised as gender-asymmetric since the 1970s. Two main strategies for eliminating asymmetry have been suggested: visibility by feminisation and de-gendering by neutralisation. Feminisation strategies seek to contribute to women's visibility in discourse by explicitly and symmetrically referring to women and men, thus continuing to highlight gender boundaries. In contrast, neutralisation strategies downplay gender boundaries by promoting the use of unmarked nouns and pronouns. We discuss feminisation and neutralisation strategies and review: (a) evidence (from our own work and that of others) on the effect of neutralisation and feminisation strategies on speakers' and readers' mental representations of gender and associated behaviours, and (b) evidence on individual variables facilitating and hampering the successful implementation of a less asymmetric---and therefore more gender-fair---language use. Based on this review, we suggest, in particular, to use feminisation strategies in contexts that are already gendered, and to use neutralisation strategies in nongendered ones (hence keeping the context gender-neutral).},
  langid = {english}
}

@misc{gallegosBiasFairnessLarge2024,
  title = {Bias and {{Fairness}} in {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Bias and {{Fairness}} in {{Large Language Models}}},
  author = {Gallegos, Isabel O. and Rossi, Ryan A. and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K.},
  year = {2024},
  month = jul,
  number = {arXiv:2309.00770},
  eprint = {2309.00770},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-19},
  abstract = {Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {C:\Users\spide\Zotero\storage\7LQGZSY3\Gallegos et al. - 2024 - Bias and Fairness in Large Language Models A Surv.pdf}
}

@article{gallienneQuelquesObservationsNotion2023,
  title = {{Quelques observations sur la notion de biais dans les mod{\`e}les de langue}},
  author = {Gallienne, Romane and Poibeau, Thierry},
  year = {2023},
  journal = {Actes de CORIA-TALN 2023. Actes de la 30e Conf{\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN), volume 3 : prises de position en TAL},
  pages = {1--13},
  abstract = {This article revisits the notion of bias in language models. We show, thanks to examples taken from generative models for French (related to the GPT family), that it is easy to direct, from precise prompts, the generated texts towards potentially harmful results (including stereotypes, bias, etc.). But the actions to be taken from there are not neutral : debiasing a model has a positive aspect but can also pose other problems (what to debias ? Who could or should decide ? Following what norm and what rules ?). Finally, we show that the questions raised are not only technological, but above all social, and linked to the context of use of the targeted applications.},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\SBYNH8JH\Gallienne and Poibeau - Quelques observations sur la notion de biais dans .pdf}
}

@misc{ganApplicationLLMAgents2024,
  title = {Application of {{LLM Agents}} in {{Recruitment}}: {{A Novel Framework}} for {{Resume Screening}}},
  shorttitle = {Application of {{LLM Agents}} in {{Recruitment}}},
  author = {Gan, Chengguang and Zhang, Qinghao and Mori, Tatsunori},
  year = {2024},
  month = aug,
  number = {arXiv:2401.08315},
  eprint = {2401.08315},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.08315},
  urldate = {2024-12-17},
  abstract = {The automation of resume screening is a crucial aspect of the recruitment process in organizations. Automated resume screening systems often encompass a range of natural language processing (NLP) tasks. This paper introduces a novel Large Language Models (LLMs) based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes. Our framework is distinct in its ability to efficiently summarize and grade each resume from a large dataset. Moreover, it utilizes LLM agents for decision-making. To evaluate our framework, we constructed a dataset from actual resumes and simulated a resume screening process. Subsequently, the outcomes of the simulation experiment were compared and subjected to detailed analysis. The results demonstrate that our automated resume screening framework is 11 times faster than traditional manual methods. Furthermore, by fine-tuning the LLMs, we observed a significant improvement in the F1 score, reaching 87.73{\textbackslash}\%, during the resume sentence classification phase. In the resume summarization and grading phase, our fine-tuned model surpassed the baseline performance of the GPT-3.5 model. Analysis of the decision-making efficacy of the LLM agents in the final offer stage further underscores the potential of LLM agents in transforming resume screening processes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\HFYCFHZT\\Gan et al. - 2024 - Application of LLM Agents in Recruitment A Novel Framework for Resume Screening.pdf;C\:\\Users\\spide\\Zotero\\storage\\VLNRZFAS\\2401.html}
}

@article{gaoScalingEvaluatingSparse,
  title = {Scaling and Evaluating Sparse Autoencoders},
  author = {Gao, Leo and Goh, Gabriel and Sutskever, Ilya},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\9V2EDCM2\Gao et al. - Scaling and evaluating sparse autoencoders.pdf}
}

@article{gardelleFiveCrewHow2016,
  title = {Five Crew, How Many Clergy : Pourquoi Certains Noms Collectifs Peuvent-Ils Servir {\`a} Nommer Des Membres ?},
  shorttitle = {Five Crew, How Many Clergy},
  author = {Gardelle, Laure},
  year = {2016},
  journal = {Anglophonia},
  number = {22},
  issn = {1278-3331, 2427-0466},
  doi = {10.4000/anglophonia.1028},
  urldate = {2022-11-13},
  abstract = {Un nom collectif est g{\'e}n{\'e}ralement d{\'e}fini comme un nom qui, au singulier, implique une pluralit{\'e} de membres ; ainsi crew au sens d'{\'e}quipage. Mais certains de ces noms peuvent {\'e}galement d{\'e}signer des membres, lorsqu'ils sont employ{\'e}s comme pluriels non fl{\'e}chis (ex. these crew) ; quelques-uns admettent m{\^e}me le d{\'e}nombrement << un >> (ex. one crew pour un membre d'{\'e}quipage). La pr{\'e}sente {\'e}tude s'int{\'e}resse {\`a} ces emplois pluriels, peu {\'e}tudi{\'e}s {\`a} ce jour, et qui concernent {\'e}galement des noms ind{\'e}nombrables de supercat{\'e}gories collectives (ex. (these) livestock). Apr{\`e}s une description des donn{\'e}es obtenues {\`a} partir de deux corpus d'anglais am{\'e}ricain, l'article montre qu'il ne s'agit pas d'emplois collectifs ; ce que d{\'e}note le nom, ce sont des membres d'une classe (ex. pour crew, d'une cat{\'e}gorie professionnelle), et le fait que ces membres sont, dans le m{\^e}me temps, typiquement repr{\'e}sent{\'e}s comme faisant partie d'un groupe. Pour les noms originellement collectifs d{\'e}nombrables, il y a changement de d{\'e}notation (d'un tout {\`a} des membres d'une cat{\'e}gorie socio-professionnelle) ; pour les noms qui d{\'e}notent des supercat{\'e}gories collectives, l'emploi pluriel indique seulement plus grande saillance des unit{\'e}s qui composent l'ensemble. Dans les deux cas, les unit{\'e}s ont en commun d'{\^e}tre peu individualis{\'e}es, car d{\'e}finies par leur appartenance {\`a} la classe, {\`a} partir d'une pluralit{\'e}. Par cons{\'e}quent, two police, par exemple, n'est pas toujours interchangeable avec two policemen. Se pose alors la question du terme qui peut nommer ces usages ; celui de << pluriel interne >> est rejet{\'e}, au profit d'<< agr{\'e}gat >>.},
  keywords = {ling. angl.,lu,noms collectifs}
}

@article{gardelleLetHerRain2015,
  title = {Let {{Her Rain}}, {{She}}'s {{Snowing Pretty Good}}: {{The Use}} of {{Feminine Pronouns}} with {{Weather Verbs}} in {{Colloquial English}}},
  shorttitle = {Let {{Her Rain}}, {{She}}'s {{Snowing Pretty Good}}},
  author = {Gardelle, Laure},
  year = {2015},
  month = jan,
  journal = {Folia Linguistica},
  volume = {49},
  number = {2},
  issn = {0165-4004, 1614-7308},
  doi = {10.1515/flin-2015-0013},
  urldate = {2023-01-19},
  abstract = {This article investigates a phenomenon which, though marginal, is important to linguistic theory: the use of feminine pronouns with weather verbs in contemporary colloquial English (e.g. She's snowing pretty good). Such uses, mentioned in a few studies only, with examples mostly drawn from fiction, have never been analysed in detail, despite a wide literature on the use of he/she for inanimate reference. The aim of the study is first to get a better understanding of the phenomenon, based on non-fictional utterances. It is shown that the data must be divided into two subsets: cases of anaphora, in which she signals personification, and less referential uses, in which the feminine pronoun emphasizes emotional involvement. This latter set is particularly important for gender research: it confirms that this emotional value of the feminine pronoun, which has been noted for inanimate reference, exists even when there is no clearly identifiable referent. The article then looks into the motivations behind the use of animate pronouns with weather verbs, taking into account the long-standing debate over the status of it in the same contexts in more formal registers. It proposes that in a number of cases in which she does not have a textual antecedent, the pronoun does not have an actual referent, but that owing to three converging factors, a slight degree of referentiality is projected on it.}
}

@inbook{gardellePronounActivismPower2023,
  title = {Pronoun Activism and the Power of Animacy},
  booktitle = {The {{Routledge Handbook}} of {{Pronouns}}},
  author = {Gardelle, Laure},
  year = {2023},
  month = oct,
  edition = {1},
  pages = {394--408},
  publisher = {Routledge},
  address = {New York},
  doi = {10.4324/9781003349891-32},
  urldate = {2024-12-02},
  collaborator = {Paterson, Laura L.},
  isbn = {978-1-003-34989-1},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\2W83P35B\Gardelle - 2023 - Pronoun activism and the power of animacy.pdf}
}

@article{glimEarlyERPIndices2023,
  title = {Early {{ERP}} Indices of Gender-Biased Processing Elicited by Generic Masculine Role Nouns and the Feminine--Masculine Pair Form},
  author = {Glim, Sarah and K{\"o}rner, Anita and H{\"a}rtl, Holden and Rummer, Ralf},
  year = {2023},
  month = jul,
  journal = {Brain and Language},
  volume = {242},
  pages = {105290},
  issn = {0093934X},
  doi = {10.1016/j.bandl.2023.105290},
  urldate = {2025-01-07},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\TDZIUQQV\Glim et al. - 2023 - Early ERP indices of gender-biased processing elicited by generic masculine role nouns and the femin.pdf}
}

@article{glimGenericMasculineRole2024,
  title = {Generic Masculine Role Nouns Interfere with the Neural Processing of Female Referents: Evidence from the {{P600}}},
  shorttitle = {Generic Masculine Role Nouns Interfere with the Neural Processing of Female Referents},
  author = {Glim, Sarah and K{\"o}rner, Anita and Rummer, Ralf},
  year = {2024},
  month = nov,
  journal = {Language, Cognition and Neuroscience},
  volume = {39},
  number = {10},
  pages = {1366--1375},
  issn = {2327-3798, 2327-3801},
  doi = {10.1080/23273798.2024.2387230},
  urldate = {2025-01-07},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\53AUPQTZ\Glim et al. - 2024 - Generic masculine role nouns interfere with the neural processing of female referents evidence from.pdf}
}

@incollection{gomilaLanguageThoughtNeoWhorfian2015,
  title = {Language and {{Thought}}: {{The Neo-Whorfian Hypothesis}}},
  booktitle = {International {{Encyclopedia}} of the {{Social}} \& {{Behavioral Sciences}}},
  author = {Gomila, Antoni},
  year = {2015},
  edition = {Second edition},
  pages = {293--299},
  publisher = {Elsevier},
  address = {Amsterdam},
  abstract = {During the last decade, a renewed interest in linguistic relativism has given rise to an exponentially growing body of literature concerning the influence of language on cognition. The current concern is not whether language influences cognition, but rather how much, and to what extent. After briefly characterizing Whorf 's linguistic relativism, the current neo-Whorfian hypothesis is introduced, and a selection of the most relevant and influential studies is reviewed. The conclusion contends that the influence of language on cognition is not homogeneous, but depends on how much recoding is brought about during linguistic development.},
  isbn = {978-0-08-097086-8}
}

@article{goodheartEmmaJaneAustens2008,
  title = {Emma: {{Jane Austen}}'s {{Errant Heroine}}},
  author = {Goodheart, Eugene},
  year = 2008,
  journal = {Johns Hopkins University Press},
  volume = {116},
  pages = {589--604},
  doi = {10.1353/sew.0.0087},
  langid = {english},
  keywords = {jane austen},
  file = {C:\Users\spide\Documents\PDF recherches\Emma Jane Austen's Errant Heroine (Goodheart, 2008).pdf}
}

@article{goudieFabricatingIdeologyClothing1998,
  title = {Fabricating {{Ideology}}: {{Clothing}}, {{Culture}}, and {{Colonialism}} in {{Melville}}'s {{Typee}}},
  author = {Goudie, S. X.},
  year = 1998,
  journal = {Wayne State University Press},
  volume = {40},
  number = {2},
  eprint = {23124331},
  eprinttype = {jstor},
  pages = {28},
  langid = {english},
  keywords = {melville},
  file = {C:\Users\spide\OneDrive\Documents\COURS S4\- APPROFONDISSEMENT\Littérature\PDF Recherches\Fabricating Ideology Clothing, Culture, and Colonialism in Melville's Typee (Goudie, 1998).pdf}
}

@misc{graeloGraeloWikipedia2023,
  title = {Graelo/Wikipedia},
  author = {{graelo}},
  year = {2023}
}

@article{grantFailureLanguageMelvilles1982,
  title = {The {{Failure}} of {{Language}} in {{Melville}}'s "{{Typee}}"},
  author = {Grant, J. Kerry},
  year = 1982,
  journal = {Modern Language Studies},
  volume = {12},
  number = {2},
  eprint = {3194476},
  eprinttype = {jstor},
  pages = {9},
  issn = {00477729},
  doi = {10.2307/3194476},
  urldate = {2021-01-06},
  langid = {english},
  keywords = {melville},
  file = {C:\Users\spide\Documents\COURS S4\- APPROFONDISSEMENT\Littérature\PDF Recherches\The Failure of Language in Melville's Typee (Grant, 1982).pdf}
}

@misc{grattafioriLlama3Herd2024,
  title = {The {{Llama}} 3 {{Herd}} of {{Models}}},
  author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and {Al-Dahle}, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and {Garcia-Olano}, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzm{\'a}n, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and van der Linde, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and {El-Arini}, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and {Rantala-Yeary}, Lauren and van der Maaten, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and de Oliveira, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and {\c C}elebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, V{\'i}tor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
  year = {2024},
  month = nov,
  number = {arXiv:2407.21783},
  eprint = {2407.21783},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.21783},
  urldate = {2025-01-24},
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\spide\\Zotero\\storage\\TJRV9AJX\\Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf;C\:\\Users\\spide\\Zotero\\storage\\GFYSFVSK\\2407.html}
}

@article{greenMainClausePhenomena1976,
  title = {Main {{Clause Phenomena}} in {{Subordinate Clauses}}},
  author = {Green, Georgia M.},
  year = {1976},
  month = jun,
  journal = {Language},
  volume = {52},
  number = {2},
  eprint = {412566},
  eprinttype = {jstor},
  pages = {17},
  issn = {00978507},
  doi = {10.2307/412566},
  urldate = {2022-08-27},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\P5E8PK98\Main Clause Phenomena in Subordinate Clauses (Green, 1976).pdf}
}

@article{grossClassificationSemantiqueCollectifs2011,
  title = {Classification S{\'e}mantique Des Collectifs Humains},
  author = {Gross, Gaston},
  year = {2011},
  journal = {Cahiers de lexicologie},
  volume = {1},
  number = {98},
  pages = {65--81},
  keywords = {lu,noms collectifs,noms humains}
}

@article{grossPeddlerPioneerDeath1965,
  title = {Peddler {{And Pioneer In}} {{{\emph{Death Of A Salesman}}}}},
  author = {Gross, Barry},
  year = {1965},
  month = feb,
  journal = {Modern Drama},
  volume = {7},
  number = {4},
  pages = {5},
  issn = {0026-7694, 1712-5286},
  doi = {10.3138/md.7.4.405},
  urldate = {2021-01-26},
  langid = {english},
  keywords = {miller},
  file = {C:\Users\spide\Zotero\storage\R5PQCXEU\Peddler And Pioneer In Death Of A Salesman (Gross, 1965).pdf}
}

@incollection{grossStatutSyntaxiqueSubstantifs2009,
  title = {Sur Le Statut Syntaxique Des Substantifs Humains},
  booktitle = {Des Topo{\"i} {\`a} La Th{\'e}orie Des St{\'e}r{\'e}otypes En Passant Par La Polyphonie et l'argumentation Dans La Langue. {{Hommages}} {\`a} {{Jean-Claude Anscombre}}},
  author = {Gross, Gaston},
  year = {2009},
  pages = {27--41},
  publisher = {Universit{\'e} de Savoie},
  urldate = {2022-11-23},
  isbn = {978-2-915797-47-3},
  keywords = {lu,noms humains}
}

@article{gstohlScandinaviaSwitzerlandSmall2002,
  title = {Scandinavia and {{Switzerland}}: Small, Successful and Stubborn towards the {{EU}}},
  shorttitle = {Scandinavia and {{Switzerland}}},
  author = {Gst{\"o}hl, Sieglinde},
  year = {2002},
  month = jan,
  journal = {Journal of European Public Policy},
  volume = {9},
  number = {4},
  pages = {23},
  issn = {1350-1763, 1466-4429},
  doi = {10.1080/13501760210152420},
  urldate = {2022-08-11},
  abstract = {Economic theory of integration expects small states and highly industrialized states to be more likely to integrate than larger or less advanced countries. Why then, did Norway, Sweden and Switzerland choose for a long time not to join the European Union? Existing political economy approaches cannot fully explain this stubbornness because they neglect the `hidden' impact of national identities. Constructivist approaches, in turn, offer insights on identity-related variables but fail to assess tangible bene ts. This article argues that economic incentives for EU membership coexist with and are often dominated by domestic and geo-historical constraints. Hence, both material interests and ideational factors are necessary to explain reluctant integration policies.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\E73CJWGS\Scandinavia and Switzerland small, successful and stubborn towards the EU (Gstöhl, 2002).pdf}
}

@misc{guptaSociodemographicBiasLanguage2024,
  title = {Sociodemographic {{Bias}} in {{Language Models}}: {{A Survey}} and {{Forward Path}}},
  shorttitle = {Sociodemographic {{Bias}} in {{Language Models}}},
  author = {Gupta, Vipul and Venkit, Pranav Narayanan and Wilson, Shomir and Passonneau, Rebecca J.},
  year = {2024},
  month = aug,
  number = {arXiv:2306.08158},
  eprint = {2306.08158},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.08158},
  urldate = {2024-12-17},
  abstract = {Sociodemographic bias in language models (LMs) has the potential for harm when deployed in real-world settings. This paper presents a comprehensive survey of the past decade of research on sociodemographic bias in LMs, organized into a typology that facilitates examining the different aims: types of bias, quantifying bias, and debiasing techniques. We track the evolution of the latter two questions, then identify current trends and their limitations, as well as emerging techniques. To guide future research towards more effective and reliable solutions, and to help authors situate their work within this broad landscape, we conclude with a checklist of open questions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\spide\\Zotero\\storage\\33CDABGU\\Gupta et al. - 2024 - Sociodemographic Bias in Language Models A Survey and Forward Path.pdf;C\:\\Users\\spide\\Zotero\\storage\\A3MXW7KB\\2306.html}
}

@article{guptaUnveilingBlackBox,
  title = {Unveiling the {{Black Box}}: {{Causal Inference}} and {{Feature Analysis}} in {{Fine-Tuned Language Models Using Sparse Autoencoders}}},
  author = {Gupta, Rini and Sica, Sean},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\Q79QB3GA\Gupta and Sica - Unveiling the Black Box Causal Inference and Feat.pdf}
}

@article{gygaxExploringOnsetMaleBiased2019,
  title = {Exploring the {{Onset}} of a {{Male-Biased Interpretation}} of {{Masculine Generics Among French Speaking Kindergarten Children}}},
  author = {Gygax, Pascal Mark and Schoenhals, Lucie and L{\'e}vy, Arik and Luethold, Patrick and Gabriel, Ute},
  year = {2019},
  month = may,
  journal = {Frontiers in Psychology},
  volume = {10},
  pages = {1225},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.01225},
  urldate = {2024-12-18},
  abstract = {In French, and other gender marked languages, there are two ways to interpret a grammatical masculine form when used to refer to social roles or occupations [e.g., les magiciens (the magiciansmasculine)]. It can refer to a group composed of only men (specific use of the masculine form), or one composed of both women and men (generic use). Studies of adults revealed that the rule that masculine forms can be interpreted as inclusive of either gender is not readily applied. To gain a better understanding of the processes shaping this phenomenon, we present a follow-up study (N = 52) to L{\'e}vy et al. (2016) to explore how French-speaking kindergarten children (3--5 years of age) resolve the semantic ambiguity of the grammatical masculine form when presented with role or occupation nouns. In a paradigm where participants' gazes were monitored, children were presented with pictures of a pair of two boys and a pair of one girl and one boy and were prompted to Look at the [role nounmasculine plural form]. First, the results suggest a stereotype effect in that children more strongly directed their gaze toward the boy-boy picture for stereotypical male role nouns, but toward the girl-boy picture for stereotypical female role nouns. Second, in the non-stereotypical/neutral condition we did not find an indication of any own-sex preference (as in L{\'e}vy et al., 2016), but of an influence of the role nouns' grammatical gender, in that children more strongly directed their gaze toward boy-boy pictures than toward girl-boy pictures. We suggest that a specific interpretation of masculine forms might already start to emerge between 3 and 5 years of age, while gender stereotypes are still activated.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\DSNESPKS\Gygax et al. - 2019 - Exploring the Onset of a Male-Biased Interpretation of Masculine Generics Among French Speaking Kind.pdf}
}

@article{gygaxGenericallyIntendedSpecifically2008,
  title = {Generically Intended, but Specifically Interpreted: {{When}} Beauticians, Musicians, and Mechanics Are All Men},
  shorttitle = {Generically Intended, but Specifically Interpreted},
  author = {Gygax, Pascal and Gabriel, Ute and Sarrasin, Oriane and Oakhill, Jane and Garnham, Alan},
  year = {2008},
  month = apr,
  journal = {Language and Cognitive Processes},
  volume = {23},
  number = {3},
  pages = {464--485},
  issn = {0169-0965, 1464-0732},
  doi = {10.1080/01690960701702035},
  urldate = {2024-12-17},
  langid = {english}
}

@article{gygaxGenericallyIntendedSpecifically2008a,
  title = {Generically Intended, but Specifically Interpreted: {{When}} Beauticians, Musicians, and Mechanics Are All Men},
  shorttitle = {Generically Intended, but Specifically Interpreted},
  author = {Gygax, Pascal and Gabriel, Ute and Sarrasin, Oriane and Oakhill, Jane and Garnham, Alan},
  year = {2008},
  month = apr,
  journal = {Language and Cognitive Processes},
  volume = {23},
  number = {3},
  pages = {464--485},
  issn = {0169-0965, 1464-0732},
  doi = {10.1080/01690960701702035},
  urldate = {2025-01-07},
  langid = {english}
}

@article{gygaxGrammaticalRulesAre2009,
  title = {Some Grammatical Rules Are More Difficult than Others: {{The}} Case of the Generic Interpretation of the Masculine},
  shorttitle = {Some Grammatical Rules Are More Difficult than Others},
  author = {Gygax, Pascal and Gabriel, Ute and Sarrasin, Oriane and Oakhill, Jane and Garnham, Alan},
  year = {2009},
  month = jun,
  journal = {European Journal of Psychology of Education},
  volume = {24},
  number = {2},
  pages = {235--246},
  issn = {0256-2928, 1878-5174},
  doi = {10.1007/BF03173014},
  urldate = {2024-12-17},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\S7A3FXCC\Gygax et al. - 2009 - Some grammatical rules are more difficult than others The case of the generic interpretation of the.pdf}
}

@article{gygaxGrammaticalRulesAre2009a,
  title = {Some Grammatical Rules Are More Difficult than Others: {{The}} Case of the Generic Interpretation of the Masculine},
  shorttitle = {Some Grammatical Rules Are More Difficult than Others},
  author = {Gygax, Pascal and Gabriel, Ute and Sarrasin, Oriane and Oakhill, Jane and Garnham, Alan},
  year = {2009},
  month = jun,
  journal = {European Journal of Psychology of Education},
  volume = {24},
  number = {2},
  pages = {235--246},
  issn = {0256-2928, 1878-5174},
  doi = {10.1007/BF03173014},
  urldate = {2025-01-07},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\DW6Q96LM\Gygax et al. - 2009 - Some grammatical rules are more difficult than others The case of the generic interpretation of the.pdf}
}

@article{gygaxMasculineFormIts2012,
  title = {The Masculine Form and Its Competing Interpretations in {{French}}: {{When}} Linking Grammatically Masculine Role Names to Female Referents Is Difficult},
  shorttitle = {The Masculine Form and Its Competing Interpretations in {{French}}},
  author = {Gygax, Pascal and Gabriel, Ute and L{\'e}vy, Arik and Pool, Eva and Grivel, Marjorie and Pedrazzini, Elena},
  year = {2012},
  month = jun,
  journal = {Journal of Cognitive Psychology},
  volume = {24},
  number = {4},
  pages = {395--408},
  publisher = {Routledge},
  issn = {2044-5911},
  doi = {10.1080/20445911.2011.642858},
  urldate = {2024-08-28},
  abstract = {Using a word association paradigm we examined the extent to which readers can overcome the specific interpretation of the grammatical masculine form in French when instructed to embrace its generic meaning. In two experiments participants were to decide whether a person introduced by a kinship term (e.g., aunt) could be part of a group represented by a role name (e.g., musicians). After the completion of the first half of the experiment, participants were explicitly reminded about the generic interpretation and use of the masculine form. Although the reminder resulted in some level of generic interpretation, there were still strong traces of the specific interpretation in the response times, regardless of participants' inhibition capacities (Experiment 1). Adding a supplementary constraint by exposing readers to distractor role names in feminine forms (Experiment 2) did not reveal any different effects. The results indicated that although readers can be motivated to elaboratively activate the generic interpretation of the masculine form, the latter can not completely overrule a more passively activated specific one.},
  keywords = {Gender representation,Inhibition processes,Masculine bias,Text comprehension},
  file = {C\:\\Users\\spide\\Zotero\\storage\\8DWKED3Z\\The_masculine_form_and_its_competing_interpretatio-1.pdf;C\:\\Users\\spide\\Zotero\\storage\\FUX5JYM2\\The_masculine_form_and_its_competing_interpretatio-1.pdf}
}

@inproceedings{habashAutomaticGenderIdentification2019,
  title = {Automatic {{Gender Identification}} and {{Reinflection}} in {{Arabic}}},
  booktitle = {Proceedings of the {{First Workshop}} on {{Gender Bias}} in {{Natural Language Processing}}},
  author = {Habash, Nizar and Bouamor, Houda and Chung, Christine},
  year = {2019},
  pages = {155--165},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-3822},
  urldate = {2022-10-16},
  abstract = {The impressive progress in many Natural Language Processing (NLP) applications has increased the awareness of some of the biases these NLP systems have with regards to gender identities. In this paper, we propose an approach to extend biased single-output genderblind NLP systems with gender-specific alternative reinflections. We focus on Arabic, a gender-marking morphologically rich language, in the context of machine translation (MT) from English, and for first-personsingular constructions only. Our contributions are the development of a system-independent gender-awareness wrapper, and the building of a corpus for training and evaluating firstperson-singular gender identification and reinflection in Arabic. Our results successfully demonstrate the viability of this approach with 8\% relative increase in BLEU score for firstperson-singular feminine, and 5.3\% comparable increase for first-person-singular masculine on top of a state-of-the-art gender-blind MT system on a held-out test set.},
  keywords = {gender rewriting,lu}
}

@misc{haddadManuelDecritureInclusive2016,
  title = {Manuel d'{\'e}criture Inclusive},
  author = {Haddad, Rapha{\"e}l},
  year = {2016},
  publisher = {Mots-Cl{\'e}s},
  urldate = {2023-01-19}
}

@article{hagendorffEthicsAIEthics2020,
  title = {The {{Ethics}} of {{AI Ethics}}: {{An Evaluation}} of {{Guidelines}}},
  shorttitle = {The {{Ethics}} of {{AI Ethics}}},
  author = {Hagendorff, Thilo},
  year = {2020},
  month = mar,
  journal = {Minds and Machines},
  volume = {30},
  number = {1},
  pages = {99--120},
  issn = {0924-6495, 1572-8641},
  doi = {10.1007/s11023-020-09517-8},
  urldate = {2024-12-16},
  abstract = {Current advances in research, development and application of artificial intelligence (AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a number of ethics guidelines have been released in recent years. These guidelines comprise normative principles and recommendations aimed to harness the ``disruptive'' potentials of new AI technologies. Designed as a semi-systematic evaluation, this paper analyzes and compares 22 guidelines, highlighting overlaps but also omissions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also examine to what extent the respective ethical principles and values are implemented in the practice of research, development and application of AI systems---and how the effectiveness in the demands of AI ethics can be improved.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\BENQ35BS\Hagendorff - 2020 - The Ethics of AI Ethics An Evaluation of Guidelines.pdf}
}

@article{hallEffectStudentColor1994,
  title = {The {{Effect}} of {{Student Color Coding}} of {{Knowledge Maps}} and {{Test Anxiety}} on {{Student Learning}}},
  author = {Hall, Richard H. and {Sidio-hall}, Maureen A.},
  year = {1994},
  month = jul,
  journal = {The Journal of Experimental Education},
  volume = {62},
  number = {4},
  pages = {11},
  issn = {0022-0973, 1940-0683},
  doi = {10.1080/00220973.1994.9944136},
  urldate = {2022-07-12},
  abstract = {Students studied a 1,500-word passage in the form of a knowledge map or traditional text. Within the knowledge map and traditional text groups, half of the students studied information that was already color coded, and half were required to color code the information themselves. Those who studied from knowledge maps re called significantly more than those who studied traditional text. In addition, a mar ginally significant (p = .08) color-coding and test-anxiety interaction was found. For students who studied materials that were pre-color coded, there was virtually no difference between the free-recall performance of high- and low-test-anxiety groups. By contrast, among students who color coded their own materials, those low in test anxiety performed substantially better on the free-recall test than those high in test anxiety. These results support previous research findings indicating the effectiveness of knowledge maps. In addition, they point to the important role of test anxiety in mediating outcomes associated with demanding and novel tasks, such as student generated color coding.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\N8FM5KN7\The Effect of Student Color Coding of Knowledge Maps and Test Anxiety on Student Learning (Hall and Sidio-hall, 1994).pdf}
}

@misc{hallSystematicStudyBias2022,
  title = {A {{Systematic Study}} of {{Bias Amplification}}},
  author = {Hall, Melissa and van der Maaten, Laurens and Gustafson, Laura and Jones, Maxwell and Adcock, Aaron},
  year = {2022},
  month = oct,
  number = {arXiv:2201.11706},
  eprint = {2201.11706},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.11706},
  urldate = {2024-12-17},
  abstract = {Recent research suggests that predictions made by machine-learning models can amplify biases present in the training data. When a model amplifies bias, it makes certain predictions at a higher rate for some groups than expected based on training-data statistics. Mitigating such bias amplification requires a deep understanding of the mechanics in modern machine learning that give rise to that amplification. We perform the first systematic, controlled study into when and how bias amplification occurs. To enable this study, we design a simple image-classification problem in which we can tightly control (synthetic) biases. Our study of this problem reveals that the strength of bias amplification is correlated to measures such as model accuracy, model capacity, model overconfidence, and amount of training data. We also find that bias amplification can vary greatly during training. Finally, we find that bias amplification may depend on the difficulty of the classification task relative to the difficulty of recognizing group membership: bias amplification appears to occur primarily when it is easier to recognize group membership than class membership. Our results suggest best practices for training machine-learning models that we hope will help pave the way for the development of better mitigation strategies. Code can be found at https://github.com/facebookresearch/cv\_bias\_amplification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\spide\\Zotero\\storage\\6BA49LXS\\Hall et al. - 2022 - A Systematic Study of Bias Amplification.pdf;C\:\\Users\\spide\\Zotero\\storage\\D8BNFYC5\\2201.html}
}

@misc{HandbookLatentSemantic,
  title = {Handbook of {{Latent Semantic Analysis}}},
  journal = {Routledge \& CRC Press},
  urldate = {2022-11-29},
  abstract = {The Handbook of Latent Semantic Analysis is the authoritative reference for the theory behind Latent Semantic Analysis (LSA), a burgeoning mathematical method used to analyze how words make meaning, with the desired outcome to program machines to understand human commands via natural language rather than strict programming protocols. The first book of its kind to deliver such a comprehensive analysis, this volume explores every area of the method and combines theoretical implications as well as},
  howpublished = {https://www.routledge.com/Handbook-of-Latent-Semantic-Analysis/Landauer-McNamara-Dennis-Kintsch/p/book/9781138004191},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\T6HL8JRV\9781138004191.html}
}

@article{hanProcessingBoundvariableSingular2022,
  title = {Processing Bound-Variable Singular {\emph{They}}},
  author = {Han, Chung-hye and Moulton, Keir},
  year = {2022},
  month = sep,
  journal = {Canadian Journal of Linguistics/Revue canadienne de linguistique},
  volume = {67},
  number = {3},
  pages = {267--301},
  issn = {0008-4131, 1710-1115},
  doi = {10.1017/cnj.2022.30},
  urldate = {2022-10-07},
  abstract = {Abstract                            The pronouns               they/them/their               are readily available with a singular interpretation as bound variables (Balhorn 2004, Bjorkman 2017). Referential interpretations are possible, but subject to pragmatic considerations and changes in progress (Bjorkman 2017, Conrod 2019, Konnelly and Cowper 2020). In a series of experiments, we tested differences between bound and referential singular               they               in acceptability and incremental processing, asking whether bound               they               is sensitive to the gender of its antecedent, as referential               they               is (Doherty and Conklin 2017, Ackerman 2018, Ackerman et al. 2018, Conrod 2019). We found that bound singular               they               has an advantage over referential singular               they               in acceptability, even when the antecedent is gendered. In processing, however, bound-variable singular               they               showed a reading time advantage over referential singular               they               only with gendered antecedents. We evaluate these results against existing formal linguistic theories of singular               they               implemented within psycholinguistic models of pronoun processing. We submit that none of the theories fully captures the range of evidence we uncover, in particular the interaction between gender and quantification. We suggest a formal account that does: we propose, using representations from Kratzer (2009) and Sudo (2012), that gender and number features are differentially represented in referential versus binding dependencies. We speculate how this representational difference relates to the processing mechanisms of antecedent retrieval and to the limited processing advantage for bound singular               they               that we found.                        ,              R{\'e}sum{\'e}                            Les pronoms anglais               they/them/their               sont facilement disponibles avec une interpr{\'e}tation singuli{\`e}re en tant que variables li{\'e}es (Balhorn 2004, Bjorkman 2017). Des interpr{\'e}tations r{\'e}f{\'e}rentielles sont possibles, mais d{\'e}pendent de facteurs pragmatiques et de changements linguistiques en cours (Bjorkman 2017, Conrod 2019, Konnelly et Cowper 2020). Dans une s{\'e}rie d'exp{\'e}riences, nous avons examin{\'e} les diff{\'e}rences entre les               they               singuliers li{\'e}s et les               they               singuliers r{\'e}f{\'e}rentiels en termes d'acceptabilit{\'e} et de traitement incr{\'e}mental, en demandant si le               they               li{\'e} est sensible au genre de son ant{\'e}c{\'e}dent comme l'est le               they               r{\'e}f{\'e}rentiel (Doherty et Conklin 2017, Ackerman et al. 2018, Ackerman et al. 2018, Conrod 2019). Nous avons constat{\'e} que le               they               singulier li{\'e} a un avantage sur le               they               singulier r{\'e}f{\'e}rentiel en termes d'acceptabilit{\'e}, m{\^e}me lorsque l'ant{\'e}c{\'e}dent est genr{\'e}. Lors du traitement, cependant, la variable li{\'e}e               they               singulier a montr{\'e} un avantage en temps de lecture par rapport au               they               singulier r{\'e}f{\'e}rentiel, mais uniquement avec des ant{\'e}c{\'e}dents genr{\'e}s. Nous {\'e}valuons ces r{\'e}sultats par rapport aux analyses existantes de               they               singulier mises en oeuvre dans les mod{\`e}les psycholinguistiques de traitement des pronoms. Nous soutenons qu'aucune de ces analyses ne saisit pleinement l'{\'e}ventail des r{\'e}sultats que nous d{\'e}couvrons, en particulier l'interaction entre le genre et la quantification. Nous sugg{\'e}rons une analyse formelle qui le fait : nous proposons, en utilisant les repr{\'e}sentations de Kratzer (2009) et Sudo (2012), que les traits de genre et de nombre sont repr{\'e}sent{\'e}s diff{\'e}remment dans les d{\'e}pendances r{\'e}f{\'e}rentielles par rapport aux d{\'e}pendances de liage. Nous sp{\'e}culons que cette diff{\'e}rence de repr{\'e}sentation est li{\'e}e aux m{\'e}canismes de traitement, en particulier ceux de la r{\'e}cup{\'e}ration des ant{\'e}c{\'e}dents, et {\`a} l'avantage limit{\'e} du traitement de               they               singulier que nous avons trouv{\'e}s.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\G5P2V65L\Processing bound-variable singular they (Han and Moulton, 2022).pdf}
}

@article{haqueTheyReallyUse2003,
  title = {Do {{They Really Use More Tag Questions}}?},
  author = {Haque, Shahriar and Wong, Bee},
  year = {2003},
  journal = {3L ; Journal of Language, Linguistics and Literature},
  pages = {40--62},
  abstract = {Tag questions are a feature of what Lakoff (1975) calls ''women's language", and women by using them more than men project themselves as weak and uncertain. However, studies show inconsistent findings. This paper presents the findings of a study that examined whether the use of tag questions in the academic context might be more inclined to members of one gender. If so, what forms of tag questions are most frequently used and what are their functions? Furthermore, it is also important for us to realize the extent to which the speaker feels the use of tag questions affect her or his confidence level. The findings of this study show that the common myths of archetypal stereotypes regarding the use of tag questions and women, and the relationship between the use of tag questions and low confidence level may be unfounded.}
}

@techreport{harrisinteractiveLecritureInclusivePopulation2017,
  title = {L'{\'e}criture Inclusive : La Population Fran{\c c}aise Conna{\^i}t-Elle l'{\'e}criture Inclusive ? {{Quelle}} Opinion En a-t-Elle ?},
  shorttitle = {L'{\'e}criture Inclusive},
  author = {{Harris Interactive}},
  year = {2017},
  urldate = {2023-03-25}
}

@article{heathKissingTellingTurning1994,
  title = {Kissing and {{Telling}}: {{Turning Round}} in {{A Room}} with a {{View}}},
  shorttitle = {Kissing and {{Telling}}},
  author = {Heath, Jeffrey},
  year = {1994},
  journal = {Twentieth Century Literature},
  volume = {40},
  number = {4},
  eprint = {441598},
  eprinttype = {jstor},
  pages = {393--433},
  publisher = {[Duke University Press, Hofstra University]},
  issn = {0041-462X},
  doi = {10.2307/441598},
  urldate = {2022-01-11}
}

@misc{heDetectPerturbNeutral2021,
  title = {Detect and {{Perturb}}: {{Neutral Rewriting}} of {{Biased}} and {{Sensitive Text}} via {{Gradient-based Decoding}}},
  shorttitle = {Detect and {{Perturb}}},
  author = {He, Zexue and Majumder, Bodhisattwa Prasad and McAuley, Julian},
  year = {2021},
  eprint = {2109.11708},
  doi = {10.48550/arXiv.2109.11708},
  urldate = {2022-10-25},
  abstract = {Written language carries explicit and implicit biases that can distract from meaningful signals. For example, letters of reference may describe male and female candidates differently, or their writing style may indirectly reveal demographic characteristics. At best, such biases distract from the meaningful content of the text; at worst they can lead to unfair outcomes. We investigate the challenge of re-generating input sentences to `neutralize' sensitive attributes while maintaining the semantic meaning of the original text (e.g. is the candidate qualified?). We propose a gradient-based rewriting framework, Detect and Perturb to Neutralize (DEPEN), that first detects sensitive components and masks them for regeneration, then perturbs the generation model at decoding time under a neutralizing constraint that pushes the (predicted) distribution of sensitive attributes towards a uniform distribution. Our experiments in two different scenarios show that DEPEN can regenerate fluent alternatives that are neutral in the sensitive attribute while maintaining the semantics of other attributes.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2109.11708},
  keywords = {gender rewriting,lu}
}

@incollection{hegartyWhoSecondGraphed2011,
  title = {Who {{Is}} the ({{Second}}) {{Graphed Sex}} and {{Why}}? {{The Meaning}} of {{Order}} in {{Graphs}} of {{Gender Differences}}.},
  booktitle = {Spatial {{Aspects}} of {{Social Thought}}},
  author = {Hegarty, Peter and Lemieux, Anthony},
  year = {2011},
  pages = {325--349},
  publisher = {Mouton de Gruyter},
  address = {Berlin}
}

@article{HistoryWhitePeople,
  title = {The {{History}} of {{White People}}},
  pages = {10},
  keywords = {civi cult. hist.},
  annotation = {CIVI CUL. HIST.},
  file = {C:\Users\spide\Zotero\storage\Q445ZW94\The History of White People.pdf}
}

@misc{HistoryWhitePeoplea,
  title = {The {{History}} of {{White People}}}
}

@book{hjelmslevEssaisLinguistiques1971,
  title = {{Essais linguistiques}},
  author = {Hjelmslev, Louis},
  year = {1971},
  series = {{Arguments}},
  edition = {Nouvelle {\'e}dition.]},
  number = {47},
  publisher = {{\'E}ditions de Minuit},
  address = {Paris},
  isbn = {978-2-7073-0289-2},
  langid = {fredaneng},
  lccn = {P123 .H45 1971},
  file = {C:\Users\spide\Zotero\storage\HZ5HYXL4\Hjelmslev - 1971 - Essais linguistiques.pdf}
}

@article{hockettCourseModernLinguistics1958,
  title = {A {{Course}} in {{Modern Linguistics}}},
  author = {Hockett, C. F.},
  year = {1958},
  month = jan,
  journal = {Language Learning},
  volume = {8},
  number = {3-4},
  pages = {73--75},
  issn = {0023-8333, 1467-9922},
  doi = {10.1111/j.1467-1770.1958.tb00870.x},
  urldate = {2024-12-01},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\DU64E2SL\Hockett - 1958 - A COURSE IN MODERN LINGUISTICS.pdf}
}

@book{hofstadterLAnalogieCoeurPensee2013,
  title = {{L'Analogie, Coeur de la Pens{\'e}e}},
  author = {Hofstadter, Douglas and Sander, Emmanuel},
  year = {2013},
  edition = {Odile Jacob},
  address = {Paris},
  langid = {french}
}

@article{horsmanScientificRacismAmerican1975,
  title = {Scientific {{Racism}} and the {{American Indian}} in the {{Mid-Nineteenth Century}}},
  author = {Horsman, Reginald},
  year = {1975},
  month = may,
  journal = {American Quarterly},
  volume = {27},
  number = {2},
  eprint = {2712339},
  eprinttype = {jstor},
  pages = {18},
  issn = {00030678},
  doi = {10.2307/2712339},
  urldate = {2021-12-17},
  langid = {english},
  keywords = {civi cult. hist.},
  annotation = {CIVI CUL. HIST.},
  file = {C:\Users\spide\Zotero\storage\7WJVISI5\Scientific Racism and the American Indian in the Mid-Nineteenth Century (Horsman, 1975).pdf}
}

@misc{HttpsCoriataln2023sciencesconforg461946,
  title = {{{https://coria-taln-2023.sciencesconf.org/461946/document}}},
  urldate = {2024-05-17},
  howpublished = {https://coria-taln-2023.sciencesconf.org/461946/document}
}

@misc{huangContentModerationLLM2024,
  title = {Content {{Moderation}} by {{LLM}}: {{From Accuracy}} to {{Legitimacy}}},
  shorttitle = {Content {{Moderation}} by {{LLM}}},
  author = {Huang, Tao},
  year = {2024},
  month = sep,
  number = {arXiv:2409.03219},
  eprint = {2409.03219},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.03219},
  urldate = {2024-12-17},
  abstract = {One trending application of LLM (large language model) is to use it for content moderation in online platforms. Most current studies on this application have focused on the metric of accuracy - the extent to which LLM makes correct decisions about content. This article argues that accuracy is insufficient and misleading, because it fails to grasp the distinction between easy cases and hard cases as well as the inevitable trade-offs in achieving higher accuracy. Closer examination reveals that content moderation is a constitutive part of platform governance, the key of which is to gain and enhance legitimacy. Instead of making moderation decisions correct, the chief goal of LLM is to make them legitimate. In this regard, this article proposes a paradigm shift from the single benchmark of accuracy towards a legitimacy-based framework of evaluating the performance of LLM moderators. The framework suggests that for easy cases, the key is to ensure accuracy, speed and transparency, while for hard cases, what matters is reasoned justification and user participation. Examined under this framework, LLM's real potential in moderation is not accuracy improvement. Rather, LLM can better contribute in four other aspects: to conduct screening of hard cases from easy cases, to provide quality explanations for moderation decisions, to assist human reviewers in getting more contextual information, and to facilitate user participation in a more interactive way. Using normative theories from law and social sciences to critically assess the new technological application, this article seeks to redefine LLM's role in content moderation and redirect relevant research in this field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Emerging Technologies,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {C\:\\Users\\spide\\Zotero\\storage\\4AP2PLXL\\Huang - 2024 - Content Moderation by LLM From Accuracy to Legitimacy.pdf;C\:\\Users\\spide\\Zotero\\storage\\B2PIPDZF\\2409.html}
}

@misc{huangNAP^2BenchmarkNaturalness2024,
  title = {{{NAP}}{\textasciicircum}2: {{A Benchmark}} for {{Naturalness}} and {{Privacy-Preserving Text Rewriting}} by {{Learning}} from {{Human}}},
  shorttitle = {{{NAP}}{\textasciicircum}2},
  author = {Huang, Shuo and MacLean, William and Kang, Xiaoxi and Wu, Anqi and Qu, Lizhen and Xu, Qiongkai and Li, Zhuang and Yuan, Xingliang and Haffari, Gholamreza},
  year = {2024},
  month = jun,
  number = {arXiv:2406.03749},
  eprint = {2406.03749},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-14},
  abstract = {Increasing concerns about privacy leakage issues in academia and industry arise when employing NLP models from third-party providers to process sensitive texts. To protect privacy before sending sensitive data to those models, we suggest sanitizing sensitive text using two common strategies used by humans: i) deleting sensitive expressions, and ii) obscuring sensitive details by abstracting them. To explore the issues and develop a tool for text rewriting, we curate the first corpus, coined NAP2, through both crowdsourcing and the use of large language models (LLMs). Compared to the prior works based on differential privacy, which lead to a sharp drop in information utility and unnatural texts, the human-inspired approaches result in more natural rewrites and offer an improved balance between privacy protection and data utility, as demonstrated by our extensive experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\spide\Zotero\storage\UPI5GETT\Huang et al. - 2024 - NAP^2 A Benchmark for Naturalness and Privacy-Pre.pdf}
}

@article{hughesSunscreenPreventionSkin,
  title = {Sunscreen and {{Prevention}} of {{Skin Aging}}},
  author = {Hughes, Maria Celia B and Williams, Gail M and Baker, Peter},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\AK6282ZP\Sunscreen and Prevention of Skin Aging (Hughes et al., ).pdf}
}

@article{huitinkProtagorasBeginningsGrammar2021,
  title = {Protagoras and the {{Beginnings}} of {{Grammar}}},
  author = {Huitink, Luuk and Willi, Andreas},
  year = {2021},
  month = dec,
  journal = {The Cambridge Classical Journal},
  volume = {67},
  pages = {66--92},
  issn = {1750-2705, 2047-993X},
  doi = {10.1017/S175027052100004X},
  urldate = {2024-12-01},
  abstract = {Offering a reevaluation of all the available evidence, including passages from Aristotle's Rhetoric, Poetics, and Sophistici Elenchi, Diogenes Laertius' biographical sketch as well as the grammar scene in Aristophanes' Clouds, the article argues that Protagoras' engagement with grammatical questions must have been more sophisticated and thorough than is often assumed. In Protagoras' discovery of grammatical gender, formal considerations -- most likely inspired by the analysis of personal names -- played a more fundamental role than semantic ones, and his typology of {$\pi\upsilon\theta\mu\acute{\epsilon}\nu\varepsilon\varsigma$} {$\lambda$}{\'undefined}{$\gamma\omega\nu$} equally presupposes the formal recognition of at least verbal mood, if not also tense.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\8T6EUNZ8\Huitink and Willi - 2021 - PROTAGORAS AND THE BEGINNINGS OF GRAMMAR.pdf}
}

@inproceedings{hutchinsonSocialBiasesNLP2020,
  title = {Social {{Biases}} in {{NLP Models}} as {{Barriers}} for {{Persons}} with {{Disabilities}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Hutchinson, Ben and Prabhakaran, Vinodkumar and Denton, Emily and Webster, Kellie and Zhong, Yu and Denuyl, Stephen},
  year = {2020},
  pages = {5491--5501},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.487},
  urldate = {2024-12-17},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\9EF2FCNE\Hutchinson et al. - 2020 - Social Biases in NLP Models as Barriers for Persons with Disabilities.pdf}
}

@article{hynesAttentionMustBe2021,
  title = {Attention {{Must Be Paid}}...},
  author = {Hynes, Joseph A},
  year = {2021},
  pages = {6},
  langid = {english},
  keywords = {miller},
  file = {C:\Users\spide\Zotero\storage\DF87WLY3\Attention Must Be Paid... (Hynes, 2021).pdf}
}

@book{ibrahimGrammaticalGenderIts1973,
  title = {Grammatical {{Gender}}: {{Its Origin}} and {{Development}}},
  shorttitle = {Grammatical {{Gender}}},
  author = {Ibrahim, Muhammad Hasan},
  year = {1973},
  month = dec,
  publisher = {DE GRUYTER},
  doi = {10.1515/9783110905397},
  urldate = {2024-12-01},
  isbn = {978-90-279-2449-0},
  file = {C:\Users\spide\Zotero\storage\9LUW3QLV\Ibrahim - 1973 - Grammatical Gender Its Origin and Development.PDF}
}

@misc{ignatPhDStudentsPerspective2023,
  title = {A {{PhD Student}}'s {{Perspective}} on {{Research}} in {{NLP}} in the {{Era}} of {{Very Large Language Models}}},
  author = {Ignat, Oana and Jin, Zhijing and Abzaliev, Artem and Biester, Laura and Castro, Santiago and Deng, Naihao and Gao, Xinyi and Gunal, Aylin and He, Jacky and Kazemi, Ashkan and Khalifa, Muhammad and Koh, Namho and Lee, Andrew and Liu, Siyang and Min, Do June and Mori, Shinka and Nwatu, Joan and {Perez-Rosas}, Veronica and Shen, Siqi and Wang, Zekun and Wu, Winston and Mihalcea, Rada},
  year = {2023},
  month = may,
  number = {arXiv:2305.12544},
  eprint = {2305.12544},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-08-12},
  abstract = {Recent progress in large language models has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that ``it's all been solved.'' Not surprisingly, this has in turn made many NLP researchers -- especially those at the beginning of their career -- wonder about what NLP research area they should focus on. This document is a compilation of NLP research directions that are rich for exploration, reflecting the views of a diverse group of PhD students in an academic research lab. While we identify many research areas, many others exist; we do not cover those areas that are currently addressed by LLMs but where LLMs lag behind in performance, or those focused on LLM development. We welcome suggestions for other research directions to include: https://bit.ly/nlp-era-llm},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\spide\Zotero\storage\P2XN5889\A PhD Student's Perspective on Research in NLP in the Era of Very Large Language Models (Ignat et al., 2023).pdf}
}

@misc{IntroductionAffectiveEncounters,
  title = {Introduction: {{Affective Encounters}} and {{Reflexive Representations}} from {{Disability}}, {{Literature}}, {{Genre}}: {{Representation}} and {{Affect}} in {{Contemporary Fiction}} on {{JSTOR}}},
  eprint = {j.ctvsn3pp7.4},
  eprinttype = {jstor},
  urldate = {2022-02-05},
  howpublished = {https://www.jstor.org/stable/j.ctvsn3pp7.4?seq=1\#metadata\_info\_tab\_contents},
  file = {C:\Users\spide\Zotero\storage\3UIDQHPE\j.ctvsn3pp7.html}
}

@misc{iowaarchaeologyAncientIowaFilm2014,
  title = {Ancient {{Iowa Film Series}}: {{Visiting}} the {{Indians}} with {{George Catlin}} (1972)},
  shorttitle = {Ancient {{Iowa Film Series}}},
  author = {{Iowa Archaeology}},
  year = {2014},
  month = aug,
  urldate = {2021-12-18},
  abstract = {"Visiting the Indians with George Catlin" documents the life of the famous American artist George Catlin. Catlin spent much of his artistic career during the 1830s studying and recording Indian customs in his artwork. Produced by Marshall McKusick. Dubbed from 16 mm prints. Ancient Iowa Film Series, 1972. Copyright: University of Iowa Office of the State Archaeologist.},
  keywords = {civi cult. hist.},
  annotation = {CIVI CULT. HIST.}
}

@article{istvanOriginGrammaticalGender1959,
  title = {The Origin of Grammatical Gender},
  author = {Istv{\'a}n, Fodor},
  year = {1959},
  journal = {Lingua},
  volume = {8},
  pages = {186--214},
  issn = {00243841},
  doi = {10.1016/0024-3841(59)90020-8},
  urldate = {2024-11-21},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  keywords = {read},
  file = {C:\Users\spide\Zotero\storage\TV6M25N6\10.1016@0024-38415990020-8.pdf}
}

@article{izakThereDeclineEcoterrorism2022,
  title = {Is There a Decline in Ecoterrorism?},
  author = {Izak, Krzysztof},
  year = {2022},
  month = may,
  journal = {Przegl{\k a}d Bezpiecze{\'n}stwa Wewn{\k e}trznego},
  volume = {14},
  number = {26},
  pages = {394--431},
  issn = {2720-0841, 2080-1335},
  doi = {10.4467/20801335PBW.21.046.15706},
  urldate = {2022-08-10},
  abstract = {The purpose of the article is to briefly characterize ecoterrorism, which is based on a specific ideology. Some of the content proclaimed by its promoters might merit support were it not for their overly dehumanizing rhetoric, and especially the practice motivated by it. The phenomenon of ecoterrorism, or environmental terrorism, is associated with the use of violence by extremist pro-environment groups and radicalized individuals. Ecoterrorism is divided into pro-environmental terrorism and pro-animalism. Environmental terrorism can also be divided into terrorism by environmentalists and terrorism by animal rights activists. The greatest development of ecoterrorism has occurred in the United States. Animal Liberation Front, ALF was the most dangerous pro-environmental organization, while the group with the most members was Earth First, EF! Violence by eco-terrorists has occurred in many countries on our continent, most notably in the United Kingdom. The ideology of ecocentrism, which motivates the activities of many groups in the West, has penetrated Poland, but in our country it has not had much influence on the activities of organized groups of environmentalists and individuals. On the other hand, incidents of violence were recorded in a series of events directed against specific companies and institutions that threatened the environment. The research shows that there is a slow but steady decline in the number of ecoterrorism incidents worldwide. It is too early to predict its decline, but there is certainly evidence of declining violent activity. Several circumstances contribute to the downscaling of aggressive environmental activities. Above all, there has been an increase in the environmental awareness of societies and governments. A large impact on the decrease in the number of ecoterrorist events is also due to the tightening of legislation, as well as the activities of environmental organizations.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\8LST8FWD\Is there a decline in ecoterrorism (Izak, 2022).pdf}
}

@article{jacobsonPolarizationGridlockPresidential2016,
  title = {Polarization, {{Gridlock}}, and {{Presidential Campaign Politics}} in 2016},
  author = {Jacobson, Gary C.},
  year = {2016},
  month = sep,
  journal = {The ANNALS of the American Academy of Political and Social Science},
  volume = {667},
  number = {1},
  pages = {20},
  issn = {0002-7162, 1552-3349},
  doi = {10.1177/0002716216658921},
  urldate = {2021-04-07},
  abstract = {The American electorate has grown increasingly divided along party lines in recent decades, by political attitudes, social values, basic demography, and even beliefs about reality. Deepening partisan divisions have inspired high levels of party-line voting and low levels of ticket splitting, resulting in thoroughly nationalized, president- and party-centered federal elections. Because of the way the electoral system aggregates votes, however, historically high levels of electoral coherence have delivered incoherent, divided government and policy stalemate. The 2016 nomination campaigns have exposed deep fissures within as well as between the parties, and their results threaten to shake up electoral patterns that have prevailed so far during this century, with uncertain and perhaps unpredictable consequences for national politics. The 2016 election is certain to polarize the electorate, but the axis of polarization may not fall so neatly along party lines as it has in recent years.},
  langid = {english},
  keywords = {history},
  file = {C:\Users\spide\Zotero\storage\ZD2499SP\Polarization, Gridlock, and Presidential Campaign Politics in 2016 (Jacobson, 2016).pdf}
}

@article{jacobsonUseNonsexistPronouns1985,
  title = {Use of {{Nonsexist Pronouns}} as a {{Function}} of {{One}}'s {{Feminist Orientation}}},
  author = {Jacobson, Marsha B. and Insko, William R.},
  year = {1985},
  month = jul,
  journal = {Sex Roles},
  volume = {13},
  number = {1-2},
  pages = {1--7},
  issn = {0360-0025, 1573-2762},
  doi = {10.1007/BF00287456},
  urldate = {2023-03-25}
}

@article{jacobsonUseNonsexistPronouns1985a,
  title = {Use of Nonsexist Pronouns as a Function of One's Feminist Orientation},
  author = {Jacobson, Marsha B. and Insko, William R.},
  year = {1985},
  month = jul,
  journal = {Sex Roles},
  volume = {13},
  number = {1-2},
  pages = {1--7},
  issn = {0360-0025, 1573-2762},
  doi = {10.1007/BF00287456},
  urldate = {2024-11-19},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\IYKXD8VV\doc.pdf&pages=1&minSrcCount=5.pdf}
}

@article{jainGeneratingGenderAugmented2021,
  title = {Generating {{Gender Augmented Data}} for {{NLP}}},
  author = {Jain, Nishtha and Popovic, Maja and Groves, Declan and Vanmassenhove, Eva},
  year = {2021},
  month = jul,
  eprint = {2107.05987},
  primaryclass = {cs},
  pages = {12},
  urldate = {2022-11-13},
  abstract = {Gender bias is a frequent occurrence in NLPbased applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of bias becomes more evident in generating conversational utterances where gender is not specified within the sentence, because most current NLP applications still work on a sentence-level context. As a step towards more inclusive NLP, this paper proposes an automatic and generalisable re-writing approach for short conversational sentences. The rewriting method can be applied to sentences that, without extra-sentential context, have multiple equivalent alternatives in terms of gender. The method can be applied both for creating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation (NMT) system trained to `translate' from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results for automatic generation of gender alternatives for conversational sentences in Spanish.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\4RKVNNC2\Generating Gender Augmented Data for NLP (Jain et al., 2021).pdf}
}

@article{jakeschHumanHeuristicsAIGenerated,
  title = {Human {{Heuristics}} for {{AI-Generated Language Are Flawed}}},
  author = {Jakesch, Maurice and Hancock, Jeffrey T and Naaman, Mor},
  pages = {9},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\6F8GWEMN\Human Heuristics for AI-Generated Language Are Flawed (Jakesch et al., ).pdf}
}

@misc{jeonImprovingBiasMitigation2023,
  title = {Improving {{Bias Mitigation}} through {{Bias Experts}} in {{Natural Language Understanding}}},
  author = {Jeon, Eojin and Lee, Mingyu and Park, Juhyeong and Kim, Yeachan and Mok, Wing-Lam and Lee, SangKeun},
  year = {2023},
  month = dec,
  number = {arXiv:2312.03577},
  eprint = {2312.03577},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.03577},
  urldate = {2024-12-17},
  abstract = {Biases in the dataset often enable the model to achieve high performance on in-distribution data, while poorly performing on out-of-distribution data. To mitigate the detrimental effect of the bias on the networks, previous works have proposed debiasing methods that down-weight the biased examples identified by an auxiliary model, which is trained with explicit bias labels. However, finding a type of bias in datasets is a costly process. Therefore, recent studies have attempted to make the auxiliary model biased without the guidance (or annotation) of bias labels, by constraining the model's training environment or the capability of the model itself. Despite the promising debiasing results of recent works, the multi-class learning objective, which has been naively used to train the auxiliary model, may harm the bias mitigation effect due to its regularization effect and competitive nature across classes. As an alternative, we propose a new debiasing framework that introduces binary classifiers between the auxiliary model and the main model, coined bias experts. Specifically, each bias expert is trained on a binary classification task derived from the multi-class classification task via the One-vs-Rest approach. Experimental results demonstrate that our proposed strategy improves the bias identification ability of the auxiliary model. Consequently, our debiased model consistently outperforms the state-of-the-art on various challenge datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\UNDHNMPC\\Jeon et al. - 2023 - Improving Bias Mitigation through Bias Experts in Natural Language Understanding.pdf;C\:\\Users\\spide\\Zotero\\storage\\E5X9Q8FA\\2312.html}
}

@article{jervisUnderstandingBushDoctrine,
  title = {Understanding the {{Bush Doctrine}}},
  author = {Jervis, Robert},
  pages = {24},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\2R796VBQ\Understanding the Bush Doctrine (Jervis, ).pdf}
}

@misc{jiangMixtralExperts2024,
  title = {Mixtral of {{Experts}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and {family=Casas}, given=Diego, prefix=de las, useprefix=false and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L{\'e}lio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2024},
  month = jan,
  eprint = {2401.04088},
  urldate = {2024-03-18},
  abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model finetuned to follow instructions, Mixtral 8x7B -- Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B -- chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2401.04088},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{jiSurveyHallucinationNatural2023,
  title = {Survey of {{Hallucination}} in {{Natural Language Generation}}},
  author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Chen, Delong and Chan, Ho Shu and Dai, Wenliang and Madotto, Andrea and Fung, Pascale},
  year = {2023},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {12},
  eprint = {2202.03629},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3571730},
  urldate = {2024-03-18},
  abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation; and (3) hallucinations in large language models (LLMs) 1. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG. CCS Concepts: {$\bullet$} Computing methodologies {$\rightarrow$} Natural language generation; Neural networks.},
  archiveprefix = {arXiv},
  keywords = {A.1,Computer Science - Computation and Language}
}

@article{k.haleAustensEmma2010,
  title = {Austen's {{Emma}}},
  author = {K. Hale, John},
  year = {2010},
  month = mar,
  pages = {122--124},
  langid = {english},
  keywords = {jane austen},
  file = {C:\Users\spide\Documents\PDF recherches\Austen's Emma (K. Hale, 2010).pdf}
}

@misc{kaddourChallengesApplicationsLarge2023,
  title = {Challenges and {{Applications}} of {{Large Language Models}}},
  author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  year = {2023},
  month = jul,
  eprint = {2307.10169},
  urldate = {2024-03-18},
  abstract = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2307.10169},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{kamruzzamanInvestigatingSubtlerBiases2024,
  title = {Investigating {{Subtler Biases}} in {{LLMs}}: {{Ageism}}, {{Beauty}}, {{Institutional}}, and {{Nationality Bias}} in {{Generative Models}}},
  shorttitle = {Investigating {{Subtler Biases}} in {{LLMs}}},
  author = {Kamruzzaman, Mahammed and Shovon, Md Minul Islam and Kim, Gene Louis},
  year = {2024},
  month = jun,
  number = {arXiv:2309.08902},
  eprint = {2309.08902},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.08902},
  urldate = {2024-12-17},
  abstract = {LLMs are increasingly powerful and widely used to assist users in a variety of tasks. This use risks the introduction of LLM biases to consequential decisions such as job hiring, human performance evaluation, and criminal sentencing. Bias in NLP systems along the lines of gender and ethnicity has been widely studied, especially for specific stereotypes (e.g., Asians are good at math). In this paper, we investigate bias along less-studied but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that LLMs make between social groups and unrelated positive and negative attributes. We ask whether LLMs hold wide-reaching biases of positive or negative sentiment for specific social groups similar to the "what is beautiful is good" bias found in people in experimental psychology. We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attribute to complete an evaluative statement about a person described as a member of a specific social group. We also reverse the completion task to select the social group based on an attribute. We report the correlations that we find for 4 cutting-edge LLMs. This dataset can be used as a benchmark to evaluate progress in more generalized biases and the templating technique can be used to expand the benchmark with minimal additional human annotation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\KF2I2CFZ\\Kamruzzaman et al. - 2024 - Investigating Subtler Biases in LLMs Ageism, Beauty, Institutional, and Nationality Bias in Generat.pdf;C\:\\Users\\spide\\Zotero\\storage\\ZDT3AR7U\\2309.html}
}

@misc{kanekoEvaluatingGenderBias2024,
  title = {Evaluating {{Gender Bias}} in {{Large Language Models}} via {{Chain-of-Thought Prompting}}},
  author = {Kaneko, Masahiro and Bollegala, Danushka and Okazaki, Naoaki and Baldwin, Timothy},
  year = {2024},
  month = jan,
  number = {arXiv:2401.15585},
  eprint = {2401.15585},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.15585},
  urldate = {2024-12-17},
  abstract = {There exist both scalable tasks, like reading comprehension and fact-checking, where model performance improves with model size, and unscalable tasks, like arithmetic reasoning and symbolic reasoning, where model performance does not necessarily improve with model size. Large language models (LLMs) equipped with Chain-of-Thought (CoT) prompting are able to make accurate incremental predictions even on unscalable tasks. Unfortunately, despite their exceptional reasoning abilities, LLMs tend to internalize and reproduce discriminatory societal biases. Whether CoT can provide discriminatory or egalitarian rationalizations for the implicit information in unscalable tasks remains an open question. In this study, we examine the impact of LLMs' step-by-step predictions on gender bias in unscalable tasks. For this purpose, we construct a benchmark for an unscalable task where the LLM is given a list of words comprising feminine, masculine, and gendered occupational words, and is required to count the number of feminine and masculine words. In our CoT prompts, we require the LLM to explicitly indicate whether each word in the word list is a feminine or masculine before making the final predictions. With counting and handling the meaning of words, this benchmark has characteristics of both arithmetic reasoning and symbolic reasoning. Experimental results in English show that without step-by-step prediction, most LLMs make socially biased predictions, despite the task being as simple as counting words. Interestingly, CoT prompting reduces this unconscious social bias in LLMs and encourages fair predictions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\KCRTFXCT\\Kaneko et al. - 2024 - Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting.pdf;C\:\\Users\\spide\\Zotero\\storage\\DLKZC5AC\\2401.html}
}

@article{kaplanEnglishCivilWar1972,
  title = {English {{Civil War Politics}} and the {{Religious Settlement}}},
  author = {Kaplan, Lawrence},
  year = {1972},
  month = sep,
  journal = {Church History},
  volume = {41},
  number = {3},
  pages = {307--325},
  issn = {0009-6407, 1755-2613},
  doi = {10.2307/3164218},
  urldate = {2022-04-20},
  abstract = {One of the more active historical controversies centers around the precise relationship of religion to politics during the period of the English Civil War. While all historians recognize the crucial role played by Puritans in the rebellion against Charles I, the extent to which religious considerations influenced political activity within the Long Parliament remains open to question. A major reason for the dispute is that terms used by contemporaries tend to be misleading. Thus, the two parties which are said to have dominated the Long Parliament during the 1640s are known by descriptive names (``the Presbyterians'' and ``the Independents'') that the bear little resemblance to their actual platforms.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\CD6Y4P7F\English Civil War Politics and the Religious Settlement (Kaplan, 1972).pdf}
}

@misc{karvonenIntuitiveExplanationSparse2024,
  title = {An {{Intuitive Explanation}} of {{Sparse Autoencoders}} for {{LLM Interpretability}}},
  author = {Karvonen, Adam},
  year = {2024},
  month = jun,
  urldate = {2024-11-11},
  abstract = {Sparse Autoencoders (SAEs) have recently become popular for interpretability of machine learning models (although sparse dictionary learning has been around since 1997). Machine learning models and LLMs are becoming more powerful and useful, but they are still black boxes, and we don't understand how they do the things that they are capable of. It seems like it would be useful if we could understand how they work.},
  howpublished = {https://adamkarvonen.github.io/machine\_learning/2024/06/11/sae-intuitions.html},
  langid = {english},
  file = {C\:\\Users\\spide\\Zotero\\storage\\RFWNQ4NN\\Karvonen - 2024 - An Intuitive Explanation of Sparse Autoencoders fo.pdf;C\:\\Users\\spide\\Zotero\\storage\\KXR8BW6C\\sae-intuitions.html}
}

@article{kayLinguisticSignificanceMeanings1978,
  title = {The {{Linguistic Significance}} of the {{Meanings}} of {{Basic Color Terms}}},
  author = {Kay, Paul and McDaniel, Chad K.},
  year = {1978},
  journal = {Language},
  volume = {54},
  number = {3},
  pages = {610--646}
}

@article{kayWhatSapirWhorfHypothesis1984,
  title = {What {{Is}} the {{Sapir-Whorf Hypothesis}}?},
  author = {Kay, Paul and Kempton, Willett},
  year = {1984},
  month = mar,
  journal = {American Anthropologist},
  volume = {86},
  number = {1},
  pages = {65--79},
  issn = {0002-7294, 1548-1433},
  doi = {10.1525/aa.1984.86.1.02a00050},
  urldate = {2023-03-25}
}

@article{keithLadiesFirstLadies2022,
  title = {Ladies {{First}} or {{Ladies Last}}: {{Do Masculine Generics Evoke}} a {{Reduced}} and {{Later Retrieval}} of {{Female Exemplars}}?},
  shorttitle = {Ladies {{First}} or {{Ladies Last}}},
  author = {Keith, Nina and Hartwig, Kristine and Richter, Tobias},
  editor = {Inbar, Yoel},
  year = {2022},
  month = feb,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {32964},
  issn = {2474-7394},
  doi = {10.1525/collabra.32964},
  urldate = {2025-01-07},
  abstract = {The use of masculine generics (i.e., grammatically masculine forms that refer to both men and women) is prevalent in many languages but has been criticized for potentially triggering male bias. Empirical evidence for this claim exists but is often based on small and selective samples. This study is a high-powered and pre-registered replication and extension of a 20-year-old study on this biasing effect in German speakers. Under 1 of 4 conditions (masculine generics vs.~three gender-inclusive alternatives), 344 participants listed 3 persons of 6 popular occupational categories (e.g., athletes, politicians). Despite 20 years of societal changes, results were remarkably similar, underscoring the high degree of automaticity involved in language comprehension (large effects of 0.71 to 1.12 of a standard deviation). Male bias tended to be particularly pronounced later rather than early in retrieval, suggesting that salient female exemplars may be recalled first but that male exemplars still dominate the overall categorical representations.},
  copyright = {http://creativecommons.org/licenses/by/4.0},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\65CBCRBI\Keith et al. - 2022 - Ladies First or Ladies Last Do Masculine Generics Evoke a Reduced and Later Retrieval of Female Exe.pdf}
}

@article{kelleterEarlyAmericanCaptivity1999,
  title = {Early {{American Captivity}}, {{Transcendental Flights}}: {{Contending Versions}} of {{National Identity}} in {{Herman Melville}}'s {{Typee}}},
  author = {Kelleter, Franz},
  year = {1999},
  journal = {The Construction and Contestation of American Cultures and Identities in the Early National Period},
  volume = {78},
  pages = {24},
  langid = {english},
  keywords = {melville},
  file = {C:\Users\spide\Documents\COURS S4\- APPROFONDISSEMENT\Littérature\PDF Recherches\Early American Captivity, Transcendental Flights Contending Versions of National Identity in Herman Melville's Typee (Kelleter, 1999).pdf}
}

@article{kesebirWordOrderDenotes2017,
  title = {Word {{Order Denotes Relevance Differences}}: {{The Case}} of {{Conjoined Phrases}} with {{Lexical Gender}}.},
  shorttitle = {Word {{Order Denotes Relevance Differences}}},
  author = {Kesebir, Selin},
  year = {2017},
  month = aug,
  journal = {Journal of Personality and Social Psychology},
  volume = {113},
  number = {2},
  pages = {262--279},
  issn = {1939-1315, 0022-3514},
  doi = {10.1037/pspi0000094},
  urldate = {2023-03-25},
  abstract = {This work explores the order of linguistic references to the two genders (e.g., men and women vs. women and men). It argues that a gender is more likely to be mentioned first when it is perceived to have higher relevance in a context rather than lower relevance, and audiences assign stronger relevance to a party when the party is mentioned first rather than second. Studies 1--3 document the current prevalence of male-first conjoined phrases in the public (but not family) domain and link the pattern to historical changes in women's public presence over the 20th century. Study 4 shows that contextual relevance cues affect the odds of first mention, such that people are more likely to refer to a woman before a man, when the two are in a primary school classroom rather than a corporate office. At the same time, Studies 4 and 5 find that people often choose to reproduce collectively preferred word order patterns (e.g., men and women). Studies 6 and 7 show that these choices matter because people assign more relevance to a party when it comes first rather than second in a conjoined phrase. Overall, this work offers theoretical grounding and empirical evidence for word order as a means of expressing and perpetuating gender stereotypes.}
}

@article{khamisKULeuvenFaculty,
  title = {{{KU Leuven Faculty}} of {{Arts Blijde Inkomststraat}} 21, Box 3301 3000 {{Leuven}}, {{Belgium}}},
  author = {Khamis, Ashraf},
  pages = {76},
  abstract = {This thesis examines the cross-linguistically rare category of possessive --ing constructions through a comparative corpus analysis of nominal gerunds (e.g. Davis' tripling of the car tax, their amassing of enormous wealth) and verbal gerunds (e.g. women's entering the work force, his attending the ceremony) with genitive subjects in Present-Day English. The current study adds to a growing body of literature on --ing nominalizations by investigating the representational, aspectual, syntactic, and referential behavior of nominal and verbal gerunds with genitive subjects and the semantic features of their possessors. The data analysis reveals significant areas of difference and overlap between the two gerundive constructions, which help draw attention to their underlying semantics and account for the use of one gerund type over the other in particular environments. It is argued, however, that the corpus results presented here might be too specific to nominal and verbal gerunds with genitive subjects, posing limitations on the extent to which they can be generalized to their subjectless counterparts.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\2HF7FW5G\KU Leuven Faculty of Arts Blijde Inkomststraat 21, box 3301 3000 Leuven, Belgium (Khamis, ).pdf}
}

@article{khosravinikRightWingPopulism2017,
  title = {Right {{Wing Populism}} in the {{West}}: {{Social Media Discourse}} and {{Echo Chambers}}},
  shorttitle = {Right {{Wing Populism}} in the {{West}}},
  author = {Khosravinik, Majid},
  year = {2017},
  month = jul,
  journal = {Insight Turkey},
  volume = {19},
  number = {3},
  pages = {17},
  issn = {1302177X, 25647717},
  doi = {10.25253/99.2017193.04},
  urldate = {2021-04-10},
  abstract = {The commentary examines the roots of electoral shifts towards right wing populist parties and groups in the West. It shows how legitimate economic grievances of lower classes have been strategically appropriated by political elites to project xenophobic discourses and how globalist-capitalist parties capitalize on such sentiments. It discusses the British Brexit vote as a quintessential example of strategic misplacement of migration issue as the main problem to disguise the democratic deficit of a hyper-normalized neoliberal economic order. The commentary also examines the links between technological design of Social Media technologies and the notion of post-politics era.},
  langid = {english},
  keywords = {history},
  file = {C:\Users\spide\Zotero\storage\ZK93B6FR\Right Wing Populism in the West Social Media Discourse and Echo Chambers (Khosravinik, 2017).pdf}
}

@incollection{kilarskiGrammaticalGenderArbitrary2007,
  title = {On Grammatical Gender as an Arbitrary and Redundant Category},
  booktitle = {Studies in the {{History}} of the {{Language Sciences}}},
  author = {Kilarski, Marcin},
  editor = {Kibbee, Douglas A.},
  year = {2007},
  volume = {112},
  pages = {24--36},
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam},
  doi = {10.1075/sihols.112.03kil},
  urldate = {2024-11-21},
  abstract = {In this paper I give an overview of tendencies in the research on grammatical gender within the Western linguistic tradition. More specifically, I focus on the recurring claims concerning the supposed semantic arbitrariness, and formal and non-functional character of this category. Representative examples are given from every period of linguistic activity, from the ancient Greek scholars up to contemporary descriptive and typological studies. Particular attention is given to the most influential works, e.g. those of the Neogrammarians and European as well as American structuralists within 19th and 20th century scholarship. While examples have been drawn mainly from the research on Indo-European, the tendencies described are also indicative of the research on other families and on systems traditionally referred to as ``noun classes''. Finally, I consider these claims in the light of the evidence that is now available of the semantic regularity of gender, its discourse functions and cognitive correlates.},
  isbn = {978-90-272-4603-5 978-90-272-9172-1},
  langid = {english},
  keywords = {read},
  file = {C:\Users\spide\Zotero\storage\XJ69TM4T\Kilarski - 2007 - On grammatical gender as an arbitrary and redundant category.pdf}
}

@article{kiziltunaliNATURALISMDISINTEGRATIONAMERICAN,
  title = {{NATURALISM AND THE DISINTEGRATION OF THE AMERICAN DREAM: EDITH WHARTON'S THE HOUSE OF MIRTH AND F. SCOTT FITZGERALD'S THE GREAT GATSBY}},
  author = {Kiziltunali, Gizem},
  pages = {20},
  abstract = {American literary naturalism and the ``American dream'' are different from each other in terms of their ideologies. This difference stems from one's being pessimistic and the other's being optimistic In this work, the similarity between American literary naturalism and the American dream has been emphasized in terms of the literary works, characters and situations in the works and these similarities are shown by means of a case study. For this purpose, in this work, American literature's most remarkable masterpiecesEdith Wharton's The House of Mirth and F. Scott Fitzgerald's The Great Gatsby have been compared. Through the result of the comparison, the seemingly different ideologies of the naturalist movement and the American Dream's similarities have been emphasized in various respects.},
  langid = {turkish},
  keywords = {fitzgerald},
  annotation = {GATSBY},
  file = {C:\Users\spide\Zotero\storage\GHVA9MKH\NATURALISM AND THE DISINTEGRATION OF THE AMERICAN DREAM EDITH WHARTON’S THE HOUSE OF MIRTH AND F. SCOTT FITZGERALD’S THE GREAT GATSBY (Kiziltunali, ).pdf}
}

@article{koehnEuroparlParallelCorpus2005,
  title = {Europarl: {{A Parallel Corpus}} for {{Statistical Machine Translation}}},
  author = {Koehn, Philipp},
  year = {2005},
  abstract = {We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web1. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.}
}

@article{kornerExaminingGlottalStop2024,
  title = {Examining the Glottal Stop as a Mark of Gender-Inclusive Language in {{German}}},
  author = {K{\"o}rner, Anita and Glim, Sarah and Rummer, Ralf},
  year = {2024},
  month = jan,
  journal = {Applied Psycholinguistics},
  volume = {45},
  number = {1},
  pages = {156--179},
  issn = {0142-7164, 1469-1817},
  doi = {10.1017/S0142716424000018},
  urldate = {2024-11-19},
  abstract = {Grammatical gender form influences readers' mental gender representations. Previous research demonstrates that the generic masculine form leads to male-biased representations, while some alternative forms lead to female-biased representations. The present research examines the recently introduced glottal stop form in spoken language in German, where a glottal stop (similar to a short pause), meant to represent all gender identities, is inserted before the gender-specific ending. In two experiments (total N = 1188), participants listened to sentences in the glottal stop, the generic masculine, or the generic feminine form and classified whether a second sentence about women or men was a sensible continuation. The generic feminine and the glottal stop led to female biases (fewer errors in sentences about women vs. men) and the generic masculine led to a male bias. The biases were smaller for the glottal stop and the generic masculine than for the generic feminine, indicating that the former two are more readily understood as representing both women and men.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\CQIZQVXS\Körner et al. - 2024 - Examining the glottal stop as a mark of gender-inc.pdf}
}

@article{kornerGenderRepresentationsElicited2022,
  title = {Gender {{Representations Elicited}} by the {{Gender Star Form}}},
  author = {K{\"o}rner, Anita and Abraham, Bleen and Rummer, Ralf and Strack, Fritz},
  year = {2022},
  month = oct,
  journal = {Journal of Language and Social Psychology},
  volume = {41},
  number = {5},
  pages = {553--571},
  issn = {0261-927X, 1552-6526},
  doi = {10.1177/0261927X221080181},
  urldate = {2024-12-17},
  abstract = {In many languages, masculine language forms are not only used to designate the male gender but also to operate in a generic fashion. This dual function has been found to lead to male biased representations when people encounter the generic masculine. In German, the now predominant substitute is the gender star form (e.g., Athlet*innen). In two experiments, we examined gender representations elicited when reading the gender star form (vs. generic masculine vs. pair forms). We found that, following the generic masculine, continuations about men (vs. women) were more frequently and more quickly judged to be compatible, replicating the male bias, even though participants were informed about the generic intention. Following the gender star form, a female bias in judgments (both Studies) and speed (only Study 2) occurred, which was somewhat smaller. Representations were most balanced when both male and female forms were mentioned.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\8EE8TGZV\Körner et al. - 2022 - Gender Representations Elicited by the Gender Star Form.pdf}
}

@inproceedings{kotekGenderBiasStereotypes2023,
  title = {Gender Bias and Stereotypes in {{Large Language Models}}},
  booktitle = {Proceedings of {{The ACM Collective Intelligence Conference}}},
  author = {Kotek, Hadas and Dockum, Rikker and Sun, David Q.},
  year = {2023},
  month = nov,
  eprint = {2308.14921},
  primaryclass = {cs},
  pages = {12--24},
  doi = {10.1145/3582269.3615599},
  urldate = {2025-01-07},
  abstract = {Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women's occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender; (b) these choices align with people's perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95\% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {C\:\\Users\\spide\\Zotero\\storage\\YIPGSPP8\\Kotek et al. - 2023 - Gender bias and stereotypes in Large Language Models.pdf;C\:\\Users\\spide\\Zotero\\storage\\GD9T2CQ3\\2308.html}
}

@article{krieg-planqueNOVLANGUELANGUEIMAGINAIRE,
  title = {{LA << NOVLANGUE >> : UNE LANGUE IMAGINAIRE AU SERVICE DE LA CRITIQUE DU << DISCOURS AUTRE >>}},
  author = {{Krieg-Planque}, Alice},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\8EP4EUJM\LA « NOVLANGUE »  UNE LANGUE IMAGINAIRE AU SERVICE DE LA CRITIQUE DU « DISCOURS AUTRE » (Krieg-Planque, ).pdf}
}

@book{labovSocialStratificationEnglish1966,
  title = {The {{Social Stratification}} of {{English}} in {{New York City}}},
  author = {Labov, William},
  year = {1966},
  publisher = {Center for Applied Linguistics, University of Michigan},
  isbn = {0-87281-149-2}
}

@article{lacazeReflexionsAutourVariation2022,
  title = {{R{\'e}flexions autour de la variation diam{\'e}sique}},
  author = {Lacaze, Gr{\'e}goire},
  year = {2022},
  month = jun,
  journal = {E-rea},
  number = {19.2},
  pages = {20},
  issn = {1638-1718},
  doi = {10.4000/erea.13428},
  urldate = {2022-07-12},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\QZNVAZXR\Réflexions autour de la variation diamésique (Lacaze, 2022).pdf}
}

@article{lakoffLanguageWomansPlace1973,
  title = {Language and {{Woman}}'s {{Place}}},
  author = {Lakoff, Robin},
  year = {1973},
  journal = {Language in Society},
  volume = {2},
  number = {1},
  eprint = {4166707},
  eprinttype = {jstor},
  pages = {45--80},
  doi = {10.2307/4166707},
  urldate = {2023-01-19},
  abstract = {Our use of language embodies attitudes as well as referential meanings. 'Woman's language' has as foundation the attitude that women are marginal to the serious concerns of life, which are pre-empted by men. The marginality and powerlessness of women is reflected in both the ways women are expected to speak, and the ways in which women are spoken of. In appropriate women's speech, strong expression of feeling is avoided, expression of uncertainty is favored, and means of expression in regard to subject-matter deemed 'trivial' to the 'real' world are elaborated. Speech about women implies an object, whose sexual nature requires euphemism, and whose social roles are derivative and dependent in relation to men. The personal identity of women thus is linguistically submerged; the language works against treatment of women, as serious persons with individual views.},
  jstor = {4166707}
}

@book{lakoffWomenFireDangerous20,
  title = {Women, Fire, and Dangerous Things: What Categories Reveal about the Mind},
  shorttitle = {Women, Fire, and Dangerous Things},
  author = {Lakoff, George},
  year = {20},
  edition = {paperback ed., [Nachdr.]},
  publisher = {The Univ. of Chicago Press},
  address = {Chicago},
  isbn = {978-0-226-46804-4},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\AX2FJX4R\Lakoff - Women, fire, and dangerous things what categories reveal about the mind.pdf}
}

@phdthesis{lambeletLapprentissageGenreGrammatical2012,
  title = {L'apprentissage Du Genre Grammatical En Langue {\'E}trang{\`e}re : {\`a} La Crois{\'e}e Des Approches Linguistiques et Cognitives},
  author = {Lambelet, Amelia},
  year = {2012},
  address = {Fribourg},
  urldate = {2023-01-19},
  school = {Universit{\'e} de Fribourg}
}

@article{lammertNomsCollectifsFrancais2014,
  title = {Les Noms Collectifs En Fran{\c c}ais, Une Vue d'ensemble},
  author = {Lammert, Marie and Lecolle, Michelle},
  year = {2014},
  journal = {Cahiers de lexicologie},
  series = {2},
  number = {105},
  pages = {203--222},
  urldate = {2022-11-23},
  abstract = {Nous pr{\'e}sentons ici un panorama des questions touchant les noms collectifs (Ncoll) en fran{\c c}ais. Les Ncoll sont des noms exprimant au singulier une pluralit{\'e}. De cette caract{\'e}risation d{\'e}coulent plusieurs propri{\'e}t{\'e}s distributionnelles et s{\'e}mantiques, que nous abordons dans une premi{\`e}re partie : pluralit{\'e} s{\'e}mantique, double niveau du tout et des {\'e}l{\'e}ments, rapport entre ces deux niveaux. Dans une deuxi{\`e}me partie, nous proposons des crit{\`e}res pouvant aboutir {\`a} un classement des Ncoll (crit{\`e}re ontologique, de mode de rassemblement des {\'e}l{\'e}ments, de ressemblance), et nous pr{\'e}sentons des types de d{\'e}rivations s{\'e}mantiques (m{\'e}tonymie et synecdoque) qui donnent lieu {\`a} des sens collectifs pour diff{\'e}rents noms. En conclusion, nous rapprochons les caract{\'e}ristiques s{\'e}mantiques des Ncoll des effets de sens qu'ils peuvent provoquer et des usages rh{\'e}toriques qui peuvent en {\^e}tre faits.},
  keywords = {lu,noms collectifs}
}

@article{lammertReferenceCollectiveMassive2014,
  title = {R{\'e}f{\'e}rence Collective Massive vs R{\'e}f{\'e}rence Plurielle Ind{\'e}finie:},
  shorttitle = {R{\'e}f{\'e}rence Collective Massive vs R{\'e}f{\'e}rence Plurielle Ind{\'e}finie},
  author = {Lammert, Marie},
  year = {2014},
  month = oct,
  journal = {Langue fran{\c c}aise},
  volume = {n{$^\circ$} 183},
  number = {3},
  pages = {87--99},
  issn = {0023-8368},
  doi = {10.3917/lf.183.0087},
  urldate = {2023-02-05},
  keywords = {a lire}
}

@book{lammertSemantiqueCognitionNoms2010,
  title = {S{\'e}mantique et Cognition : Les Noms Collectifs},
  author = {Lammert, Marie},
  year = {2010},
  publisher = {Droz},
  address = {Gen{\`e}ve},
  isbn = {978-2-600-01312-3}
}

@article{landraginReferenceCoreferencePronom2014,
  title = {{R{\'e}f{\'e}rence et cor{\'e}f{\'e}rence du pronom ind{\'e}fini on:}},
  shorttitle = {{R{\'e}f{\'e}rence et cor{\'e}f{\'e}rence du pronom ind{\'e}fini on}},
  author = {Landragin, Fr{\'e}d{\'e}ric and Tanguy, Noalig},
  year = {2014},
  month = sep,
  journal = {Langages},
  volume = {N{$^\circ$} 195},
  number = {3},
  pages = {16},
  issn = {0458-726X},
  doi = {10.3917/lang.195.0099},
  urldate = {2022-11-03},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\F38NAVTR\Référence et coréférence du pronom indéfini on (Landragin and Tanguy, 2014).pdf}
}

@article{lawrenceRightDreamMillers1964,
  title = {The {{Right Dream}} in {{Miller}}'s {{Death}} of a {{Salesman}}},
  author = {Lawrence, Stephen A.},
  year = {1964},
  month = apr,
  journal = {College English},
  volume = {25},
  number = {7},
  eprint = {373244},
  eprinttype = {jstor},
  pages = {4},
  issn = {00100994},
  doi = {10.2307/373244},
  urldate = {2021-01-29},
  langid = {english},
  keywords = {miller},
  file = {C:\Users\spide\Zotero\storage\K96MX9CG\The Right Dream in Miller's Death of a Salesman (Lawrence, 1964).pdf}
}

@article{lecolleIdentiteAlteriteNoms2008,
  title = {Identit{\'e}/Alt{\'e}rit{\'e} et Noms Collectifs Humains : Le Cas de Communaut{\'e}},
  shorttitle = {Identit{\'e}/Alt{\'e}rit{\'e} et Noms Collectifs Humains},
  author = {Lecolle, Michelle},
  year = {2008},
  journal = {Questions de communication},
  number = {13},
  pages = {323--342},
  issn = {1633-5961},
  doi = {10.4000/questionsdecommunication.1862},
  urldate = {2022-11-13},
  abstract = {En articulant faits de langue et faits de discours, l'article aborde la question de l'identit{\'e} du point de vue des groupes humains sous un angle lexical et s{\'e}mantique, par le biais des noms collectifs humains et particuli{\`e}rement du nom collectif communaut{\'e}, que l'on contraste avec d'autres noms collectifs, davantage marqu{\'e}s a priori d'un point de vue << identitaire >> comme ethnie et race. Apr{\`e}s avoir pr{\'e}sent{\'e} la notion de nom collectif en s{\'e}mantique, il d{\'e}crit, pour communaut{\'e}, la diversit{\'e} de ses sens et emplois r{\'e}f{\'e}rentiels. Partant de la constatation de la grande flexibilit{\'e} de ce nom collectif, dont on peut dire qu'il est employ{\'e} comme un << mot-joker >>, il cherche {\`a} montrer que, dans le contexte de faits sociaux discut{\'e}s et probl{\'e}matis{\'e}s dans les discours publics, comme celui de l'identit{\'e} et de son corollaire, l'alt{\'e}rit{\'e}, communaut{\'e} voit son sens se sp{\'e}cifier pour devenir un d{\'e}signateur << identitaire >>. C'est ce qu'on observe, d'une part, dans la structure syntaxique [la communaut{\'e} + adjectif cat{\'e}gorisant], d'autre part, en {\'e}tudiant ses d{\'e}riv{\'e}s communautaire et communautarisme dont on postule que leur sens (suppos{\'e} d{\'e}riv{\'e}) op{\`e}re un effet en retour sur le sens de communaut{\'e}.},
  keywords = {lu,noms collectifs,noms humains}
}

@article{lecolleNomsCollectifsHumains2013,
  title = {Noms Collectifs Humains : Un Point de Vue de S{\'e}mantique Lexicale Sur l'identit{\'e} Dans Le Rapport Individu/Groupe},
  author = {Lecolle, Michelle},
  year = {2013},
  journal = {{\textquestiondown} Interrogations ?},
  number = {16},
  issn = {1778-3747},
  urldate = {2022-11-23},
  abstract = {L'article aborde la question de l'identit{\'e} en linguistique et cherche {\`a} montrer l'importance des ph{\'e}nom{\`e}nes de langue et de discours dans la description de l'identit{\'e} des {\^e}tres humains. On propose une approche de cette question en termes de rapport individu/groupe, en d{\'e}crivant les caract{\'e}ristiques s{\'e}mantiques des noms collectifs -- noms au singulier d{\'e}signant des groupes (foule, public, caste, population). Apr{\`e}s avoir pr{\'e}sent{\'e} plusieurs d{\'e}finitions philosophiques de l'identit{\'e}, on {\'e}value leur pertinence quant au rapport individu/groupe et au groupe lui-m{\^e}me. Dans une deuxi{\`e}me partie, on pr{\'e}sente diff{\'e}rents noms collectifs, qu'on classe selon la fa{\c c}on dont ils regroupent les individus. Une derni{\`e}re partie aborde l'identit{\'e} par le biais de son corollaire, l'alt{\'e}rit{\'e} : on propose ici plusieurs exemples de noms collectifs dont la signification, ou les emplois en discours reposent sur l'alt{\'e}rit{\'e}.},
  keywords = {lu,noms collectifs,noms humains}
}

@article{lecolleNomsCollectifsHumains2016,
  title = {Noms Collectifs Humains : Nomination et Pr{\'e}dication},
  author = {Lecolle, Michelle},
  year = {2016},
  journal = {Argumentation et analyse du discours},
  number = {17},
  issn = {1565-8961},
  doi = {10.4000/aad.2208},
  urldate = {2022-11-03},
  abstract = {L'article porte sur la nomination collective d'{\^e}tres humains : il s'agit, plus pr{\'e}cis{\'e}ment, d'aborder la s{\'e}mantique des noms collectifs humains (Ncoll-H), sous l'angle d'une dimension {\'e}valuative ou pr{\'e}dicative potentielle. On observe, {\`a} propos des Ncoll-H, cette dimension pr{\'e}dicative sous diff{\'e}rents aspects : elle peut {\^e}tre li{\'e}e {\`a} la s{\'e}mantique du nom collectif m{\^e}me, aux traits lexicaux de telle s{\'e}rie de Ncoll-H ou {\`a} leurs emplois pr{\'e}f{\'e}rentiels. Une premi{\`e}re partie explore le rapprochement entre nomination et pr{\'e}dication. Dans une deuxi{\`e}me partie, on pr{\'e}sente les propri{\'e}t{\'e}s s{\'e}mantiques des Ncoll-H et la signification lexicale de certains, en rapport avec leurs potentialit{\'e}s pr{\'e}dicatives ou {\'e}valuatives. Une derni{\`e}re partie aborde les emplois attributifs de Ncoll-H. Cette {\'e}tude permet de situer les << enjeux socio-politiques >> de la nomination de groupes humains dans le cadre d'une analyse s{\'e}mantique et discursive plus g{\'e}n{\'e}rale.},
  keywords = {lu,noms collectifs,noms humains}
}

@book{lecolleNomsCollectifsHumains2019,
  title = {Les Noms Collectifs Humains En Fran{\c c}ais. {{Enjeux}} S{\'e}mantiques, Lexicaux et Discursifs},
  author = {Lecolle, Michelle},
  editor = {{Jacquet-Pfau}, Christine},
  year = {2019},
  publisher = {Lambert-Lucas},
  address = {Universit{\'e} de Lorraine},
  abstract = {On entend par nom collectif humain un nom qui renvoie de mani{\`e}re cod{\'e}e {\`a} un groupe constitu{\'e} d'{\^e}tres humains (foule, parti, peuple, public, patronat, {\'e}lite{\dots}). Outre le singulier morphologique, le nom collectif humain associe trois caract{\'e}ristiques{\textbackslash},: (1) {\textbackslash},la r{\'e}f{\'e}rence {\`a} l'humain, (2) {\textbackslash},le regroupement, (3) {\textbackslash},la pluralit{\'e}. Les noms collectifs ont d{\'e}j{\`a} fait l'objet de nombreuses recherches, mais le trait humain n'a que peu {\'e}t{\'e} explor{\'e}. Le pr{\'e}sent ouvrage montre comment il intervient sur le sens des lex{\`e}mes, sur leur polys{\'e}mie, sur les s{\'e}ries lexicales ainsi que sur les causes de cr{\'e}ation lexicale et de lexicalisation, et leurs contextes. Ces noms peuvent {\^e}tre mis au service de vis{\'e}es discursives -- descriptives, ludiques, po{\'e}tiques, argumentatives -- par la mise en exergue des valeurs de groupe, d'appartenance ou d'identit{\'e} sociale, les ambigu{\"i}t{\'e}s syst{\'e}miques, les jeux discursifs sur le rapport entre le tout et la partie. La compr{\'e}hension de cet objet complexe sollicite diverses disciplines des sciences humaines, susceptibles d'informer le point de vue linguistique et d'en {\^e}tre inform{\'e}es en retour.},
  isbn = {978-2-35935-280-1},
  keywords = {a lire,noms collectifs,noms humains}
}

@misc{leeGradientBasedInferenceNetworks2019,
  title = {Gradient-{{Based Inference}} for {{Networks}} with {{Output Constraints}}},
  author = {Lee, Jay Yoon and Mehta, Sanket Vaibhav and Wick, Michael and Tristan, Jean-Baptiste and Carbonell, Jaime},
  year = {2019},
  month = apr,
  eprint = {1707.08608},
  urldate = {2023-02-27},
  abstract = {Practitioners apply neural networks to increasingly complex problems in natural language processing, such as syntactic parsing and semantic role labeling that have rich output structures. Many such structured-prediction problems require deterministic constraints on the output values; for example, in sequence-to-sequence syntactic parsing, we require that the sequential outputs encode valid trees. While hidden units might capture such properties, the network is not always able to learn such constraints from the training data alone, and practitioners must then resort to post-processing. In this paper, we present an inference method for neural networks that enforces deterministic constraints on outputs without performing rule-based post-processing or expensive discrete search. Instead, in the spirit of gradient-based training, we enforce constraints with gradient-based inference (GBI): for each input at test-time, we nudge continuous model weights until the network's unconstrained inference procedure generates an output that satisfies the constraints. We study the efficacy of GBI on three tasks with hard constraints: semantic role labeling, syntactic parsing, and sequence transduction. In each case, the algorithm not only satisfies constraints, but improves accuracy, even when the underlying network is stateof-the-art.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/1707.08608},
  keywords = {Computer Science - Computation and Language}
}

@article{leemanHurlerRageRayonner1991,
  title = {{Hurler de rage, rayonner de bonheur : Remarques sur une construction en de}},
  shorttitle = {{Hurler de rage, rayonner de bonheur}},
  author = {Leeman, Danielle},
  year = {1991},
  journal = {Langue fran{\c c}aise},
  volume = {91},
  number = {1},
  pages = {80--101},
  issn = {0023-8368},
  doi = {10.3406/lfr.1991.6206},
  urldate = {2022-08-13},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\LSLSXHMV\Hurler de rage, rayonner de bonheur  Remarques sur une construction en de (Leeman, 1991).pdf}
}

@article{lefflerUnitedStatesStrategic1988,
  title = {The {{United States}} and the {{Strategic Dimensions}} of the {{Marshall Plan}}},
  author = {Leffler, Melvyn P.},
  year = {1988},
  month = jul,
  journal = {Diplomatic History},
  volume = {12},
  number = {3},
  pages = {30},
  issn = {0145-2096, 1467-7709},
  doi = {10.1111/j.1467-7709.1988.tb00477.x},
  urldate = {2021-12-17},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\XS454Q6Q\The United States and the Strategic Dimensions of the Marshall Plan (Leffler, 1988).pdf}
}

@article{leiResearchesNominalizationMajor2019,
  title = {Researches on {{Nominalization}} by {{Major Linguistics Schools}}},
  author = {Lei, Yue},
  year = {2019},
  volume = {7},
  number = {4},
  pages = {11},
  abstract = {As a kind of lexical and grammatical resource, nominalization exerts a crucial part in language activities and draws great attention in academic fields. This study reviewed theoretical researches on nominalization by major linguistic schools, which are structural school, transformational-generative school, cognitive school and systemic functional school. Through reviewing these studies, in terms of classification of nominalization, it is found that apart from systemic functional school, other three schools mainly focused on nouns which are derived from verbs and adjectives, therefore, neglecting other possibilities. Besides, to some extent, other schools' studies on nominalization are still inadequate, and they mainly study the form or syntactic level, seldom investigating functions of nominalization in context. Therefore, compared with other schools, systemic functional school proposed that nominalization belongs to metaphorical language and the formation of nominalization undergoes a complicated process, instead of adding suffix. Thus, under the theoretical basis of systemic functional grammar, nominalization has profound meaning.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\AP6WGP29\Researches on Nominalization by Major Linguistics Schools (Lei, 2019).pdf}
}

@article{lenepveuAdverbesMentQuantifieurs,
  title = {{Des adverbes en --ment quantifieurs de totalit{\'e} : totalement, compl{\`e}tement et enti{\`e}rement}},
  author = {Lenepveu, V{\'e}ronique},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\A6HI9LQA\Des adverbes en –ment quantifieurs de totalité  totalement, complètement et entièrement (Lenepveu, ).pdf}
}

@article{lenzEndApotheosisLabor2005,
  title = {The {{End}} or the {{Apotheosis}} of "{{Labor}}"? {{Hannah Arendt}}'s {{Contribution}} to the {{Question}} of the {{Good Life}} in {{Times}} of {{Global Superfluity}} of {{Human Labor Power}}},
  shorttitle = {The {{End}} or the {{Apotheosis}} of "{{Labor}}"?},
  author = {Lenz, Claudia and Postl, Gertrude},
  year = {2005},
  journal = {Hypatia},
  volume = {20},
  number = {2},
  eprint = {3811168},
  eprinttype = {jstor},
  pages = {135--154},
  publisher = {[Hypatia, Inc., Wiley]},
  issn = {0887-5367},
  urldate = {2022-01-07},
  abstract = {This paper relates Arendt's critique of a labor society to her thoughts on the "good life." I begin with the claim that in the post-mass production era, Western societies, traditionally centered around gainful employment, encounter a decrease in the relevance of labor and can thus no longer rely on it as a resource for individual or social meaning. From Arendt's perspective, however, the current situation allows for the possibility of a transition from a society based on labor to a society centered around activities. I explore Arendt's different types of activities-labor, work, action-with respect to the question of justice between the genders.}
}

@article{lernerINCLUREDatasetToolkit2024,
  title = {{{INCLURE}}: A {{Dataset}} and {{Toolkit}} for {{Inclusive French Translation}}},
  author = {Lerner, Paul and Grouin, Cyril},
  year = {2024},
  abstract = {Inclusive French (gender-neutral language) is a variety of French that is used to highlight awareness of gender and identity against Standard French, which enforces the use of masculine for generic usage or plural. Although widely used and challenging to a set of NLP tools, Inclusive French was very little studied in NLP. Detractors of Inclusive French argue that it is difficult to read, while its supporters argue that it provides a fairer representation of women and gender minorities. We provide Inclure, the first large-scale parallel corpus for Standard to Inclusive French translation, and vice-versa, thus providing a ``bilingual'' access to French, for both detractors and supporters of Inclusive French. This corpus comes with a toolkit that can be readily applied to larger French corpora and could be extended to other languages, for which the number of inclusive varieties is growing. We also provide Fabien.ne BARThez, a sequence-to-sequence model trained on Inclure. Apart from its direct application to translation, this model could also be used in most NLP pipelines, either as a pre-processing step to improve downstream processing or as a post-processing according to the user's preference.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\HV76VAXX\Lerner and Grouin - INCLURE a Dataset and Toolkit for Inclusive French Translation.pdf}
}

@article{lewisParadoxesTimeTravel,
  title = {The {{Paradoxes}} of {{Time Travel}}},
  author = {Lewis, David},
  pages = {7},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\3GRLKENS\THE PARADOXES OF TIME TRAVEL (Lewis, ).pdf}
}

@article{lightfootNothingSyntaxMakes2018,
  title = {Nothing in {{Syntax Makes Sense Except}} in the {{Light}} of {{Change}}},
  author = {Lightfoot, David W.},
  editor = {Gallego, {\'A}ngel J. and Martin, Roger},
  year = {2018},
  month = oct,
  journal = {Language, Syntax, and the Natural Sciences},
  pages = {26},
  doi = {10.1017/9781316591529.013},
  urldate = {2022-08-18},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\FUINTV7P\Nothing in Syntax Makes Sense Except in the Light of Change (Lightfoot, 2018).pdf}
}

@article{lindheimRethinkingSexualityClass2007,
  title = {Rethinking {{Sexuality}} and {{Class}} in {{Twelfth Night}}},
  author = {Lindheim, Nancy},
  year = 2007,
  journal = {University of toronto quarterly},
  volume = {76},
  pages = {679--713},
  langid = {english},
  keywords = {shakespeare},
  file = {C:\Users\spide\Documents\PDF recherches\Rethinking Sexuality and Class in Twelfth Night (Lindheim, 2007).pdf}
}

@article{LINDIENCATLINAUTRE,
  title = {{L'INDIEN DE CATLIN : AUTRE OU ALTER EGO}},
  pages = {11},
  langid = {french},
  keywords = {civi cult. hist.},
  annotation = {CIVI CUL. HIST.},
  file = {C:\Users\spide\Zotero\storage\4I9L3NG5\L'INDIEN DE CATLIN  AUTRE OU ALTER EGO (, ).pdf}
}

@article{linUnderstandingMicronoteLifecycle2004,
  title = {Understanding the {{Micronote Lifecycle}}: {{Improving Mobile Support}} for {{Informal Note Taking}}},
  author = {Lin, Min and Lutters, Wayne G and Kim, Tina S},
  year = {2004},
  volume = {6},
  number = {1},
  pages = {8},
  abstract = {People frequently write messages to themselves. These informal, hurried personal jottings serve as temporary storage for notable information as well as reminders for future action. Many mobile technologies have been designed specifically to support this ubiquitous behavior; however, adoption has been universally problematic. Despite its clear utility, the process of taking micronotes stubbornly resists computing support. This field study examines the lifecycles of the canonical micronote forms (immediate use, temporary storage, and prospective memory aid), pinpointing the behaviors that are mismatched with current mobile support. Implications for improving the design of these systems are presented, culminating in a vision for integrated paper-digital micronote systems. This shifts the development focus away from trying to support the entire micronote lifecycle, emphasizing instead the different behaviors best supported by the different technologies.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\N8YPG9X8\Understanding the Micronote Lifecycle Improving Mobile Support for Informal Note Taking (Lin et al., 2004).pdf}
}

@misc{liPreTrainedLanguageModels2022,
  title = {Pre-{{Trained Language Models}} for {{Interactive Decision-Making}}},
  author = {Li, Shuang and Puig, Xavier and Paxton, Chris and Du, Yilun and Wang, Clinton and Fan, Linxi and Chen, Tao and Huang, De-An and Aky{\"u}rek, Ekin and Anandkumar, Anima and Andreas, Jacob and Mordatch, Igor and Torralba, Antonio and Zhu, Yuke},
  year = {2022},
  month = oct,
  number = {arXiv:2202.01771},
  eprint = {2202.01771},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.01771},
  urldate = {2024-12-17},
  abstract = {Language model (LM) pre-training is useful in many language processing tasks. But can pre-trained LMs be further leveraged for more general machine learning problems? We propose an approach for using LMs to scaffold learning and generalization in general sequential decision-making problems. In this approach, goals and observations are represented as a sequence of embeddings, and a policy network initialized with a pre-trained LM predicts the next action. We demonstrate that this framework enables effective combinatorial generalization across different environments and supervisory modalities. We begin by assuming access to a set of expert demonstrations, and show that initializing policies with LMs and fine-tuning them via behavior cloning improves task completion rates by 43.6\% in the VirtualHome environment. Next, we integrate an active data gathering procedure in which agents iteratively interact with the environment, relabel past "failed" experiences with new goals, and update their policies in a self-supervised loop. Active data gathering further improves combinatorial generalization, outperforming the best baseline by 25.1\%. Finally, we explain these results by investigating three possible factors underlying the effectiveness of the LM-based policy. We find that sequential input representations (vs. fixed-dimensional feature vectors) and LM-based weight initialization are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans; these representations can aid learning and generalization even outside of language processing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\spide\\Zotero\\storage\\ZPVB7ULF\\Li et al. - 2022 - Pre-Trained Language Models for Interactive Decision-Making.pdf;C\:\\Users\\spide\\Zotero\\storage\\88N8CGEM\\2202.html}
}

@misc{liuMitigatingGenderBias2020,
  title = {Mitigating {{Gender Bias}} for {{Neural Dialogue Generation}} with {{Adversarial Learning}}},
  author = {Liu, Haochen and Wang, Wentao and Wang, Yiqi and Liu, Hui and Liu, Zitao and Tang, Jiliang},
  year = {2020},
  month = oct,
  number = {arXiv:2009.13028},
  eprint = {2009.13028},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-12},
  abstract = {Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people's gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality. The implementation of the proposed framework is released1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\spide\Zotero\storage\SKMRJ44G\Liu et al. - 2020 - Mitigating Gender Bias for Neural Dialogue Generat.pdf}
}

@misc{liuPreTrainPromptPredict2021,
  title = {Pre-{{Train}}, {{Prompt}}, and {{Predict}}: {{A Systematic Survey}} of {{Prompting Methods}} in {{Natural Language Processing}}},
  shorttitle = {Pre-{{Train}}, {{Prompt}}, and {{Predict}}},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  year = {2021},
  month = jul,
  eprint = {2107.13586},
  urldate = {2024-03-18},
  abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2107.13586},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{liuTokenlevelReferencefreeHallucination2022,
  title = {A {{Token-level Reference-free Hallucination Detection Benchmark}} for {{Free-form Text Generation}}},
  author = {Liu, Tianyu and Zhang, Yizhe and Brockett, Chris and Mao, Yi and Sui, Zhifang and Chen, Weizhu and Dolan, Bill},
  year = {2022},
  month = apr,
  eprint = {2104.08704},
  urldate = {2024-03-18},
  abstract = {Large pretrained generative models like GPT3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications. Existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level. However ground-truth references may not be readily available for many free-form text generation applications, and sentence- or document-level detection may fail to provide the fine-grained signals that would prevent fallacious content in real time. As a first step to addressing these issues, we propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named HADES (HAllucination DEtection dataSet) 1. To create this dataset, we first perturb a large number of text segments extracted from English language Wikipedia, and then verify these with crowdsourced annotations. To mitigate label imbalance during annotation, we utilize an iterative model-in-loop strategy. We conduct comprehensive data analyses and create multiple baseline models.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2104.08704},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{llphiNdegattribueParBibliotheque,
  title = {{N{$^\circ$}attribu{\'e} par la biblioth{\`e}que}},
  author = {Llphi, U F R},
  pages = {390},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\XVE3TX5X\N°attribué par la bibliothèque (Llphi, ).pdf}
}

@misc{longLLMsDrivenSyntheticData2024,
  title = {On {{LLMs-Driven Synthetic Data Generation}}, {{Curation}}, and {{Evaluation}}: {{A Survey}}},
  shorttitle = {On {{LLMs-Driven Synthetic Data Generation}}, {{Curation}}, and {{Evaluation}}},
  author = {Long, Lin and Wang, Rui and Xiao, Ruixuan and Zhao, Junbo and Ding, Xiao and Chen, Gang and Wang, Haobo},
  year = {2024},
  month = jun,
  number = {arXiv:2406.15126},
  eprint = {2406.15126},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.15126},
  urldate = {2025-01-23},
  abstract = {Within the evolving landscape of deep learning, the dilemma of data quantity and quality has been a long-standing problem. The recent advent of Large Language Models (LLMs) offers a data-centric solution to alleviate the limitations of real-world data with synthetic data generation. However, current investigations into this field lack a unified framework and mostly stay on the surface. Therefore, this paper provides an organization of relevant studies based on a generic workflow of synthetic data generation. By doing so, we highlight the gaps within existing research and outline prospective avenues for future study. This work aims to shepherd the academic and industrial communities towards deeper, more methodical inquiries into the capabilities and applications of LLMs-driven synthetic data generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\UDTEECDA\\Long et al. - 2024 - On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation A Survey.pdf;C\:\\Users\\spide\\Zotero\\storage\\U867QLH4\\2406.html}
}

@incollection{luGenderBiasNeural2020,
  title = {Gender {{Bias}} in {{Neural Natural Language Processing}}},
  booktitle = {Logic, {{Language}}, and {{Security}}},
  author = {Lu, Kaiji and Mardziel, Piotr and Wu, Fangjing and Amancharla, Preetam and Datta, Anupam},
  editor = {Nigam, Vivek and Ban Kirigin, Tajana and Talcott, Carolyn and Guttman, Joshua and Kuznetsov, Stepan and Thau Loo, Boon and Okada, Mitsuhiro},
  year = {2020},
  volume = {12300},
  pages = {189--202},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-62077-6_14},
  urldate = {2024-12-17},
  isbn = {978-3-030-62076-9 978-3-030-62077-6},
  langid = {english}
}

@incollection{lundquistMorphologyProtoIndoEuropean2018,
  title = {The Morphology of {{Proto-Indo-European}}},
  booktitle = {Handbook of {{Comparative}} and {{Historical Indo-European Linguistics}}},
  author = {Lundquist, Jesse and Yates, Anthony D.},
  editor = {Klein, Jared and Joseph, Brian and Fritz, Matthias},
  year = {2018},
  month = jun,
  pages = {2079--2195},
  publisher = {De Gruyter},
  doi = {10.1515/9783110542431-043},
  urldate = {2024-12-14},
  isbn = {978-3-11-054243-1},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\4Q3N93HQ\Lundquist and Yates - 2018 - 122. The morphology of Proto-Indo-European.pdf}
}

@article{luraghiOriginProtoIndoEuropeanGender2011,
  title = {The Origin of the {{Proto-Indo-European}} Gender System: {{Typological}} Considerations},
  shorttitle = {The Origin of the {{Proto-Indo-European}} Gender System},
  author = {Luraghi, Silvia},
  year = {2011},
  month = jan,
  journal = {Folia Linguistica},
  volume = {45},
  number = {2},
  issn = {0165-4004, 1614-7308},
  doi = {10.1515/flin.2011.016},
  urldate = {2024-11-21},
  langid = {english},
  keywords = {read},
  file = {C:\Users\spide\Zotero\storage\5RETU6BF\Luraghi - 2011 - The origin of the Proto-Indo-European gender system Typological considerations.pdf}
}

@inbook{lynnNickCarraway1989,
  title = {Within and {{Without}}: {{Nick Carraway}}},
  shorttitle = {Within and {{Without}}},
  booktitle = {The {{Hero}}'s {{Tale}}},
  author = {Lynn, David H.},
  year = {1989},
  pages = {19},
  publisher = {Palgrave Macmillan UK},
  address = {London},
  urldate = {2021-08-22},
  collaborator = {Lynn, David H.},
  isbn = {978-1-349-19718-7 978-1-349-19716-3},
  langid = {english},
  keywords = {fitzgerald},
  annotation = {GATSBY},
  file = {C:\Users\spide\Zotero\storage\EVJQ3VCE\Within and Without Nick Carraway (Lynn, 1989).pdf}
}

@misc{makelovPrincipledEvaluationsSparse2024,
  title = {Towards {{Principled Evaluations}} of {{Sparse Autoencoders}} for {{Interpretability}} and {{Control}}},
  author = {Makelov, Aleksandar and Lange, George and Nanda, Neel},
  year = {2024},
  month = may,
  number = {arXiv:2405.08366},
  eprint = {2405.08366},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-11},
  abstract = {Disentangling model activations into meaningful features is a central problem in interpretability. However, the absence of ground-truth for these features in realistic scenarios makes validating recent approaches, such as sparse dictionary learning, elusive. To address this challenge, we propose a framework for evaluating feature dictionaries in the context of specific tasks, by comparing them against supervised feature dictionaries. First, we demonstrate that supervised dictionaries achieve excellent approximation, control, and interpretability of model computations on the task. Second, we use the supervised dictionaries to develop and contextualize evaluations of unsupervised dictionaries along the same three axes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\spide\Zotero\storage\MQNSDN9M\Makelov et al. - 2024 - Towards Principled Evaluations of Sparse Autoencod.pdf}
}

@misc{marchitanQwenItDetect2025,
  title = {Qwen It Detect Machine-Generated Text?},
  author = {Marchitan, Teodor-George and Creanga, Claudiu and Dinu, Liviu P.},
  year = {2025},
  month = jan,
  number = {arXiv:2501.09813},
  eprint = {2501.09813},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.09813},
  urldate = {2025-01-23},
  abstract = {This paper describes the approach of the Unibuc - NLP team in tackling the Coling 2025 GenAI Workshop, Task 1: Binary Multilingual Machine-Generated Text Detection. We explored both masked language models and causal models. For Subtask A, our best model achieved first-place out of 36 teams when looking at F1 Micro (Auxiliary Score) of 0.8333, and second-place when looking at F1 Macro (Main Score) of 0.8301},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\7CZVZLQA\\Marchitan et al. - 2025 - Qwen it detect machine-generated text.pdf;C\:\\Users\\spide\\Zotero\\storage\\IU9RICV2\\2501.html}
}

@article{marianaguiarARRANGEDMARRIAGECULTURAL2013,
  title = {{{ARRANGED MARRIAGE}}: {{CULTURAL REGENERATION IN TRANSNATIONAL SOUTH ASIAN POPULAR CULTURE}}},
  shorttitle = {{{ARRANGED MARRIAGE}}},
  author = {{Marian Aguiar}},
  year = {2013},
  journal = {Cultural Critique},
  volume = {84},
  eprint = {10.5749/culturalcritique.84.2013.0181},
  eprinttype = {jstor},
  pages = {34},
  issn = {08824371},
  doi = {10.5749/culturalcritique.84.2013.0181},
  urldate = {2022-04-20},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\LEIC7RTT\ARRANGED MARRIAGE CULTURAL REGENERATION IN TRANSNATIONAL SOUTH ASIAN POPULAR CULTURE (Marian Aguiar, 2013).pdf}
}

@misc{marksSparseFeatureCircuits2024,
  title = {Sparse {{Feature Circuits}}: {{Discovering}} and {{Editing Interpretable Causal Graphs}} in {{Language Models}}},
  shorttitle = {Sparse {{Feature Circuits}}},
  author = {Marks, Samuel and Rager, Can and Michaud, Eric J. and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  year = {2024},
  month = mar,
  number = {arXiv:2403.19647},
  eprint = {2403.19647},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-13},
  abstract = {We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\spide\Zotero\storage\AL44IFDY\Marks et al. - 2024 - Sparse Feature Circuits Discovering and Editing I.pdf}
}

@inproceedings{martinCamemBERTTastyFrench2020,
  title = {{{CamemBERT}}: A {{Tasty French Language Model}}},
  shorttitle = {{{CamemBERT}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Martin, Louis and Muller, Benjamin and Su{\'a}rez, Pedro Javier Ortiz and Dupont, Yoann and Romary, Laurent and de la Clergerie, {\'E}ric Villemonte and Seddah, Djam{\'e} and Sagot, Beno{\^i}t},
  year = {2020},
  eprint = {1911.03894},
  primaryclass = {cs},
  pages = {7203--7219},
  doi = {10.18653/v1/2020.acl-main.645},
  urldate = {2025-01-09},
  abstract = {Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models --in all languages except English-- very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\GGQZC97M\\Martin et al. - 2020 - CamemBERT a Tasty French Language Model.pdf;C\:\\Users\\spide\\Zotero\\storage\\DHSU2GJG\\1911.html}
}

@article{martinNatureTragedyArthur1996,
  title = {The {{Nature}} of {{Tragedy}} in {{Arthur Miller}}'s "{{Death}} of a {{Salesman}}"},
  author = {Martin, Robert A.},
  year = 1996,
  journal = {South Atlantic Review},
  volume = {61},
  number = {4},
  eprint = {3201170},
  eprinttype = {jstor},
  pages = {11},
  issn = {0277335X},
  doi = {10.2307/3201170},
  urldate = {2021-01-26},
  langid = {english},
  keywords = {favorite,miller},
  file = {C:\Users\spide\Zotero\storage\HCRXVJD7\The Nature of Tragedy in Arthur Miller's Death of a Salesman (Martin, 1996).pdf}
}

@article{martinSociolinguisticComparisonFrench,
  title = {A {{Sociolinguistic Comparison}} of the {{French}} and {{Anglo-Saxon Cultures From}} Codeswitched Substantives to Borrowings : The Issue of Grammatical Gender},
  author = {Martin, {\'E}lodie},
  langid = {english},
  keywords = {read},
  file = {C:\Users\spide\Zotero\storage\J2VMPI6K\Martin - A Sociolinguistic Comparison of the French and Anglo-Saxon Cultures From codeswitched substantives t.pdf}
}

@book{matasovicGenderIndoEuropean2004,
  title = {Gender in {{Indo-European}}},
  author = {Matasovi{\'c}, Ranko},
  year = {2004},
  series = {Indogermanische {{Bibliothek}} 3. {{Reihe}}},
  publisher = {Winter},
  address = {Heidelberg},
  abstract = {This book discusses the origin and history of the grammatical category of gender in the Indo-European family of languages. Gender systems of Proto-Indo-European (PIE), and of the various daughter languages are assessed from historical, typological, and areal points of view. In addition, common properties and tendencies (or drift) in the development of gender in different Indo-European branches are presented. The formal and semantic principles of gender assignment in PIE are examined on the basis of a reconstructed lexicon of PIE nouns, and the scope of gender agreement in the proto-language is reconstructed by comparing the agreement rules in the early Indo-European dialects. The Early PIE two-gender system and the development of the feminine gender in Late PIE are also discussed, and finally the PIE gender system is contrasted with the typologically rather different gender systems found in the neighboring areas of Eurasia},
  isbn = {978-3-8253-1666-2},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\MXFQZK49\Matasović - 2004 - Gender in Indo-European.pdf}
}

@article{matthewgentzkowPolarization2016,
  title = {Polarization in 2016},
  author = {{Matthew Gentzkow}},
  journal = {Stanford University},
  pages = {23},
  keywords = {history},
  file = {C:\Users\spide\Zotero\storage\9ANJDCAG\PolarizationIn2016.pdf}
}

@article{mcdermottBarackObamaAmericans2014,
  title = {Barack {{Obama}} and {{Americans}}' {{Racial Attitudes}}: {{Rallying}} and {{Polarization}}},
  shorttitle = {Barack {{Obama}} and {{Americans}}' {{Racial Attitudes}}},
  author = {McDermott, Monika L. and Belcher, Cornell},
  year = {2014},
  month = jul,
  journal = {Polity},
  volume = {46},
  number = {3},
  pages = {22},
  issn = {0032-3497, 1744-1684},
  doi = {10.1057/pol.2014.8},
  urldate = {2021-04-07},
  langid = {english},
  keywords = {history},
  file = {C:\Users\spide\Zotero\storage\GUR27NLD\McDermott and Belcher - 2014 - Barack Obama and Americans’ Racial Attitudes Rall.pdf}
}

@article{mcmahonEisenhowerThirdWorld1986,
  title = {Eisenhower and {{Third World Nationalism}}: {{A Critique}} of the {{Revisionists}}},
  shorttitle = {Eisenhower and {{Third World Nationalism}}},
  author = {McMahon, Robert J.},
  year = {1986},
  journal = {Political Science Quarterly},
  volume = {101},
  number = {3},
  eprint = {2151625},
  eprinttype = {jstor},
  pages = {22},
  issn = {00323195},
  doi = {10.2307/2151625},
  urldate = {2021-12-17},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\QZ4QN2QH\Eisenhower and Third World Nationalism A Critique of the Revisionists (McMahon, 1986).pdf}
}

@article{mcmahonMakingSenseAmerican1995,
  title = {Making {{Sense}} of {{American Foreign Policy}} during the {{Reagan Years}}},
  author = {Mcmahon, Robert J.},
  year = {1995},
  month = mar,
  journal = {Diplomatic History},
  volume = {19},
  number = {2},
  pages = {18},
  issn = {0145-2096, 1467-7709},
  doi = {10.1111/j.1467-7709.1995.tb00663.x},
  urldate = {2021-12-17},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\PWTAA5JM\Making Sense of American Foreign Policy during the Reagan Years (Mcmahon, 1995).pdf}
}

@misc{mcmilinSelectionInducedCollider2022,
  title = {Selection {{Induced Collider Bias}}: {{A Gender Pronoun Uncertainty Case Study}}},
  shorttitle = {Selection {{Induced Collider Bias}}},
  author = {McMilin, Emily},
  year = {2022},
  month = nov,
  eprint = {2210.00131},
  urldate = {2023-02-27},
  abstract = {In this paper, we cast the problem of task underspecification in causal terms, and develop a method for empirical measurement of spurious associations between gender and gender-neutral entities for unmodified large language models, detecting previously unreported spurious correlations. We then describe a lightweight method to exploit the resulting spurious associations for prediction task uncertainty classification, achieving over 90\% accuracy on a Winogender Schemas challenge set. Finally, we generalize our approach to address a wider range of prediction tasks and provide open-source demos for each method described here.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2210.00131},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@book{meilletLinguistiqueHistoriqueLinguistique1921,
  title = {{Linguistique historique et linguistique g{\'e}n{\'e}rale}},
  author = {Meillet, Antoine},
  year = {1921},
  publisher = {Lambert-Lucas},
  address = {Limoges},
  isbn = {978-2-35935-140-8},
  langid = {fre},
  lccn = {417.7},
  keywords = {read},
  file = {C:\Users\spide\Zotero\storage\CTNYN4N8\maillet_ling_hist.pdf}
}

@misc{meisenbacherDPMLMDifferentiallyPrivate2024,
  title = {{{DP-MLM}}: {{Differentially Private Text Rewriting Using Masked Language Models}}},
  shorttitle = {{{DP-MLM}}},
  author = {Meisenbacher, Stephen and Chevli, Maulik and Vladika, Juraj and Matthes, Florian},
  year = {2024},
  month = jun,
  number = {arXiv:2407.00637},
  eprint = {2407.00637},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-14},
  abstract = {The task of text privatization using Differential Privacy has recently taken the form of text rewriting, in which an input text is obfuscated via the use of generative (large) language models. While these methods have shown promising results in the ability to preserve privacy, these methods rely on autoregressive models which lack a mechanism to contextualize the private rewriting process. In response to this, we propose DP-MLM, a new method for differentially private text rewriting based on leveraging masked language models (MLMs) to rewrite text in a semantically similar and obfuscated manner. We accomplish this with a simple contextualization technique, whereby we rewrite a text one token at a time. We find that utilizing encoder-only MLMs provides better utility preservation at lower {$\varepsilon$} levels, as compared to previous methods relying on larger models with a decoder. In addition, MLMs allow for greater customization of the rewriting mechanism, as opposed to generative approaches. We make the code for DP-MLM public and reusable, found at https://github.com/sjmeis/DPMLM.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\spide\Zotero\storage\RI97TFPT\Meisenbacher et al. - 2024 - DP-MLM Differentially Private Text Rewriting Usin.pdf}
}

@misc{meisenbacherJustRewriteIt2024,
  title = {Just {{Rewrite It Again}}: {{A Post-Processing Method}} for {{Enhanced Semantic Similarity}} and {{Privacy Preservation}} of {{Differentially Private Rewritten Text}}},
  shorttitle = {Just {{Rewrite It Again}}},
  author = {Meisenbacher, Stephen and Matthes, Florian},
  year = {2024},
  month = may,
  number = {arXiv:2405.19831},
  eprint = {2405.19831},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-14},
  abstract = {The study of Differential Privacy (DP) in Natural Language Processing often views the task of text privatization as a rewriting task, in which sensitive input texts are rewritten to hide explicit or implicit private information. In order to evaluate the privacy-preserving capabilities of a DP text rewriting mechanism, empirical privacy tests are frequently employed. In these tests, an adversary is modeled, who aims to infer sensitive information (e.g., gender) about the author behind a (privatized) text. Looking to improve the empirical protections provided by DP rewriting methods, we propose a simple post-processing method based on the goal of aligning rewritten texts with their original counterparts, where DP rewritten texts are rewritten again. Our results show that such an approach not only produces outputs that are more semantically reminiscent of the original inputs, but also texts which score on average better in empirical privacy evaluations. Therefore, our approach raises the bar for DP rewriting methods in their empirical privacy evaluations, providing an extra layer of protection against malicious adversaries.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\spide\Zotero\storage\4D6XH2HR\Meisenbacher and Matthes - 2024 - Just Rewrite It Again A Post-Processing Method fo.pdf}
}

@book{mihatschNomsDhumainsCategorie2015,
  title = {{Les noms d'humains : une cat{\'e}gorie {\`a} part ?}},
  shorttitle = {{Les noms d'humains}},
  author = {Mihatsch, Wiltrud and Schnedecker, Catherine},
  year = {2015},
  series = {{Romanistik}},
  number = {Band 40},
  publisher = {Franz Steiner Verlag},
  address = {Stuttgart},
  isbn = {978-3-515-11157-7},
  langid = {fre}
}

@misc{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1301.3781},
  urldate = {2024-12-17},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\X2W7SEQA\\Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf;C\:\\Users\\spide\\Zotero\\storage\\76CSN6KU\\1301.html}
}

@article{miletichGenderedCuriosityThree2015,
  title = {Gendered {{Curiosity}}: {{Three Translations}} of an {{Interpolated Novel}} in {{Don Quixote}}},
  author = {Miletich, Marko},
  year = {2015},
  volume = {8},
  number = {2},
  pages = {29},
  abstract = {The impertinent curiosity of a foolish husband reveals a very fixed set of mores that trigger the transformation of a woman who evolves from being a submissive housewife into a courageous lover. Three characters in the interpolated novel El curioso impertinente part of Cervantes{\quotedblbase} monumental work, Don Quixote (1605), display their feminine and masculine selves through a series of mishaps that will eventually lead to their doom. This article concentrates on three of the best-known literary translations of the interpolated novel El curioso impertinente into English. A careful analysis of key passages in translation will illuminate how individual translational choices have transformed the Spanish tale into English. In addition, it will shed light on the translator{\quotedblbase}s particular understanding regarding the representations of femininity and masculinity reflected in this melancholic Cervantine tale.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\ADBFGIVM\Gendered Curiosity Three Translations of an Interpolated Novel in Don Quixote (Miletich, 2015).pdf}
}

@article{MonsieurTouteMonde,
  title = {Monsieur {{Toute}} Le Monde, {{Maud Machin-Chouette}}, {{Denis Trucmuche}} et Les Autres},
  pages = {17},
  file = {C:\Users\spide\Zotero\storage\QJQGQLLW\Monsieur Toute le monde, Maud Machin-Chouette, Denis Trucmuche et les autres.pdf}
}

@misc{montaniSpaCyIndustrialstrengthNatural2024,
  title = {{{spaCy}}: {{Industrial-strength Natural Language Processing}} in {{Python}}},
  shorttitle = {Explosion/{{spaCy}}},
  author = {Montani, Ines and Honnibal, Matthew and Boyd, Adriane and Landeghem, Sofie Van and Peters, Henning},
  year = {2024},
  month = oct,
  doi = {10.5281/ZENODO.1212303},
  urldate = {2024-12-17},
  abstract = {✨ New features and improvements Update \_\_all\_\_ fields (\#13063). 🔴 Bug fixes \#13035: Remove Pathy requirement. \#13053: Restore spacy.cli.project API. \#13057: Support Any comparisons for Token and Span. 📖 Documentation and examples Many updates for spacy-llm including Azure OpenAI, PaLM, and Mistral support. Various documentation corrections. 👥 Contributors @adrianeboyd, @honnibal, @ines, @rmitsch, @svlandeg},
  copyright = {Creative Commons Attribution 4.0 International},
  howpublished = {Zenodo}
}

@misc{moraffahAdversarialTextPurification2024,
  title = {Adversarial {{Text Purification}}: {{A Large Language Model Approach}} for {{Defense}}},
  shorttitle = {Adversarial {{Text Purification}}},
  author = {Moraffah, Raha and Khandelwal, Shubh and Bhattacharjee, Amrita and Liu, Huan},
  year = {2024},
  month = feb,
  number = {arXiv:2402.06655},
  eprint = {2402.06655},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-12},
  abstract = {Adversarial purification is a defense mechanism for safeguarding classifiers against adversarial attacks without knowing the type of attacks or training of the classifier. These techniques characterize and eliminate adversarial perturbations from the attacked inputs, aiming to restore purified samples that retain similarity to the initially attacked ones and are correctly classified by the classifier. Due to the inherent challenges associated with characterizing noise perturbations for discrete inputs, adversarial text purification has been relatively unexplored. In this paper, we investigate the effectiveness of adversarial purification methods in defending text classifiers. We propose a novel adversarial text purification that harnesses the generative capabilities of Large Language Models (LLMs) to purify adversarial text without the need to explicitly characterize the discrete noise perturbations. We utilize prompt engineering to exploit LLMs for recovering the purified samples for given adversarial examples such that they are semantically similar and correctly classified. Our proposed method demonstrates remarkable performance over various classifiers, improving their accuracy under the attack by over 65\% on average.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C:\Users\spide\Zotero\storage\ZJXR9TP2\Moraffah et al. - 2024 - Adversarial Text Purification A Large Language Mo.pdf}
}

@incollection{moreauLaccordProximiteDans2019,
  title = {L'accord de Proximit{\'e} Dans l'{\'e}criture Inclusive. {{Peut-on}} Utiliser n'importe Quel Argument ?},
  booktitle = {Les Discours de R{\'e}f{\'e}rence Sur La Langue Fran{\c c}aise},
  author = {Moreau, Marie-Louise},
  year = {2019},
  edition = {Anne Dister, Sophie Piron},
  pages = {351--378},
  publisher = {Presses de l'Universit{\'e} Saint-Louis},
  urldate = {2023-01-19},
  abstract = {Les discours de r{\'e}f{\'e}rence sur la langue fran{\c c}aise sont des prises de position {\'e}manant des instances officielles, des sp{\'e}cialistes (linguistes et grammairiens) ou de personnalit{\'e}s publiques reconnues comme tels, mais aussi des m{\'e}dias, qui jouent un r{\^o}le non n{\'e}gligeable dans la diffusion des normes. Ces discours, m{\^e}me s'ils peuvent {\^e}tre remis en question, constituent n{\'e}anmoins pour les francophones des rep{\`e}res souvent symboliques et des avis difficilement contournables. Anne Dister et Sophie Piron collaborent depuis de nombreuses ann{\'e}es. Elles ont organis{\'e} deux colloques, {\`a} Montr{\'e}al et {\`a} Bruxelles, consacr{\'e}s aux discours de r{\'e}f{\'e}rence sur la langue fran{\c c}aise. La pr{\'e}sente publication regroupe des articles qui sont, pour certains, issus de communications pr{\'e}sent{\'e}es lors de ces colloques, pour d'autres, originaux.},
  isbn = {978-2-8028-0245-7}
}

@incollection{motschenbacherStructuralLinguisticGender2008,
  title = {Structural {{Linguistic Gender Categories}} and {{Discursive Materialization}}: {{A Deconstructionist Analysis}}},
  booktitle = {Gender in {{Language}}: {{Classic Questions}}, {{New Contexts}}},
  author = {Motschenbacher, Heiko},
  year = {2008},
  abstract = {This article argues for a stronger re-integration of structural linguistic analysis into contemporary language and gender research by demonstrating its usefulness for poststructuralist discussions of gender. Aiming at a deconstruction of the binarisms ``female'' vs. ``male'' and ``grammatical gender language'' vs. ``natural gender language,'' it deals with lexical, social, grammatical, and referential gender as the major mechanisms of linguistic gender construction in personal nouns. These mechanisms are scrutinized in order to expose inconsistencies between and within these categories that strongly contest the stability of the above-mentioned binarisms. This is, to a large extent, done by means of structural analyses of data from Basque (for lexical gender), German (for social gender), and Croatian (for grammatical gender). The gender indexicality of the four categories is finally explained in terms of discursive materialization.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\WGYCU6ZQ\Motschenbacher - STRUCTURAL LINGUISTIC GENDER CATEGORIES AND DISCURSIVE MATERIALIZATION A DECONSTRUCTIONIST ANALYSIS.pdf}
}

@article{murphyAmDeadPoe,
  title = {I Am {{Dead}}: {{Poe}} and {{French Theory}}},
  author = {Murphy, Kieran M},
  pages = {14},
  langid = {english},
  keywords = {poe},
  file = {C:\Users\spide\Zotero\storage\LRIES837\I am Dead Poe and French Theory (Murphy, ).pdf}
}

@article{namerDemonette2DerivationalDatabase2023,
  title = {{D{\'e}monette-2, a derivational database for French with broad lexical coverage and fine-grained morphological descriptions}},
  author = {Namer, Fiammetta and Hathout, Nabil and Amiot, Dany and Barque, Lucie and Bonami, Olivier and Boy{\'e}, Gilles and Calderone, Basilio and Cattini, Julie and Dal, Georgette and Delaporte, Alexander and Duboisdindien, Guillaume and Falaise, Achille and Grabar, Natalia and Haas, Pauline and Henry, Fr{\'e}d{\'e}rique and Huguin, Mathilde and Juniarta, Nyoman and Li{\'e}geois, Lo{\"i}c and Lignon, St{\'e}phanie and Macchi, Lucie and Manucharian, Grigoriy and Masson, Caroline and Montermini, Fabio and Okinina, Nadejda and Sajous, Franck and Sanacore, Daniele and Tran, Mai Thi and Thuilier, Juliette and Toussaint, Yannick and Tribout, Delphine},
  year = {2023},
  month = dec,
  journal = {Lexique},
  number = {33},
  issn = {2804-7397},
  doi = {10.54563/lexique.1242},
  urldate = {2025-01-09},
  abstract = {Morphological databases play an important role in linguistic research today. While several exist for the study of inflectional morphology in French, there is still a lack of resources for derivational morphology. This article presents D{\'e}monette-2, a new release of the French derivational morphology database D{\'e}monette. Developed in the context of the D{\'e}monext project, this database provides a framework in which it is possible to integrate various existing derivational resources. D{\'e}monette is characterized by its relational nature, which makes it possible to describe an extensive set of word formation patterns, including suffixation, prefixation, and a variety of non-canonical derivational processes such as conversion and parasynthetic formations. It reconciles broad coverage and fine-grained descriptions and is suitable for different audiences: morphologists working in different theoretical frameworks, teachers, speech therapists, NLP specialists, etc. The article presents in detail the structure and content of D{\'e}monette, its evolution since the first version, and its query interface.           ,              Les bases de donn{\'e}es morphologiques jouent aujourd'hui un r{\^o}le important dans les recherches en linguistique. S'il en existe plusieurs pour les {\'e}tudes de morphologie flexionnelle pour le fran{\c c}ais, l'offre reste insuffisante pour la morphologie d{\'e}rivationnelle. Cet article pr{\'e}sente D{\'e}monette-2, une nouvelle version de la base de donn{\'e}es morphologique d{\'e}rivationnelle du fran{\c c}ais D{\'e}monette. D{\'e}velopp{\'e}e dans le cadre du projet D{\'e}monext, cette base offre un cadre dans lequel il est possible d'int{\'e}grer diverses ressources d{\'e}rivationnelles existantes. D{\'e}monette se caract{\'e}rise par sa nature relationnelle qui permet d'y d{\'e}crire un ensemble {\'e}tendu de constructions morphologiques qui inclut les proc{\'e}d{\'e}s de suffixation, de pr{\'e}fixation, et une grande vari{\'e}t{\'e} de d{\'e}rivations non canoniques comme la conversion et les constructions parasynth{\'e}tiques. Elle concilie couverture {\'e}tendue et finesse des descriptions et est adapt{\'e}e {\`a} diff{\'e}rents publics cibles~: morphologues travaillant dans diff{\'e}rents cadres th{\'e}oriques, enseignants, orthophonistes, sp{\'e}cialistes de TAL, etc. L'article pr{\'e}sente en d{\'e}tail la structure et le contenu de D{\'e}monette, son {\'e}volution depuis la premi{\`e}re version et son interface d'interrogation.},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\VV5GLPJI\Namer et al. - 2023 - Démonette-2, a derivational database for French with broad lexical coverage and fine-grained morphol.pdf}
}

@book{nevalainenGenderGrammarCognition2011,
  title = {Gender in {{Grammar}} and {{Cognition}}},
  editor = {Nevalainen, Terttu and Rissanen, Matti and Saari, Mirja and Unterbeck, Barbara and Saari, Mirja and Unterbeck, Barbara and Nevalainen, Terttu and Rissanen, Matti},
  year = {2011},
  series = {Trends in {{Linguistics}}. {{Studies}} and {{Monographs}} [{{TiLSM}}]},
  number = {124},
  publisher = {De Gruyter},
  address = {Berlin},
  doi = {10.1515/9783110802603},
  isbn = {978-3-11-016241-7 978-3-11-080260-3},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\ZRIUL9TF\Nevalainen et al. - 2011 - Gender in Grammar and Cognition.pdf}
}

@article{NixingerismNATODetente,
  title = {``{{Nixingerism}},'' {{NATO}}, and {{Detente}}},
  journal = {DIPLOMATIC HISTORY},
  pages = {25},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\83KQWJ72\“Nixingerism,” NATO, and Detente (, ).pdf}
}

@misc{nllbteamNoLanguageLeft2022,
  title = {No {{Language Left Behind}}: {{Scaling Human-Centered Machine Translation}}},
  shorttitle = {No {{Language Left Behind}}},
  author = {{NLLB Team} and {Costa-juss{\`a}}, Marta R. and Cross, James and {\c C}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and Sun, Anna and Wang, Skyler and Wenzek, Guillaume and Youngblood, Al and Akula, Bapi and Barrault, Loic and Gonzalez, Gabriel Mejia and Hansanti, Prangthip and Hoffman, John and Jarrett, Semarley and Sadagopan, Kaushik Ram and Rowe, Dirk and Spruit, Shannon and Tran, Chau and Andrews, Pierre and Ayan, Necip Fazil and Bhosale, Shruti and Edunov, Sergey and Fan, Angela and Gao, Cynthia and Goswami, Vedanuj and Guzm{\'a}n, Francisco and Koehn, Philipp and Mourachko, Alexandre and Ropers, Christophe and Saleem, Safiyyah and Schwenk, Holger and Wang, Jeff},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2207.04672},
  urldate = {2024-12-17},
  abstract = {Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44\% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  keywords = {68T50,Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,I.2.7}
}

@misc{Nuessel1987REVIEW,
  title = {Nuessel1987 - {{REVIEW}}},
  file = {C:\Users\spide\Zotero\storage\92HZEUU8\nuessel1987.pdf}
}

@article{offnerAnotherSuchVictory1999,
  title = {"{{Another Such Victory}}": {{President Truman}}, {{American Foreign Policy}}, and the {{Cold War}}},
  shorttitle = {"{{Another Such Victory}}"},
  author = {Offner, Arnold A.},
  year = {1999},
  month = apr,
  journal = {Diplomatic History},
  volume = {23},
  number = {2},
  pages = {30},
  issn = {0145-2096, 1467-7709},
  doi = {10.1111/1467-7709.00159},
  urldate = {2021-12-17},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\SQ73QCIE\Another Such Victory President Truman, American Foreign Policy, and the Cold War (Offner, 1999).pdf}
}

@article{olahZoomIntroductionCircuits2020,
  title = {Zoom {{In}}: {{An Introduction}} to {{Circuits}}},
  shorttitle = {Zoom {{In}}},
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  year = {2020},
  month = mar,
  journal = {Distill},
  volume = {5},
  number = {3},
  pages = {10.23915/distill.00024.001},
  issn = {2476-0757},
  doi = {10.23915/distill.00024.001},
  urldate = {2024-11-11}
}

@article{omoregbeTextMessagingBasedMedical2020,
  title = {Text {{Messaging-Based Medical Diagnosis Using Natural Language Processing}} and {{Fuzzy Logic}}},
  author = {Omoregbe, Nicholas A. I. and Ndaman, Israel O. and Misra, Sanjay and {Abayomi-Alli}, Olusola O. and Dama{\v s}evi{\v c}ius, Robertas},
  editor = {Dogra, Ayush},
  year = {2020},
  month = sep,
  journal = {Journal of Healthcare Engineering},
  volume = {2020},
  pages = {1--14},
  issn = {2040-2309, 2040-2295},
  doi = {10.1155/2020/8839524},
  urldate = {2024-12-17},
  abstract = {The use of natural language processing (NLP) methods and their application to developing conversational systems for health diagnosis increases patients' access to medical knowledge. In this study, a chatbot service was developed for the Covenant University Doctor (CUDoctor) telehealth system based on fuzzy logic rules and fuzzy inference. The service focuses on assessing the symptoms of tropical diseases in Nigeria. Telegram Bot Application Programming Interface (API) was used to create the interconnection between the chatbot and the system, while Twilio API was used for interconnectivity between the system and a short messaging service (SMS) subscriber. The service uses the knowledge base consisting of known facts on diseases and symptoms acquired from medical ontologies. A fuzzy support vector machine (SVM) is used to effectively predict the disease based on the symptoms inputted. The inputs of the users are recognized by NLP and are forwarded to the CUDoctor for decision support. Finally, a notification message displaying the end of the diagnosis process is sent to the user. The result is a medical diagnosis system which provides a personalized diagnosis utilizing self-input from users to effectively diagnose diseases. The usability of the developed system was evaluated using the system usability scale (SUS), yielding a mean SUS score of 80.4, which indicates the overall positive evaluation.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\6BZS2ZIZ\Omoregbe et al. - 2020 - Text Messaging-Based Medical Diagnosis Using Natural Language Processing and Fuzzy Logic.pdf}
}

@misc{otherwise1892JohnFosterDulles2010,
  title = {John {{Foster Dulles}} on {{Guatemala}} (1954)},
  author = {{otherwise1892}},
  year = {2010},
  month = nov,
  urldate = {2021-12-17},
  abstract = {Outtakes of Secretary of State John Foster Dulles discussing the future of Guatemala in the wake of the overthrow of Guatemalan President Jacobo Arbenz.  Source - www.criticalpast.com Uploaded here with permission.},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.}
}

@article{ottawayGreaterMiddleEast,
  title = {The {{Greater Middle East Initiative}}: {{Off}} to a {{False Start}}},
  author = {Ottaway, Marina and Carothers, Thomas},
  pages = {8},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\38RCU6WF\The Greater Middle East Initiative Off to a False Start (Ottaway and Carothers, ).pdf}
}

@misc{ouyangTrainingLanguageModels2022,
  title = {Training {{Language Models}} to {{Follow Instructions}} with {{Human Feedback}}},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  eprint = {2203.02155},
  urldate = {2024-03-18},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2203.02155},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{papineniBLEUMethodAutomatic2002,
  title = {{{BLEU}}: {{A Method}} for {{Automatic Evaluation}} of {{Machine Translation}}},
  shorttitle = {{{BLEU}}},
  booktitle = {Proceedings of the 40th {{Annual Meeting}} on {{Association}} for {{Computational Linguistics}} - {{ACL}} '02},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  year = {2002},
  publisher = {Association for Computational Linguistics},
  address = {Philadelphia, Pennsylvania},
  doi = {10.3115/1073083.1073135},
  urldate = {2023-02-27}
}

@inbook{pawleyUsingHeShe2004,
  title = {Using {{{\emph{He}}}} and {{{\emph{She}}}} for {{Inanimate Referents}} in {{English}}: {{Questions}} of {{Grammar}} and {{World View}}},
  shorttitle = {Using {{{\emph{He}}}} and {{{\emph{She}}}} for {{Inanimate Referents}} in {{English}}},
  booktitle = {Ethnosyntax},
  author = {Pawley, Andrew},
  year = {2004},
  month = feb,
  edition = {1},
  pages = {110--137},
  publisher = {Oxford University PressOxford},
  doi = {10.1093/acprof:oso/9780199266500.003.0006},
  urldate = {2024-12-02},
  abstract = {English is often described as a language in which grammatical gender, marked only in third person singular pronouns, correlates with natural gender. Human referents take masculine or feminine pronouns matching their biological gender. With certain exceptions inanimate entities and lower animals (e.g. insects, worms, frogs, fish) take neuter pronouns. Higher animals of known sex can be treated either like humans or lower animals. However, there are varieties of English in which a wide range of inanimate referents as well as lower animals are `animated', i.e. referred to by masculine or feminine pronouns. This paper describes pronominal gender assignment in `animated style' in Australian Vernacular English (AVE). In this style just a few classes of things are consistently masculine, chiefly trees and other plants, and vehicles when regarded as a unit under the control of a driver, as well as lower animals, and higher animals of unknown sex. Most inanimate things, concrete or abstract, are feminine, e.g. roads, rivers, mountains, wind, rain, buildings, machines, situations, jobs, retirement, and almost all body-parts. This pattern is consistent with the view that masculine is the unmarked gender for living things and feminine for inanimate things. However, some things, chiefly portable objects, although most often feminine, are sometimes represented by a masculine or feminine pronoun, with the choice possibly reflecting attitudes, such as degree of emotional attachment to (feminine) or detachment from (masculine) the object. Historical data, though fragmentary, indicate that a pattern of gender assignment quite like the AVE system was part of middle class speech in 17th and 18th century England and may go back to Middle English. The same system occurs in New Zealand Vernacular English and it appears that there are close parallels in parts of North America and Britain.},
  collaborator = {Enfield, N. J.},
  isbn = {978-0-19-926650-0 978-0-19-171936-3},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\EBZK2963\Pawley - 2004 - Using He and She for Inanimate Referents in English Questions of Grammar and World Vi.pdf}
}

@article{pechManuelScolaireEcrit2017,
  title = {Un Manuel Scolaire {\'E}crit {\`a} La Sauce F{\'e}ministe},
  author = {Pech, Marie-Estelle},
  year = {2017},
  month = sep,
  journal = {Le Figaro},
  urldate = {2023-02-03},
  abstract = {Destin{\'e} aux {\'e}coliers, il promeut l'{\'e}criture << inclusive >> ou << genr{\'e}e >> qui f{\'e}minise tous les noms. On y lit que << gr{\^a}ce aux agriculteur.rice.s, aux artisan.e.s et aux commer{\c c}ant.e.s, la Gaule {\'e}tait un pays riche >>.}
}

@misc{PhaethonPersephoneRoom,
  title = {Phaethon, {{Persephone}}, and "{{A Room}} with a {{View}}" on {{JSTOR}}},
  urldate = {2022-01-11},
  howpublished = {https://www-jstor-org.lama.univ-amu.fr/stable/40246766?Search=yes\&resultItemClick=true\&searchText=a+room+with+a+view\&searchUri=\%2Faction\%2FdoBasicSearch\%3FQuery\%3Da\%2Broom\%2Bwith\%2Ba\%2Bview\%26so\%3Drel\&ab\_segments=0\%2Fbasic\_search\_gsv2\%2Fcontrol\&refreqid=fastly-default\%3Aaffe28e17245ced7ebcabaf13b19641b\&seq=1\#metadata\_info\_tab\_contents},
  file = {C:\Users\spide\Zotero\storage\94NR4RKU\40246766.html}
}

@misc{piergentiliInclusiveLanguageGenderNeutral2023,
  title = {From {{Inclusive Language}} to {{Gender-Neutral Machine Translation}}},
  author = {Piergentili, Andrea and Fucci, Dennis and Savoldi, Beatrice and Bentivogli, Luisa and Negri, Matteo},
  year = {2023},
  month = jan,
  eprint = {2301.10075},
  urldate = {2023-02-20},
  abstract = {Gender inclusivity in language has become a central topic of debate and research. Its application in the cross-lingual contexts of human and machine translation (MT), however, remains largely unexplored. Here, we discuss Gender-Neutral Translation (GNT) as a form of gender inclusivity in translation and advocate for its adoption for MT models, which have been found to perpetuate gender bias and discrimination. To this aim, we review a selection of relevant institutional guidelines for Gender-Inclusive Language (GIL) to collect and systematize useful strategies of gender neutralization. Then, we discuss GNT and its scenarios of use, devising a list of desiderata. Finally, we identify the main technical challenges to the implementation of GNT in MT. Throughout these contributions we focus on translation from English into Italian, as representative of salient linguistic transfer problems, due to the different rules for gender marking in their grammar.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2301.10075},
  keywords = {Computer Science - Computation and Language}
}

@article{pironApresQueSuivi2021,
  title = {{Apr{\`e}s que suivi de l'indicatif ou du subjonctif : quelles voies de changement dans les ouvrages de r{\'e}f{\'e}rence ?}},
  shorttitle = {{Apr{\`e}s que suivi de l'indicatif ou du subjonctif}},
  author = {Piron, Sophie and Vincent, Nadine},
  year = {2021},
  month = jul,
  journal = {Linx},
  number = {82},
  issn = {0246-8743, 2118-9692},
  doi = {10.4000/linx.8088},
  urldate = {2022-12-24},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\NLDHYAHR\Après que suivi de l’indicatif ou du subjonctif  quelles voies de changement dans les ouvrages de référence  (Piron and Vincent, 2021).pdf}
}

@misc{pomerenkeINCLUSIFYBenchmarkModel2022,
  title = {{{INCLUSIFY}}: {{A Benchmark}} and a {{Model}} for {{Gender-Inclusive German}}},
  shorttitle = {{{INCLUSIFY}}},
  author = {Pomerenke, David},
  year = {2022},
  month = dec,
  eprint = {2212.02564},
  urldate = {2023-01-02},
  abstract = {Gender-inclusive language is important for achieving gender equality in languages with gender inflections, such as German. While stirring some controversy, it is increasingly adopted by companies and political institutions. A handful of tools have been developed to help people use gender-inclusive language by identifying instances of the generic masculine and providing suggestions for more inclusive reformulations. In this report, we define the underlying tasks in terms of natural language processing, and present a dataset and measures for benchmarking them. We also present a model that implements these tasks, by combining an inclusive language database with an elaborate sequence of processing steps via standard pre-trained models. Our model achieves a recall of 0.89 and a precision of 0.82 in our benchmark for identifying exclusive language; and one of its top five suggestions is chosen in real-world texts in 44\% of cases. We sketch how the area could be further advanced by training end-to-end models and using large language models; and we urge the community to include more gender-inclusive texts in their training data in order to not present an obstacle to the adoption of gender-inclusive language. Through these efforts, we hope to contribute to restoring justice in language and, to a small extent, in reality.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2212.02564},
  keywords = {a lire,inclusive rewriting}
}

@article{prestonImplyingAuthorsGreat1997,
  title = {Implying {{Authors}} in "{{The Great Gatsby}}"},
  author = {Preston, Elizabeth},
  year = {1997},
  journal = {Narrative},
  volume = {5},
  number = {2},
  eprint = {20107113},
  eprinttype = {jstor},
  pages = {21},
  issn = {1063-3685},
  urldate = {2021-08-22},
  keywords = {fitzgerald},
  annotation = {GATSBY},
  file = {C:\Users\spide\Zotero\storage\ZMXINIUI\Implying Authors in The Great Gatsby (Preston, 1997).pdf}
}

@incollection{prierCommandingTrend2020,
  title = {Commanding the Trend},
  booktitle = {Information Warfare in the Age of Cyber Conflict},
  author = {Prier, Jarred},
  editor = {Whyte, Christopher and Thrall, A. Trevor and Mazanec, Brian M.},
  year = {2020},
  month = jul,
  edition = {1},
  pages = {37},
  publisher = {Routledge},
  doi = {10.4324/9780429470509-7},
  urldate = {2021-04-10},
  abstract = {This article demonstrates how social media is a tool for modern information-age warfare. It builds on analysis of three distinct topics: social networking, propaganda, and news and information sharing. Two case studies are used to show how state and nonstate actors use social media to employ time-tested propaganda techniques to yield far-reaching results. The spread of the propaganda message is accomplished by tapping into an existing narrative, then amplifying that message with a network of automatic ``bot'' accounts to force the social media platform algorithm to recognize that message as a trending topic. The first case study analyzes Islamic State (IS) as a nonstate actor, while the second case observes Russia as a state actor, with each providing evidence of successful influence operations using social media. Coercion and persuasion will continue to be decisive factors in information warfare as more countries attempt to build influence operations on social media.},
  isbn = {978-0-429-47050-9},
  langid = {english},
  keywords = {history},
  file = {C:\Users\spide\Zotero\storage\GBPEHY7W\Commanding the trend (Prier, 2020).pdf}
}

@misc{princetonuniversityWordNet2010,
  title = {About {{WordNet}}},
  author = {{Princeton University}},
  year = {2010},
  publisher = {Princeton University},
  langid = {english}
}

@article{proninHowWeSee2008,
  title = {How {{We See Ourselves}} and {{How We See Others}}},
  author = {Pronin, Emily},
  year = {2008},
  month = may,
  journal = {Science},
  volume = {320},
  number = {5880},
  pages = {4},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1154199},
  urldate = {2023-08-12},
  abstract = {People see themselves differently from how they see others. They are immersed in their own sensations, emotions, and cognitions at the same time that their experience of others is dominated by what can be observed externally. This basic asymmetry has broad consequences. It leads people to judge themselves and their own behavior differently from how they judge others and those others' behavior. Often, those differences produce disagreement and conflict. Understanding the psychological basis of those differences may help mitigate some of their negative effects.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\2BGJQ3X3\How We See Ourselves and How We See Others (Pronin, 2008).pdf}
}

@article{pullumEnglishNominalGerund,
  title = {English Nominal Gerund Phrases as Noun Phrases with Verb-Phrase Heads},
  author = {Pullum, Geoffrey K},
  pages = {38},
  abstract = {This paper focuses on the theoretical notion 'head' in syntax. The English nominal-gerund-phrase construction exemplified by (your) having broken the record is examined as a case of a construction type in which there is a problem about identifying the head. The properties of the construction are sytematically enumerated; previous generative analyses of the construction are briefly examined and criticized; and a generalized phrase-structure grammar (GPSG) analysis is provided. Under the proposed analysis, nominal gerund phrases are noun phrases with verb-phrase heads. This is a possible situation in GPSG terms and by no means deprives the notion 'head' of content; several features on the head are assigned values via the head-feature convention despite the disagreement between head and mother in the major category features N and V. The proposed analysis is shown to entail or be compatible with all the properties of the construction. It is comparable m coverage {$Io$} {$\ddot{\iota}$}'{$\eta\alpha\ddot{\iota}$} proriaea by Abnty aria yaperiw to oiker published analyses. Potential future lines of research are suggested, linking nominal gerunds to attributive modifiers and partitives.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\D92378SC\English nominal gerund phrases as noun phrases with verb-phrase heads (Pullum, ).pdf}
}

@article{puschJeDiraisQue2020,
  title = {Je Dirais Que La Grosse Majorit{\'e} Sont L{\`a} : Questions d'accord(s) Avec Les Noms Collectifs Dans Les Donn{\'e}es {{OFROM}}},
  author = {Pusch, Claus D.},
  year = {2020},
  publisher = {Universit{\"a}t Graz},
  doi = {10.25364/19.2020.4.3},
  urldate = {2023-02-14},
  abstract = {In French, collective nouns (Ncoll), which refer to a plurality of objects or individuals through a singular form, are known to be subject to variable verbal (but also pronominal) agreement, particularly in spoken language. The study presented here analyzes the OFROM corpus to provide a distributional description of an array of Ncoll, selected on the basis of the samples used by Tristram (2014) and Mougeon \& Mougeon (2017a), in the variety of French spoken in Switzerland. Specifically, it focuses on the agreement patterns associated with these nouns. The results indicate that Swiss speakers of French tend to closely follow the norm and mostly apply grammati cal agreement. A part of the diverging tokens with semantic agreement attested in the data can be explained as agreement attraction by a pluralized nominal complement within the NP, while the results concerning the effect of syntagmatic distance and intervening syntactic boundaries between the Ncoll and the target(s) for agreement turn out to be inconclusive.}
}

@misc{puSummarizationAlmostDead2023,
  title = {Summarization Is ({{Almost}}) {{Dead}}},
  author = {Pu, Xiao and Gao, Mingqi and Wan, Xiaojun},
  year = {2023},
  month = sep,
  number = {arXiv:2309.09558},
  eprint = {2309.09558},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.09558},
  urldate = {2025-01-09},
  abstract = {How well can large language models (LLMs) generate summaries? We develop new datasets and conduct human evaluation experiments to evaluate the zero-shot generation capability of LLMs across five distinct summarization tasks. Our findings indicate a clear preference among human evaluators for LLM-generated summaries over human-written summaries and summaries generated by fine-tuned models. Specifically, LLM-generated summaries exhibit better factual consistency and fewer instances of extrinsic hallucinations. Due to the satisfactory performance of LLMs in summarization tasks (even surpassing the benchmark of reference summaries), we believe that most conventional works in the field of text summarization are no longer necessary in the era of LLMs. However, we recognize that there are still some directions worth exploring, such as the creation of novel datasets with higher quality and more reliable evaluation methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\MAE3MRIX\\Pu et al. - 2023 - Summarization is (Almost) Dead.pdf;C\:\\Users\\spide\\Zotero\\storage\\44WMX3SB\\2309.html}
}

@article{qianliuStylisticAnalysisEdgar2017,
  title = {A {{Stylistic Analysis}} of {{Edgar Allan Poe}}{\textbackslash}'s the {{Black Cat}}},
  author = {Qian Liu, Qian Liu and {TJPRC}},
  year = {2017},
  journal = {International Journal of English and Literature},
  volume = {7},
  number = {6},
  pages = {6},
  issn = {2249-6912},
  doi = {10.24247/ijeldec20178},
  urldate = {2021-01-19},
  langid = {english},
  keywords = {poe},
  file = {C:\Users\spide\Documents\COURS S4\- APPROFONDISSEMENT\Littérature\PDF Recherches\A Stylistic Analysis of Edgar Allan Poe's the Black Cat (Qian Liu and TJPRC, 2017).pdf}
}

@inproceedings{qinMachineTranslationTechnology2022,
  title = {Machine {{Translation Technology Based}} on {{Natural Language Processing}}},
  booktitle = {2022 {{European Conference}} on {{Natural Language Processing}} and {{Information Retrieval}} ({{ECNLPIR}})},
  author = {Qin, Mo},
  year = {2022},
  month = jul,
  pages = {10--13},
  publisher = {IEEE},
  address = {Hangzhou, China},
  doi = {10.1109/ECNLPIR57021.2022.00014},
  urldate = {2024-12-17},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-7382-8}
}

@article{radfordLanguageModelsAre2019,
  title = {Language {{Models Are Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.}
}

@misc{raffelExploringLimitsTransfer2020,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  month = jul,
  eprint = {1910.10683},
  urldate = {2023-02-27},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/1910.10683},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{raffelExploringLimitsTransfer2023,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2023},
  month = sep,
  eprint = {1910.10683},
  urldate = {2024-03-18},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/1910.10683},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{rasanenSexualLonelinessNeglected2023,
  title = {Sexual Loneliness: {{A}} Neglected Public Health Problem?},
  shorttitle = {Sexual Loneliness},
  author = {R{\"a}s{\"a}nen, Joona},
  year = {2023},
  month = feb,
  journal = {Bioethics},
  volume = {37},
  number = {2},
  pages = {101--102},
  issn = {0269-9702, 1467-8519},
  doi = {10.1111/bioe.13134},
  urldate = {2023-05-30},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\MGA3T8F2\Sexual loneliness A neglected public health problem (Räsänen, 2023).pdf}
}

@article{ReadingEmmaLesson,
  title = {Reading {{Emma}} as a {{Lesson}} on {{Ladyhood}}},
  keywords = {jane austen},
  file = {C:\Users\spide\Documents\PDF recherches\Reading Emma as a Lesson on Ladyhood.pdf}
}

@article{reynoldsScarletLetterRevolutions1985,
  title = {The {{Scarlet Letter}} and {{Revolutions Abroad}}},
  author = {Reynolds, Larry J.},
  year = {1985},
  journal = {American Literature},
  volume = {57},
  number = {1},
  eprint = {2926312},
  eprinttype = {jstor},
  pages = {21},
  issn = {0002-9831},
  doi = {10.2307/2926312},
  urldate = {2021-11-09},
  keywords = {hawthorne},
  annotation = {SCARLET LETTER},
  file = {C:\Users\spide\Zotero\storage\EZYQ4H8S\The Scarlet Letter and Revolutions Abroad (Reynolds, 1985).pdf}
}

@article{richyDemelerEffetsStereotypes2021,
  title = {D{\'e}m{\^e}ler Les Effets Des St{\'e}r{\'e}otypes et Le Genre Grammatical Dans Le Biais Masculin : Une Approche Exp{\'e}rimentale},
  shorttitle = {D{\'e}m{\^e}ler Les Effets Des St{\'e}r{\'e}otypes et Le Genre Grammatical Dans Le Biais Masculin},
  author = {Richy, C{\'e}lia and Burnett, Heather},
  year = {2021},
  month = jul,
  journal = {GLAD!},
  number = {10},
  issn = {2551-0819},
  doi = {10.4000/glad.2839},
  urldate = {2025-01-07},
  file = {C:\Users\spide\Zotero\storage\W84JVFDJ\Richy and Burnett - 2021 - Démêler les effets des stéréotypes et le genre grammatical dans le biais masculin  une approche exp.pdf}
}

@article{riouxSexGenderTerminology2022,
  title = {Sex and {{Gender Terminology}}: {{A Glossary}} for {{Gender-Inclusive Epidemiology}}},
  shorttitle = {Sex and {{Gender Terminology}}},
  author = {Rioux, Charlie and Par{\'e}, Ash and {London-Nadeau}, Kira and Juster, Robert-Paul and Weedon, Scott and {Levasseur-Puhach}, Sydney and Freeman, Makayla and Roos, Leslie E and {Tomfohr-Madsen}, Lianne M},
  year = {2022},
  month = aug,
  journal = {Journal of Epidemiology and Community Health},
  volume = {76},
  number = {8},
  pages = {764--768},
  issn = {0143-005X, 1470-2738},
  doi = {10.1136/jech-2022-219171},
  urldate = {2023-01-19},
  abstract = {There is increased interest in inclusion, diversity, and representativeness in epidemiological and community health research. Despite this progress, misunderstanding and conflation of sex and gender have precluded both the accurate description of sex and gender as sample demographics and their inclusion in scientific enquiry aiming to distinguish health disparities due to biological systems, gendered experiences, or their social and environmental interactions. The present glossary aims to define and improve understanding of current sex- and gender-related terminology as an important step to gender-inclusive epidemiological research. Effectively, a proper understanding of sex, gender, and their subtleties as well as acknowledgement and inclusion of diverse gender identities and modalities can make epidemiology not only more equitable, but also more scientifically accurate and representative. In turn, this can improve public health efforts aimed at promoting the well-being of all communities and reducing health inequities.}
}

@article{RiseNewWoman,
  title = {Rise of the {{New Woman}}},
  pages = {19},
  keywords = {civi. women},
  annotation = {WOMEN},
  file = {C:\Users\spide\Zotero\storage\SJNSPJR7\N WOLOCH ch 12.pdf}
}

@misc{RiseNewWomana,
  title = {Rise of the {{New Woman}}}
}

@article{robinsonMeasuringAttitudesMental2019,
  title = {Measuring Attitudes towards Mental Health Using Social Media: Investigating Stigma and Trivialisation},
  shorttitle = {Measuring Attitudes towards Mental Health Using Social Media},
  author = {Robinson, Patrick and Turk, Daniel and Jilka, Sagar and Cella, Matteo},
  year = {2019},
  month = jan,
  journal = {Social Psychiatry and Psychiatric Epidemiology},
  volume = {54},
  number = {1},
  pages = {51--58},
  issn = {0933-7954, 1433-9285},
  doi = {10.1007/s00127-018-1571-5},
  urldate = {2023-04-07},
  abstract = {Background{\enspace} There are numerous campaigns targeting mental health stigma. However, evaluating how effective these are in changing perceptions is complex. Social media may be used to assess stigma levels and highlight new trends. This study uses a social media platform, Twitter, to investigate stigmatising and trivialising attitudes across a range of mental and physical health conditions. Methods{\enspace} Tweets (i.e. messages) associated with five mental and five physical health conditions were collected in ten 72-h windows over a 50-day period using automated software. A random selection of tweets per condition was considered for the analyses. Tweets were categorised according to their topic and presence of stigmatising and trivialising attitudes. Qualitative thematic analysis was performed on all stigmatising and trivialising tweets. Results{\enspace} A total of 1,059,258 tweets were collected, and from this sample 1300 tweets per condition were randomly selected for analysis. Overall, mental health conditions were found to be more stigmatised (12.9\%) and trivialised (14.3\%) compared to physical conditions (8.1 and 6.8\%, respectively). Amongst mental health conditions the most stigmatised condition was schizophrenia (41\%) while the most trivialised was obsessive compulsive disorder (33\%). Conclusions{\enspace} Our findings show that mental health stigma is common on social media. Trivialisation is also common, suggesting that while society may be more open to discussing mental health problems, care should be taken to ensure this is done appropriately. This study further demonstrates the potential for social media to be used to measure the general public's attitudes towards mental health conditions.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\2TYJ6ASE\Measuring attitudes towards mental health using social media investigating stigma and trivialisation (Robinson et al., 2019).pdf}
}

@incollection{romaineGenderGrammarSpace1997,
  title = {Gender, Grammar, and the Space in Between},
  booktitle = {Pragmatics \& {{Beyond New Series}}},
  author = {Romaine, Suzanne},
  editor = {Kotthoff, Helga and Wodak, Ruth},
  year = {1997},
  volume = {42},
  pages = {51},
  publisher = {John Benjamins Publishing Company},
  address = {Amsterdam},
  doi = {10.1075/pbns.42.05rom},
  urldate = {2024-11-21},
  isbn = {978-90-272-5055-1 978-1-55619-804-5 978-90-272-8974-2},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\2QJDYTAT\Romaine - 1997 - Gender, grammar, and the space in between.pdf}
}

@misc{RoomViewEthical,
  title = {Room with a {{View}}: {{Ethical Encounters}} in {{Room}} 13 on {{JSTOR}}},
  urldate = {2022-01-11},
  howpublished = {https://www-jstor-org.lama.univ-amu.fr/stable/23392061?Search=yes\&resultItemClick=true\&searchText=a+room+with+a+view\&searchUri=\%2Faction\%2FdoBasicSearch\%3FQuery\%3Da\%2Broom\%2Bwith\%2Ba\%2Bview\%26so\%3Drel\&ab\_segments=0\%2Fbasic\_search\_gsv2\%2Fcontrol\&refreqid=fastly-default\%3Aaffe28e17245ced7ebcabaf13b19641b\&seq=1\#metadata\_info\_tab\_contents},
  file = {C:\Users\spide\Zotero\storage\9L7YJF7T\23392061.html}
}

@article{rosengrantNabokovOneginTheory1994,
  title = {Nabokov, {{Onegin}}, and the {{Theory}} of {{Translation}}},
  author = {Rosengrant, Judson},
  year = 1994,
  journal = {The Slavic and East European Journal},
  volume = {38},
  number = {1},
  eprint = {308543},
  eprinttype = {jstor},
  pages = {16},
  issn = {00376752},
  doi = {10.2307/308543},
  urldate = {2022-07-24},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\4THPNUTG\Nabokov, Onegin, and the Theory of Translation (Rosengrant, 1994).pdf}
}

@article{rothermundRemindingMayNot2024,
  title = {Reminding {{May Not Be Enough}}: {{Overcoming}} the {{Male Dominance}} of the {{Generic Masculine}}},
  shorttitle = {Reminding {{May Not Be Enough}}},
  author = {Rothermund, Patrick and Strack, Fritz},
  year = {2024},
  month = sep,
  journal = {Journal of Language and Social Psychology},
  volume = {43},
  number = {4},
  pages = {468--485},
  issn = {0261-927X, 1552-6526},
  doi = {10.1177/0261927X241237739},
  urldate = {2024-11-19},
  abstract = {In gender-marked languages, masculine and feminine grammatical forms are distinct, with the masculine form also used for gender-mixed groups (generic masculine). Previous research indicates that the generic masculine elicits male-biased representations. Psychologically, this may be due to a misunderstanding of the communicative intention, an automatic activation of male associations, or both. In two preregistered experiments, we tested whether the male bias is affected by emphasizing the generic intention. Adding contextual information that conveyed a group's gender-mixed composition eliminated the male bias (Study 1). However, the male bias remained robust when continuously reminding participants of the generic intention via a novel grammatical marker (Study 2). These results suggest that the male bias is partly driven by associative processes that are immune against a purely explicit disambiguation of the generic intention.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\IGBSJJEF\Rothermund and Strack - 2024 - Reminding May Not Be Enough Overcoming the Male D.pdf}
}

@article{safinaEffectsGrammaticalGender2024,
  title = {Effects of {{Grammatical Gender}} on {{Gender Inferences}}: {{Experimental Evidence From Italian Common Gender Nouns}}},
  shorttitle = {Effects of {{Grammatical Gender}} on {{Gender Inferences}}},
  author = {Safina, Elena Sofia},
  year = {2024},
  month = jun,
  journal = {International Journal of Linguistics},
  volume = {16},
  number = {3},
  pages = {60},
  issn = {1948-5425},
  doi = {10.5296/ijl.v16i3.21824},
  urldate = {2025-01-09},
  abstract = {Recent psycholinguistic research has focused on how different grammatical gender marking strategies affect people's mental representation of referents' gender. Such works particularly explored how explicitly encoded linguistic elements, such as grammatical gender markers, may drive the inferential process as attentional clues. Results of reading comprehension tasks in French and German have shown that the explicit encoding of masculine gender in plural forms of role nouns often leads to a male bias, a specific masculine inference corresponding to the grammatical gender clue, even when the masculine form was intended as generic, thus including women and men. Moreover, comparing generic masculine forms with gender-fair alternatives revealed that the latter significantly reduce this male bias. The present study examines the impact of three gender marking strategies on the construction of generic mental representations. Indeed, the experiment tested generic masculine against two gender-fair forms (split masculine/feminine forms and ambiguous syntactic reformulations) among 38 Italian speakers. No significant effect was found in generating a generic mental representation through form manipulation. However, ambiguous syntactic reformulation realised by presenting target nouns from the Italian common gender noun class and through the neutralisation of determiners' gender, increased the probability of a male-specific inference. Additionally, a keen interest in gender-fair language topics was linked to longer reaction times, indicating a higher cognitive effort during the inference process.},
  copyright = {http://creativecommons.org/licenses/by/4.0},
  file = {C:\Users\spide\Zotero\storage\LREPALVQ\Safina - 2024 - Effects of Grammatical Gender on Gender Inferences Experimental Evidence From Italian Common Gender.pdf}
}

@article{sagotLefffFreelyAvailable,
  title = {The {{Lefff}}, a Freely Available and Large-Coverage Morphological and Syntactic Lexicon for {{French}}},
  author = {Sagot, Beno{\^i}t},
  pages = {9},
  abstract = {In this paper, we introduce the Lefff , a freely available, accurate and large-coverage morphological and syntactic lexicon for French, used in many NLP tools such as large-coverage parsers. We first describe Alexina, the lexical framework in which the Lefff is developed as well as the linguistic notions and formalisms it is based on. Next, we describe the various sources of lexical data we used for building the Lefff , in particular semi-automatic lexical development techniques and conversion and merging of existing resources. Finally, we illustrate the coverage and precision of the resource by comparing it with other resources and by assessing its impact in various NLP tools.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\QYLFCAH3\The Lefff, a freely available and large-coverage morphological and syntactic lexicon for French (Sagot, ).pdf}
}

@article{sahariaPhotorealisticTexttoImageDiffusion2022,
  title = {Photorealistic {{Text-to-Image Diffusion Models}} with {{Deep Language Understanding}}},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  month = may,
  pages = {25},
  urldate = {2022-08-25},
  abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and imagetext alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, GLIDE and DALL-E 2, and find that human raters prefer Imagen over other models in side-byside comparisons, both in terms of sample quality and image-text alignment. See imagen.research.google for an overview of the results.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\G7V6E2G4\Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (Saharia et al., 2022).pdf}
}

@inproceedings{salazarMaskedLanguageModel2020,
  title = {Masked {{Language Model Scoring}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Salazar, Julian and Liang, Davis and Nguyen, Toan Q. and Kirchhoff, Katrin},
  year = {2020},
  eprint = {1910.14659},
  pages = {2699--2712},
  doi = {10.18653/v1/2020.acl-main.240},
  urldate = {2023-02-27},
  abstract = {Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an endto-end LibriSpeech model's WER by 30\% relative and adds up to +1.7 BLEU on state-of-theart baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL's unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https: //github.com/awslabs/mlm-scoring.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@inproceedings{sapRiskRacialBias2019,
  title = {The {{Risk}} of {{Racial Bias}} in {{Hate Speech Detection}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A.},
  year = {2019},
  pages = {1668--1678},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1163},
  urldate = {2024-12-17},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\I8M7RYIQ\Sap et al. - 2019 - The Risk of Racial Bias in Hate Speech Detection.pdf}
}

@article{savoldiGenderBiasMachine2021,
  title = {Gender {{Bias}} in {{Machine Translation}}},
  author = {Savoldi, Beatrice and Gaido, Marco and Bentivogli, Luisa and Negri, Matteo and Turchi, Marco},
  year = {2021},
  month = aug,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {845--874},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00401},
  urldate = {2025-01-09},
  abstract = {Abstract             Machine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, processing, and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, studies of gender bias in MT still lack cohesion. This advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\2CMAWLJB\Savoldi et al. - 2021 - Gender Bias in Machine Translation.pdf}
}

@article{schlueterRememberingWillysIntroducing,
  title = {Re-Membering {{Willy}}'s {{Past}}: {{Introducing Postmodern Concerns}} through {{Death}} of a {{Salesman}}},
  author = {Schlueter, June},
  pages = {13},
  langid = {english},
  keywords = {miller},
  file = {C:\Users\spide\Zotero\storage\RAZWEIUX\Re-membering Willy’s Past Introducing Postmodern Concerns through Death of a Salesman (Schlueter, ).pdf}
}

@article{schnedeckerNomsGenerauxDenotant2018,
  title = {{Les noms g{\'e}n{\'e}raux d{\'e}notant l'humain. Introduction et pr{\'e}sentation}},
  author = {Schnedecker, Catherine},
  year = {2018},
  number = {76},
  pages = {9--22},
  langid = {french}
}

@article{schudsonNonBinaryGenderSex2022,
  title = {Non-{{Binary Gender}}/{{Sex Identities}}},
  author = {Schudson, Zach C. and Morgenroth, Thekla},
  year = {2022},
  month = dec,
  journal = {Current Opinion in Psychology},
  volume = {48},
  issn = {2352250X},
  doi = {10.1016/j.copsyc.2022.101499},
  urldate = {2023-01-19},
  abstract = {An increasing number of individuals openly identify as nonbinary (i.e., not exclusively female or male). Accordingly, psychological research on non-binary identities has expanded rapidly. We review key insights from this growing literature, first examining work that has demonstrated links between beliefs about the true nature of gender and/or sex (gender/sex) and feelings toward non-binary people. We also review research on non-binary people's self-concepts, which has shown the inadequacy of binary-focused gender/sex measurement practices for effectively studying non-binary people's lives and has suggested treating gender/sex as multidimensional. Then, we consider scholarship on non-binary people's wellbeing, including work exploring sources of joy and pleasure in nonbinary people's lives (e.g., gender euphoria). Finally, we discuss recent advances in gender-inclusive theories and methods.}
}

@article{sczesnyCanGenderFairLanguage2016,
  title = {Can {{Gender-Fair Language Reduce Gender Stereotyping}} and {{Discrimination}}?},
  author = {Sczesny, Sabine and Formanowicz, Magda and Moser, Franziska},
  year = {2016},
  month = feb,
  journal = {Frontiers in Psychology},
  volume = {7},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.00025},
  urldate = {2024-03-23},
  abstract = {Gender-fair language (GFL) aims at reducing gender stereotyping and discrimination. Two principle strategies have been employed to make languages gender-fair and to treat women and men symmetrically: neutralization and feminization. Neutralization is achieved, for example, by replacing male-masculine forms (policeman) with genderunmarked forms (police officer), whereas feminization relies on the use of feminine forms to make female referents visible (i.e., the applicant... he or she instead of the applicant... he). By integrating research on (1) language structures, (2) language policies, and (3) individual language behavior, we provide a critical review of how GFL contributes to the reduction of gender stereotyping and discrimination. Our review provides a basis for future research and for scientifically based policy-making.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\3G29BLP2\Can Gender-Fair Language Reduce Gender Stereotyping and Discrimination (Sczesny et al., 2016).pdf}
}

@misc{sellamBLEURTLearningRobust2020,
  title = {{{BLEURT}}: {{Learning Robust Metrics}} for {{Text Generation}}},
  shorttitle = {{{BLEURT}}},
  author = {Sellam, Thibault and Das, Dipanjan and Parikh, Ankur P.},
  year = {2020},
  month = may,
  eprint = {2004.04696},
  urldate = {2024-03-18},
  abstract = {Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgments. We propose BLEURT, a learned evaluation metric based on BERT that can model human judgments with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-ofthe-art results on the last three years of the WMT Metrics shared task and the WebNLG Competition dataset. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2004.04696},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{sennrichControllingPolitenessNeural2016,
  title = {Controlling {{Politeness}} in {{Neural Machine Translation}} via {{Side Constraints}}},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  year = {2016},
  pages = {35--40},
  publisher = {Association for Computational Linguistics},
  address = {San Diego, California},
  doi = {10.18653/v1/N16-1005},
  urldate = {2022-12-07},
  abstract = {Many languages use honorifics to express politeness, social distance, or the relative social status between the speaker and their addressee(s). In machine translation from a language without honorifics such as English, it is difficult to predict the appropriate honorific, but users may want to control the level of politeness in the output. In this paper, we perform a pilot study to control honorifics in neural machine translation (NMT) via side constraints, focusing on English{$\rightarrow$}German. We show that by marking up the (English) source side of the training data with a feature that encodes the use of honorifics on the (German) target side, we can control the honorifics produced at test time. Experiments show that the choice of honorifics has a big impact on translation quality as measured by BLEU, and oracle experiments show that substantial improvements are possible by constraining the translation to the desired level of politeness.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\44ETSPYR\Controlling Politeness in Neural Machine Translation via Side Constraints (Sennrich et al., 2016).pdf}
}

@article{sewellAcuteEffectsTHC2013,
  title = {Acute Effects of {{THC}} on Time Perception in Frequent and Infrequent Cannabis Users},
  author = {Sewell, R. Andrew and Schnakenberg, Ashley and Elander, Jacqueline and Radhakrishnan, Rajiv and Williams, Ashley and Skosnik, Patrick D. and Pittman, Brian and Ranganathan, Mohini and D'Souza, D. Cyril},
  year = {2013},
  month = mar,
  journal = {Psychopharmacology},
  volume = {226},
  number = {2},
  pages = {401--413},
  issn = {0033-3158, 1432-2072},
  doi = {10.1007/s00213-012-2915-6},
  urldate = {2022-11-05},
  abstract = {Methods IV THC was administered at doses from 0.015 to 0.05 mg/kg to 44 subjects who participated in several double-blind, randomized, counterbalanced, crossover, placebo-controlled studies. Visual time estimation and production tasks in the seconds range were presented to subjects three times on each test day. Results All doses induced time overestimation and underproduction. Chronic cannabis use had no effect on baseline time perception. While infrequent/nonsmokers showed temporal overestimation at medium and high doses and temporal underproduction at all doses, frequent cannabis users showed no differences. THC effects on time perception were not dose related. Conclusions A psychoactive dose of THC increases internal clock speed as indicated by time overestimation and underproduction. This effect is not dose related and is blunted in chronic cannabis smokers who did not otherwise have altered baseline time perception.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\XYZ73Z6N\Acute effects of THC on time perception in frequent and infrequent cannabis users (Sewell et al., 2013).pdf}
}

@misc{seyeditabariEmotionDetectionText2018,
  title = {Emotion {{Detection}} in {{Text}}: A {{Review}}},
  shorttitle = {Emotion {{Detection}} in {{Text}}},
  author = {Seyeditabari, Armin and Tabari, Narges and Zadrozny, Wlodek},
  year = {2018},
  month = jun,
  number = {arXiv:1806.00674},
  eprint = {1806.00674},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1806.00674},
  urldate = {2024-12-17},
  abstract = {In recent years, emotion detection in text has become more popular due to its vast potential applications in marketing, political science, psychology, human-computer interaction, artificial intelligence, etc. Access to a huge amount of textual data, especially opinionated and self-expression text also played a special role to bring attention to this field. In this paper, we review the work that has been done in identifying emotion expressions in text and argue that although many techniques, methodologies, and models have been created to detect emotion in text, there are various reasons that make these methods insufficient. Although, there is an essential need to improve the design and architecture of current systems, factors such as the complexity of human emotions, and the use of implicit and metaphorical language in expressing it, lead us to think that just re-purposing standard methodologies will not be enough to capture these complexities, and it is important to pay attention to the linguistic intricacies of emotion expression.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\ZVDQMVGH\\Seyeditabari et al. - 2018 - Emotion Detection in Text a Review.pdf;C\:\\Users\\spide\\Zotero\\storage\\6V96SH6F\\1806.html}
}

@article{shcherbakovaEvolutionaryPathwaysComplexity2023,
  title = {Evolutionary Pathways of Complexity in Gender Systems},
  author = {Shcherbakova, Olena and {Allassonni{\`e}re-Tang}, Marc},
  year = {2023},
  month = dec,
  journal = {Journal of Language Evolution},
  volume = {8},
  number = {2},
  pages = {120--133},
  issn = {2058-4571, 2058-458X},
  doi = {10.1093/jole/lzae001},
  urldate = {2024-11-21},
  abstract = {Abstract             Humans categorize the experience they encounter in various ways, which is mirrored, for instance, in grammatical gender systems of languages. In such systems, nouns are grouped based on whether they refer to masculine/feminine beings, (non-)humans, (in)animate entities, or objects with specific shapes. Languages differ greatly in how many gender assignment rules are incorporated in gender systems and how many word classes carry gender marking (gender agreement patterns). It has been suggested that these two dimensions are positively associated as numerous assignment rules are better sustained by numerous agreement patterns. We test this claim by analyzing the correlated evolution (Continuous method in BayesTraits) and making the causal inferences about the relationships (phylogenetic path analysis) between these 2 dimensions in 482 languages from the global Grambank database. By applying these methods to linguistic data matched to phylogenetic trees (a world tree and individual families), we evaluate whether various types of gender assignment rules (semantic, phonological, and unpredictable) are causally linked to more gender agreement patterns on the global level and in individual language families. Our results on the world language tree suggest that semantic rules are weakly positively correlated with gender agreement and that the development of agreement patterns is facilitated by different rules in individual families. For example, in Indo-European languages, more agreement patterns are caused by the presence of phonological and unpredictable rules, while in Bantu languages, the driving force of agreement patterns is the variety of semantic rules. Our study shows that the relationships between agreement and rules are family-specific and yields support to the idea that more distinct rules and/or rule types might be more robust in languages with more pervasive gender agreement.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {read},
  file = {C:\Users\spide\Zotero\storage\QGTSFQQZ\Shcherbakova and Allassonnière-Tang - 2023 - Evolutionary pathways of complexity in gender systems.pdf}
}

@misc{shuRewriteLMInstructionTunedLarge2023,
  title = {{{RewriteLM}}: {{An Instruction-Tuned Large Language Model}} for {{Text Rewriting}}},
  shorttitle = {{{RewriteLM}}},
  author = {Shu, Lei and Luo, Liangchen and Hoskere, Jayakumar and Zhu, Yun and Liu, Yinxiao and Tong, Simon and Chen, Jindong and Meng, Lei},
  year = {2023},
  month = dec,
  number = {arXiv:2305.15685},
  eprint = {2305.15685},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-14},
  abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities in creative tasks such as storytelling and E-mail generation. However, as LLMs are primarily trained on final text results rather than intermediate revisions, it might be challenging for them to perform text rewriting tasks. Most studies in the rewriting tasks focus on a particular transformation type within the boundaries of single sentences. In this work, we develop new strategies for instruction tuning and reinforcement learning to better align LLMs for cross-sentence rewriting tasks using diverse wording and structures expressed through natural languages including 1) generating rewriting instruction data from Wiki edits and public corpus through instruction generation and chain-of-thought prompting; 2) collecting comparison data for reward model training through a new ranking function. To facilitate this research, we introduce OPENREWRITEEVAL, a novel benchmark covers a wide variety of rewriting types expressed through natural language instructions. Our results show significant improvements over a variety of baselines. The public repository is available on GitHub under Google Research1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\spide\Zotero\storage\GDJ7TGCX\Shu et al. - 2023 - RewriteLM An Instruction-Tuned Large Language Mod.pdf}
}

@book{siemundPronominalGenderEnglish2002,
  title = {Pronominal {{Gender}} in {{English}}: {{A Study}} of {{English Varieties}} from a {{Cross-Linguistic Perspective}}},
  shorttitle = {Pronominal {{Gender}} in {{English}}},
  author = {Siemund, Peter},
  year = {2002},
  month = jun,
  edition = {1},
  publisher = {Routledge},
  doi = {10.4324/9780203455944},
  urldate = {2024-12-02},
  isbn = {978-0-203-45594-4},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\53DZNNJ3\Siemund - 2002 - Pronominal Gender in English A Study of English Varieties from a Cross-Linguistic Perspective.pdf}
}

@article{SignificanceRace,
  title = {Significance of {{Race}}},
  pages = {21},
  keywords = {civi cult. hist.},
  annotation = {CIVI CUL. HIST.},
  file = {C:\Users\spide\Zotero\storage\4MD4DTQH\Lears Signifiance of Race.pdf}
}

@article{silveiraGenericMasculineWords1980,
  title = {Generic Masculine Words and Thinking},
  author = {Silveira, Jeanette},
  year = {1980},
  month = jan,
  journal = {Women's Studies International Quarterly},
  volume = {3},
  number = {2-3},
  pages = {165--178},
  issn = {01480685},
  doi = {10.1016/S0148-0685(80)92113-2},
  urldate = {2025-01-07},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@misc{siPromptingGPT3Be2023,
  title = {Prompting {{GPT-3 To Be Reliable}}},
  author = {Si, Chenglei and Gan, Zhe and Yang, Zhengyuan and Wang, Shuohang and Wang, Jianfeng and {Boyd-Graber}, Jordan and Wang, Lijuan},
  year = {2023},
  month = feb,
  eprint = {2210.09150},
  urldate = {2024-03-18},
  abstract = {Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions.1 Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2210.09150},
  keywords = {Computer Science - Computation and Language}
}

@article{sistermarybettinaholecekFunctionsDramaticStructure1960,
  title = {The {{Functions}} of {{Dramatic Structure}} in {{Death}} of a {{Salesman}}},
  author = {{Sister Mary Bettina Holecek}},
  year = {1960},
  pages = {59},
  keywords = {miller},
  file = {C:\Users\spide\Zotero\storage\AC55HVZ5\The Functions of Dramatic Structure in Death of a Salesman (Sister Mary Bettina Holecek, 1960).pdf}
}

@article{smith-rosenbergPubertyMenopauseCycle2020,
  title = {Puberty to {{Menopause}}: {{The Cycle}} of {{Femininity}} in {{Nineteenth-Century America}}},
  author = {{Smith-Rosenberg}, Carroll},
  year = {2020},
  pages = {16},
  langid = {english},
  keywords = {civi. women},
  annotation = {WOMEN},
  file = {C:\Users\spide\Zotero\storage\NRDQ9V6B\Puberty to Menopause The Cycle of Femininity in Nineteenth-Century America (Smith-Rosenberg, 2020).pdf}
}

@inproceedings{sobhaniFairerNLPModels2024,
  title = {Towards {{Fairer NLP Models}}: {{Handling Gender Bias In Classification Tasks}}},
  shorttitle = {Towards {{Fairer NLP Models}}},
  booktitle = {Proceedings of the 5th {{Workshop}} on {{Gender Bias}} in {{Natural Language Processing}} ({{GeBNLP}})},
  author = {Sobhani, Nasim and Delany, Sarah},
  year = {2024},
  pages = {167--178},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.gebnlp-1.10},
  urldate = {2025-01-09},
  langid = {english}
}

@article{spinelliNeutralNotFair2023,
  title = {Neutral Is Not Fair Enough: Testing the Efficiency of Different Language Gender-Fair Strategies},
  shorttitle = {Neutral Is Not Fair Enough},
  author = {Spinelli, Elsa and Chevrot, Jean-Pierre and Varnet, L{\'e}o},
  year = {2023},
  month = sep,
  journal = {Frontiers in Psychology},
  volume = {14},
  pages = {1256779},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2023.1256779},
  urldate = {2024-12-17},
  abstract = {In many languages with grammatical gender, the use of masculine forms as a generic reference has been associated with a bias favoring masculine-specific representations. This article examines the efficiency of gender-fair forms, specifically gender-unmarked forms (neutralization strategy, e.g., ``l'enfant'') and contracted double forms (re-feminization strategy, e.g., ``un{$\cdot$}e enfant''), in reducing gender biases in language. Extensive empirical research has shown that gender-fair forms have the potential to promote more gender-balanced representations. However, the relative efficiency of these strategies remains a subject of debate in the scientific literature. In order to explore these questions, two experiments were conducted in French. We analyzed the response times and percent correct scores using a sentence evaluation paradigm, where the participants had to decide whether a second sentence starting with a gendered personal pronoun (``il'' or ``elle'') was a sensible continuation of the first sentence written in a gender-fair form. Experiment 1 confirmed that gender-unmarked forms are not fully effective in neutralizing the masculine bias. In Experiment 2, a comparison was made between gender-unmarked forms and contracted double forms, to assess their respective abilities to generate more balanced representations. The findings indicated that contracted double forms are more effective in promoting gender balance compared to gender-unmarked forms. This study contributes to the existing scientific literature by shedding light on the relative efficiency of neutralization and re-feminization strategies in reducing gender biases in language. These results have implications for informing efforts to promote more inclusive and unbiased language practices.},
  file = {C:\Users\spide\Zotero\storage\BGC8RMYI\Spinelli et al. - 2023 - Neutral is not fair enough testing the efficiency of different language gender-fair strategies.pdf}
}

@book{spolskySociolinguistics1998,
  title = {Sociolinguistics},
  author = {Spolsky, Bernard},
  year = {1998},
  publisher = {Oxford University Press},
  address = {Oxford},
  abstract = {Sociolinguistics is the study of the different ways in which various groups of people use language. This book provides a brief yet comprehensive introduction to the field. It explores how sociolinguistics is linked to other disciplines such as history, politics and gender studies.},
  isbn = {‎ 978-0194372114}
}

@article{stahlbergNameYourFavorite2001,
  title = {Name {{Your Favorite Musician}}: {{Effects}} of {{Masculine Generics}} and of Their {{Alternatives}} in {{German}}},
  shorttitle = {Name {{Your Favorite Musician}}},
  author = {Stahlberg, Dagmar and Sczesny, Sabine and Braun, Friederike},
  year = {2001},
  month = dec,
  journal = {Journal of Language and Social Psychology},
  volume = {20},
  number = {4},
  pages = {464--469},
  issn = {0261-927X, 1552-6526},
  doi = {10.1177/0261927X01020004004},
  urldate = {2023-03-25},
  abstract = {This article reports on two experiments waith native speakers of German that were conducted to determine the influence of different types of German generics on the cognitive inclusion of women. The results of these studies show that masculine versus other types of generics influence the retrieval of male and female exemplars from memory. This is the first piece of empirical evidence for this kind of effect with regard to the German language.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\EQETURKG\Name Your Favorite Musician Effects of Masculine Generics and of their Alternatives in German (Stahlberg et al., 2001).pdf}
}

@misc{stanczakSurveyGenderBias2021,
  title = {A {{Survey}} on {{Gender Bias}} in {{Natural Language Processing}}},
  author = {Stanczak, Karolina and Augenstein, Isabelle},
  year = {2021},
  month = dec,
  eprint = {2112.14168},
  urldate = {2023-03-25},
  abstract = {Language can be used as a means of reproducing and enforcing harmful stereotypes and biases and has been analysed as such in numerous research. In this paper, we present a survey of 304 papers on gender bias in natural language processing. We analyse definitions of gender and its categories within social sciences and connect them to formal definitions of gender bias in NLP research. We survey lexica and datasets applied in research on gender bias and then compare and contrast approaches to detecting and mitigating gender bias. We find that research on gender bias suffers from four core limitations. 1) Most research treats gender as a binary variable neglecting its fluidity and continuity. 2) Most of the work has been conducted in monolingual setups for English or other high-resource languages. 3) Despite a myriad of papers on gender bias in NLP methods, we find that most of the newly developed algorithms do not test their models for bias and disregard possible ethical considerations of their work. 4) Finally, methodologies developed in this line of research are fundamentally flawed covering very limited definitions of gender bias and lacking evaluation baselines and pipelines. We see overcoming these limitations as a necessary development in future research. CCS Concepts: {$\bullet$} Computing methodologies {$\rightarrow$} Natural language processing; Machine Learning; {$\bullet$} Computing methodologies {$\rightarrow$} Language resources.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2112.14168},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society}
}

@article{StereotypingIndians,
  title = {Stereotyping {{Indians}}},
  pages = {9},
  keywords = {civi cult. hist.},
  annotation = {CIVI CUL. HIST.},
  file = {C:\Users\spide\Zotero\storage\6ASW3HI2\Stereotyping Indians.pdf}
}

@article{stormeEffectsGrammaticalGender2024,
  title = {Effects of Grammatical Gender on Gender Inferences: {{Evidence}} from {{French}} Hybrid Nouns},
  shorttitle = {Effects of Grammatical Gender on Gender Inferences},
  author = {Storme, Benjamin and Delaloye Saillen, Laura},
  year = {2024},
  month = mar,
  journal = {Linguistics Vanguard},
  volume = {0},
  number = {0},
  issn = {2199-174X},
  doi = {10.1515/lingvan-2022-0064},
  urldate = {2024-08-24},
  abstract = {A growing body of research shows that readers/listeners are biased by the grammatical gender of a noun when making inferences about the gender of its referent. This result is central in debates about gender-fair language but has mostly been established using masculine generics. This paper presents two preregistered studies on French that aim to replicate this result but using a lesser-studied type of nouns: generic hybrid nouns. These nouns can refer to both male and female individuals but are either masculine or feminine, depending on the noun (e.g. un talent `a talent' and une vedette `a star'). The availability of both genders for hybrid nouns allows for a more comprehensive test of the effect of grammatical gender than permitted by masculine generics. Overall, the paper replicates the role of grammatical biases in gender inferences, with masculine hybrid nouns being judged as more likely to refer to male individuals as compared to feminine hybrid nouns. However the results did not reveal a symmetric bias for feminine nouns, which were interpreted as gender-neutral. But this latter result should be interpreted with caution as it could be due to uncontrolled effects of gender stereotypes coming from the specific stimuli used in the study.},
  copyright = {http://creativecommons.org/licenses/by/4.0},
  langid = {english},
  annotation = {French!},
  file = {C:\Users\spide\Zotero\storage\DXW5Q2GG\Effects of grammatical gender on gender inferences Evidence from French hybrid nouns (Storme and Delaloye Saillen, 2024).pdf}
}

@inproceedings{stosicBaseDonneesNHUMA2013,
  title = {{La base de donn{\'e}es NHUMA: avanc{\'e}es, probl{\`e}mes et perspectives}},
  booktitle = {{Journ{\'e}es d'{\'e}tude NHUMA 4}},
  author = {Stosic, Dejan and Lagae, V{\'e}ronique},
  year = {2013},
  month = mar,
  address = {Strasbourg, France},
  langid = {french}
}

@article{strkaljdespotHowLanguageInfluences2021,
  title = {How {{Language Influences Conceptualization}}: {{From Whorfianism}} to {{Neo-Whorfianism}}},
  shorttitle = {How {{Language Influences Conceptualization}}},
  author = {{\v S}trkalj Despot, Kristina},
  year = {2021},
  journal = {Collegium antropologicum},
  volume = {45},
  number = {4},
  pages = {373--380},
  issn = {18489486, 03506134},
  doi = {10.5671/ca.45.4.9},
  urldate = {2023-03-25},
  abstract = {Do speakers of different languages think alike because of the universality of the experience of being human or do we all think differently because of differences in our languages? The answer to these questions has changed throughout the history of linguistic thought, ranging from observing languages merely as tools for expressing our thoughts to strongly believing that languages shape and even constrain our thoughts. This paper presents an overview of two most important theories that deal with these questions: the ``rise and fall'' of linguistic determinism (Whorfianism), and the development of its more cautious version -- linguistic relativism (Neo-Whorfianism) -- advocated today primarily within the framework of cognitive views of language, as well as their criticisms, most commonly within the framework of generative views of language.}
}

@article{summerfieldbaldwinAEstheticTheoryEdgar2021,
  title = {The {{{\AE}sthetic Theory}} of {{Edgar Poe}}},
  author = {{Summerfield Baldwin}},
  year = {2021},
  pages = {13},
  langid = {english},
  keywords = {poe},
  file = {C:\Users\spide\Zotero\storage\QKVUXABB\The Æsthetic Theory of Edgar Poe (Summerfield Baldwin, 2021).pdf}
}

@misc{sunAreWeAIGenerated2024,
  title = {Are {{We}} in the {{AI-Generated Text World Already}}? {{Quantifying}} and {{Monitoring AIGT}} on {{Social Media}}},
  shorttitle = {Are {{We}} in the {{AI-Generated Text World Already}}?},
  author = {Sun, Zhen and Zhang, Zongmin and Shen, Xinyue and Zhang, Ziyi and Liu, Yule and Backes, Michael and Zhang, Yang and He, Xinlei},
  year = {2024},
  month = dec,
  number = {arXiv:2412.18148},
  eprint = {2412.18148},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.18148},
  urldate = {2025-01-23},
  abstract = {Social media platforms are experiencing a growing presence of AI-Generated Texts (AIGTs). However, the misuse of AIGTs could have profound implications for public opinion, such as spreading misinformation and manipulating narratives. Despite its importance, a systematic study to assess the prevalence of AIGTs on social media is still lacking. To address this gap, this paper aims to quantify, monitor, and analyze the AIGTs on online social media platforms. We first collect a dataset (SM-D) with around 2.4M posts from 3 major social media platforms: Medium, Quora, and Reddit. Then, we construct a diverse dataset (AIGTBench) to train and evaluate AIGT detectors. AIGTBench combines popular open-source datasets and our AIGT datasets generated from social media texts by 12 LLMs, serving as a benchmark for evaluating mainstream detectors. With this setup, we identify the best-performing detector (OSM-Det). We then apply OSM-Det to SM-D to track AIGTs over time and observe different trends of AI Attribution Rate (AAR) across social media platforms from January 2022 to October 2024. Specifically, Medium and Quora exhibit marked increases in AAR, rising from 1.77\% to 37.03\% and 2.06\% to 38.95\%, respectively. In contrast, Reddit shows slower growth, with AAR increasing from 1.31\% to 2.45\% over the same period. Our further analysis indicates that AIGTs differ from human-written texts across several dimensions, including linguistic patterns, topic distributions, engagement levels, and the follower distribution of authors. We envision our analysis and findings on AIGTs in social media can shed light on future research in this domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Social and Information Networks},
  file = {C\:\\Users\\spide\\Zotero\\storage\\TNLRFCLD\\Sun et al. - 2024 - Are We in the AI-Generated Text World Already Quantifying and Monitoring AIGT on Social Media.pdf;C\:\\Users\\spide\\Zotero\\storage\\5MVE76AF\\2412.html}
}

@article{sunMitigatingGenderBias2019,
  title = {Mitigating {{Gender Bias}} in {{Natural Language Processing}}: {{Literature Review}}},
  shorttitle = {Mitigating {{Gender Bias}} in {{Natural Language Processing}}},
  author = {Sun, Tony and Gaut, Andrew and Tang, Shirlyn and Huang, Yuxin and ElSherief, Mai and Zhao, Jieyu and Mirza, Diba and Belding, Elizabeth and Chang, Kai-Wei and Wang, William Yang},
  year = {2019},
  journal = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages = {10},
  doi = {10.18653/v1/P19-1159},
  urldate = {2022-08-10},
  abstract = {As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\GYJFJGZE\Mitigating Gender Bias in Natural Language Processing Literature Review (Sun et al., 2019).pdf}
}

@misc{sunTheyThemTheirs2021,
  title = {They, {{Them}}, {{Theirs}}: {{Rewriting}} with {{Gender-Neutral English}}},
  shorttitle = {They, {{Them}}, {{Theirs}}},
  author = {Sun, Tony and Webster, Kellie and Shah, Apu and Wang, William Yang and Johnson, Melvin},
  year = {2021},
  month = feb,
  number = {arXiv:2102.06788},
  eprint = {2102.06788},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-10-16},
  abstract = {Responsible development of technology involves applications being inclusive of the diverse set of users they hope to support. An important part of this is understanding the many ways to refer to a person and being able to fluently change between the different forms as needed. We perform a case study on the singular they, a common way to promote gender inclusion in English. We define a rewriting task, create an evaluation benchmark, and show how a model can be trained to produce gender-neutral English with {$<$}1\% word error rate with no human-labeled data. We discuss the practical applications and ethical considerations of the task, providing direction for future work into inclusive natural language systems.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\JG2WIICC\They, Them, Theirs Rewriting with Gender-Neutral English (Sun et al., 2021).pdf}
}

@misc{sunTheyThemTheirs2021a,
  title = {They, {{Them}}, {{Theirs}}: {{Rewriting}} with {{Gender-Neutral English}}},
  shorttitle = {They, {{Them}}, {{Theirs}}},
  author = {Sun, Tony and Webster, Kellie and Shah, Apu and Wang, William Yang and Johnson, Melvin},
  year = {2021},
  eprint = {2102.06788},
  doi = {10.48550/arXiv.2102.06788},
  urldate = {2022-10-16},
  abstract = {Responsible development of technology involves applications being inclusive of the diverse set of users they hope to support. An important part of this is understanding the many ways to refer to a person and being able to fluently change between the different forms as needed. We perform a case study on the singular they, a common way to promote gender inclusion in English. We define a rewriting task, create an evaluation benchmark, and show how a model can be trained to produce gender-neutral English with {$<$}1\% word error rate with no human-labeled data. We discuss the practical applications and ethical considerations of the task, providing direction for future work into inclusive natural language systems.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2102.06788},
  keywords = {gender rewriting,lu}
}

@book{susolopezFrancaisFaceAux2004,
  title = {{Le fran{\c c}ais face aux d{\'e}fis actuels: histoire, langue et culture}},
  shorttitle = {{Le fran{\c c}ais face aux d{\'e}fis actuels}},
  editor = {Suso L{\'o}pez, Javier and L{\'o}pez Carillo, Rodrigo},
  year = {2004},
  publisher = {Universidad de Granada},
  address = {Granada},
  isbn = {978-84-338-3237-5},
  langid = {french},
  annotation = {OCLC: 470227013},
  file = {C:\Users\spide\Zotero\storage\FQC8VK3S\Suso López and López Carillo - 2004 - Le français face aux défis actuels histoire, langue et culture.pdf}
}

@misc{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  month = dec,
  eprint = {1409.3215},
  urldate = {2023-02-27},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/1409.3215},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{sutskeverSequenceSequenceLearning2014a,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  month = dec,
  eprint = {1409.3215},
  urldate = {2024-03-18},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/1409.3215},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{svartengrenFeminineGenderInanimate1927,
  title = {The {{Feminine Gender}} for {{Inanimate Things}} in {{Anglo-American}}},
  author = {Svartengren, T. Hilding},
  year = {1927},
  month = dec,
  journal = {American Speech},
  volume = {3},
  number = {2},
  eprint = {451510},
  eprinttype = {jstor},
  issn = {00031283},
  doi = {10.2307/451510},
  urldate = {2023-01-19},
  jstor = {451510}
}

@misc{tangScienceDetectingLLMGenerated2023,
  title = {The {{Science}} of {{Detecting LLM-Generated Texts}}},
  author = {Tang, Ruixiang and Chuang, Yu-Neng and Hu, Xia},
  year = {2023},
  month = jun,
  number = {arXiv:2303.07205},
  eprint = {2303.07205},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.07205},
  urldate = {2025-01-09},
  abstract = {The emergence of large language models (LLMs) has resulted in the production of LLM-generated texts that is highly sophisticated and almost indistinguishable from texts written by humans. However, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system. Although many detection approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. This survey aims to provide an overview of existing LLM-generated text detection techniques and enhance the control and regulation of language generation models. Furthermore, we emphasize crucial considerations for future research, including the development of comprehensive evaluation metrics and the threat posed by open-source LLMs, to drive progress in the area of LLM-generated text detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\HK3V9JVH\\Tang et al. - 2023 - The Science of Detecting LLM-Generated Texts.pdf;C\:\\Users\\spide\\Zotero\\storage\\5UQBYSXM\\2303.html}
}

@article{tianRoleLargeLanguage2024,
  title = {The Role of Large Language Models in Medical Image Processing: A Narrative Review},
  shorttitle = {The Role of Large Language Models in Medical Image Processing},
  author = {Tian, Dianzhe and Jiang, Shitao and Zhang, Lei and Lu, Xin and Xu, Yiyao},
  year = {2024},
  month = jan,
  journal = {Quantitative Imaging in Medicine and Surgery},
  volume = {14},
  number = {1},
  pages = {1108--1121},
  issn = {2223-4292},
  doi = {10.21037/qims-23-892},
  abstract = {BACKGROUND AND OBJECTIVE: The rapid advancement of artificial intelligence (AI) has ushered in a new era in natural language processing (NLP), with large language models (LLMs) like ChatGPT leading the way. This paper explores the profound impact of AI, particularly LLMs, in the field of medical image processing. The objective is to provide insights into the transformative potential of AI in improving healthcare by addressing historical challenges associated with manual image interpretation. METHODS: A comprehensive literature search was conducted on the Web of Science and PubMed databases from 2013 to 2023, focusing on the transformations of LLMs in Medical Imaging Processing. Recent publications on the arXiv database were also reviewed. Our search criteria included all types of articles, including abstracts, review articles, letters, and editorials. The language of publications was restricted to English to facilitate further content analysis. KEY CONTENT AND FINDINGS: The review reveals that AI, driven by LLMs, has revolutionized medical image processing by streamlining the interpretation process, traditionally characterized by time-intensive manual efforts. AI's impact on medical care quality and patient well-being is substantial. With their robust interactivity and multimodal learning capabilities, LLMs offer immense potential for enhancing various aspects of medical image processing. Additionally, the Transformer architecture, foundational to LLMs, is gaining prominence in this domain. CONCLUSIONS: In conclusion, this review underscores the pivotal role of AI, especially LLMs, in advancing medical image processing. These technologies have the capacity to enhance transfer learning efficiency, integrate multimodal data, facilitate clinical interactivity, and optimize cost-efficiency in healthcare. The potential applications of LLMs in clinical settings are promising, with far-reaching implications for future research, clinical practice, and healthcare policy. The transformative impact of AI in medical image processing is undeniable, and its continued development and implementation are poised to reshape the healthcare landscape for the better.},
  langid = {english},
  pmcid = {PMC10784029},
  pmid = {38223123},
  keywords = {artificial intelligence (AI),Large language models (LLMs),medical image processing},
  file = {C:\Users\spide\Zotero\storage\TZVK6G4I\Tian et al. - 2024 - The role of large language models in medical image processing a narrative review.pdf}
}

@misc{tokpoFairFlowAutomatedApproach2024,
  title = {{{FairFlow}}: {{An Automated Approach}} to {{Model-based Counterfactual Data Augmentation For NLP}}},
  shorttitle = {{{FairFlow}}},
  author = {Tokpo, Ewoenam Kwaku and Calders, Toon},
  year = {2024},
  month = jul,
  number = {arXiv:2407.16431},
  eprint = {2407.16431},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-25},
  abstract = {Despite the evolution of language models, they continue to portray harmful societal biases and stereotypes inadvertently learned from training data. These inherent biases often result in detrimental effects in various applications. Counterfactual Data Augmentation (CDA), which seeks to balance demographic attributes in training data, has been a widely adopted approach to mitigate bias in natural language processing. However, many existing CDA approaches rely on word substitution techniques using manually compiled word-pair dictionaries. These techniques often lead to out-of-context substitutions, resulting in potential quality issues. The advancement of model-based techniques, on the other hand, has been challenged by the need for parallel training data. Works in this area resort to manually generated parallel data that are expensive to collect and are consequently limited in scale. This paper proposes FairFlow, an automated approach to generating parallel data for training counterfactual text generator models that limits the need for human intervention. Furthermore, we show that FairFlow significantly overcomes the limitations of dictionary-based word-substitution approaches whilst maintaining good performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\spide\Zotero\storage\JHXLR5Z8\Tokpo and Calders - 2024 - FairFlow An Automated Approach to Model-based Cou.pdf}
}

@inproceedings{tokpoTextStyleTransfer2022,
  title = {Text {{Style Transfer}} for {{Bias Mitigation}} Using {{Masked Language Modeling}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}: {{Student Research Workshop}}},
  author = {Tokpo, Ewoenam Kwaku and Calders, Toon},
  year = {2022},
  pages = {163--171},
  publisher = {Association for Computational Linguistics},
  address = {Hybrid: Seattle, Washington + Online},
  doi = {10.18653/v1/2022.naacl-srw.21},
  urldate = {2024-09-25},
  abstract = {It is well known that textual data on the internet and other digital platforms contain significant levels of bias and stereotypes. Various research findings have concluded that biased texts have significant effects on target demographic groups. For instance, masculine-worded job advertisements tend to be less appealing to female applicants. In this paper, we present a text-style transfer model that can be trained on non-parallel data and be used to automatically mitigate bias in textual data. Our style transfer model improves on the limitations of many existing text style transfer techniques such as the loss of content information. Our model solves such issues by combining latent content encoding with explicit keyword replacement. We will show that this technique produces better content preservation whilst maintaining good style transfer accuracy.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\AUAJT4GT\Tokpo and Calders - 2022 - Text Style Transfer for Bias Mitigation using Mask.pdf}
}

@misc{touvronLLaMAOpenEfficient2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  eprint = {2302.13971},
  urldate = {2023-03-25},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community1.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2302.13971},
  keywords = {Computer Science - Computation and Language}
}

@article{trubowitzChinaCardPlaying,
  title = {The {{China Card}}: {{Playing Politics}} with {{Sino-American Relations}}},
  author = {Trubowitz, Peter and Seo, Jungkun},
  pages = {24},
  langid = {english},
  keywords = {civi for. pol.},
  annotation = {CIVI. FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\7BQGB5JW\The China Card Playing Politics with Sino-American Relations (Trubowitz and Seo, ).pdf}
}

@book{trudgillMillenniaLanguageChange2020,
  title = {Millennia of {{Language Change}}: {{Sociolinguistic Studies}} in {{Deep Historical Linguistics}}},
  shorttitle = {Millennia of {{Language Change}}},
  author = {Trudgill, Peter},
  year = {2020},
  month = apr,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781108769754},
  urldate = {2024-12-01},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-1-108-76975-4 978-1-108-47739-0 978-1-108-70864-7},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\GMMIQ9LC\Trudgill - 2020 - Millennia of Language Change Sociolinguistic Studies in Deep Historical Linguistics.pdf}
}

@article{umera-okekeLinguisticSexismOverview2012,
  title = {Linguistic {{Sexism}}: {{An Overview}} of the {{English Language}} in {{Everyday Discourse}}},
  author = {{Umera-Okeke}, Nneka},
  year = {2012},
  journal = {An International Journal of Language, Literature and Gender Studies Bahir Dar, Ethiopia},
  volume = {1},
  number = {1},
  langid = {english}
}

@misc{unescoircaiChallengingSystematicPrejudices2024,
  title = {Challenging Systematic Prejudices: An {{Investigation}} into {{Gender Bias}} in {{Large Language Models}}},
  author = {{UNESCO IRCAI}},
  year = {2024},
  langid = {english}
}

@incollection{unterbeckGenderNewLight2000,
  title = {Gender: {{New}} Light on an Old Category. {{An}} Introduction},
  shorttitle = {Gender},
  booktitle = {Gender in {{Grammar}} and {{Cognition}}},
  author = {Unterbeck, Barbara},
  editor = {Unterbeck, Barbara and Rissanen, Matti and Nevalainen, Terttu and Saari, Mirja},
  year = {2000},
  month = dec,
  pages = {XV-XLVI},
  publisher = {DE GRUYTER MOUTON},
  doi = {10.1515/9783110802603.xv},
  urldate = {2024-12-02},
  isbn = {978-3-11-016241-7},
  file = {C:\Users\spide\Zotero\storage\HQ8CV9F6\Unterbreck and Unterbeck - 2000 - Gender New light on an old category. An introduction.pdf}
}

@article{UseSingularThey,
  title = {The {{Use}} of {{Singular They}} vs. {{Gendered Pronouns}} ({{Thesis}})},
  pages = {66}
}

@misc{vanmassenhoveGenderBiasMachine2024,
  title = {Gender {{Bias}} in {{Machine Translation}} and {{The Era}} of {{Large Language Models}}},
  author = {Vanmassenhove, Eva},
  year = {2024},
  month = jan,
  number = {arXiv:2401.10016},
  eprint = {2401.10016},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.10016},
  urldate = {2025-01-09},
  abstract = {This chapter examines the role of Machine Translation in perpetuating gender bias, highlighting the challenges posed by cross-linguistic settings and statistical dependencies. A comprehensive overview of relevant existing work related to gender bias in both conventional Neural Machine Translation approaches and Generative Pretrained Transformer models employed as Machine Translation systems is provided. Through an experiment using ChatGPT (based on GPT-3.5) in an English-Italian translation context, we further assess ChatGPT's current capacity to address gender bias. The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {C\:\\Users\\spide\\Zotero\\storage\\T4NZPFGW\\Vanmassenhove - 2024 - Gender Bias in Machine Translation and The Era of Large Language Models.pdf;C\:\\Users\\spide\\Zotero\\storage\\ET5QR3TY\\2401.html}
}

@misc{vanmassenhoveGenderBiasMachine2024a,
  title = {Gender {{Bias}} in {{Machine Translation}} and {{The Era}} of {{Large Language Models}}},
  author = {Vanmassenhove, Eva},
  year = {2024},
  month = jan,
  number = {arXiv:2401.10016},
  eprint = {2401.10016},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.10016},
  urldate = {2025-01-23},
  abstract = {This chapter examines the role of Machine Translation in perpetuating gender bias, highlighting the challenges posed by cross-linguistic settings and statistical dependencies. A comprehensive overview of relevant existing work related to gender bias in both conventional Neural Machine Translation approaches and Generative Pretrained Transformer models employed as Machine Translation systems is provided. Through an experiment using ChatGPT (based on GPT-3.5) in an English-Italian translation context, we further assess ChatGPT's current capacity to address gender bias. The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {C\:\\Users\\spide\\Zotero\\storage\\YSQ2LJT8\\Vanmassenhove - 2024 - Gender Bias in Machine Translation and The Era of Large Language Models.pdf;C\:\\Users\\spide\\Zotero\\storage\\JZ63SJZV\\2401.html}
}

@misc{vanmassenhoveNeuTralRewriterRuleBased2021,
  title = {{{NeuTral Rewriter}}: {{A Rule-Based}} and {{Neural Approach}} to {{Automatic Rewriting}} into {{Gender-Neutral Alternatives}}},
  shorttitle = {{{NeuTral Rewriter}}},
  author = {Vanmassenhove, Eva and Emmery, Chris and Shterionov, Dimitar},
  year = {2021},
  eprint = {2109.06105},
  doi = {10.48550/arXiv.2109.06105},
  urldate = {2022-10-16},
  abstract = {Recent years have seen an increasing need for gender-neutral and inclusive language. Within the field of NLP, there are various mono- and bilingual use cases where gender inclusive language is appropriate, if not preferred due to ambiguity or uncertainty in terms of the gender of referents. In this work, we present a rulebased and a neural approach to gender-neutral rewriting for English along with manually curated synthetic data (WinoBias+) and natural data (OpenSubtitles and Reddit) benchmarks. A detailed manual and automatic evaluation highlights how our NeuTral Rewriter, trained on data generated by the rule-based approach, obtains word error rates (WER) below 0.18\% on synthetic, in-domain and out-domain test sets.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2109.06105},
  keywords = {gender rewriting,lu}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  booktitle = {31st {{Conference}} on {{Neural Information Processing Systems}} ({{NIPS}} 2017},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  address = {Long Beach, CA, USA},
  urldate = {2023-02-27},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.}
}

@inproceedings{velosoRewritingApproachGender2023,
  title = {A {{Rewriting Approach}} for {{Gender Inclusivity}} in {{Portuguese}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Veloso, Leonor and Coheur, Luisa and Ribeiro, Rui},
  year = {2023},
  pages = {8747--8759},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.585},
  urldate = {2024-03-18},
  abstract = {In recent years, there has been a notable rise in research interest regarding the integration of gender-inclusive and gender-neutral language in natural language processing models. A specific area of focus that has gained practical and academic significant interest is gender-neutral rewriting, which involves converting binarygendered text to its gender-neutral counterpart. However, current approaches to gender-neutral rewriting for gendered languages tend to rely on large datasets, which may not be an option for languages with fewer resources, such as Portuguese. In this paper, we present a rule-based and a neural-based tool for genderneutral rewriting for Portuguese, a heavily gendered Romance language whose morphology creates different challenges from the ones tackled by other gender-neutral rewriters. Our neural approach relies on fine-tuning large multilingual machine translation models on examples generated by the rule-based model. We evaluate both models on texts from different sources and contexts. We provide the first Portuguese dataset explicitly containing gender-neutral language and neopronouns, as well as a manually annotated golden collection of 500 sentences that allows for evaluation of future work.}
}

@article{veltmanSimoneBeauvoirHannah2010,
  title = {Simone de {{Beauvoir}} and {{Hannah Arendt}} on {{Labor}}},
  author = {VELTMAN, {\relax ANDREA}},
  year = {2010},
  journal = {Hypatia},
  volume = {25},
  number = {1},
  eprint = {40602640},
  eprinttype = {jstor},
  pages = {55--78},
  publisher = {[Hypatia, Inc., Wiley]},
  issn = {0887-5367},
  urldate = {2022-01-07},
  abstract = {Comparing the typologies of human activities developed by Beauvoir and Arendt, I argue that these philosophers share the same concept of labor as well as a similar insight that labor cannot provide a justification or evaluative measure for human life. But Beauvoir and Arendt think differently about work (as contrasted with labor), and Arendt alone illuminates the inability of constructive work to provide non-utilitarian value for human existence. Beauvoir, on the other hand, exceeds Arendt in examining the ethical implications of our existential need for a plurality of free peers in a public realm.}
}

@article{veronTwittaInfluenceuseIntellectuelle2021,
  title = {{<< Twitta >>, << influenceuse >>, << intellectuelle >>, << communicante >> ? {\^E}tre enseignante-chercheuse sur Twitter}},
  shorttitle = {{<< Twitta >>, << influenceuse >>, << intellectuelle >>, << communicante >> ?}},
  author = {V{\'e}ron, La{\'e}lia},
  year = {2021},
  month = dec,
  journal = {Trac{\'e}s},
  number = {\#21},
  pages = {21},
  issn = {1763-0061, 1963-1812},
  doi = {10.4000/traces.13173},
  urldate = {2022-07-15},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\CDLMJGKB\« Twitta », « influenceuse », « intellectuelle », « communicante »  Être enseignante-chercheuse sur Twitter (Véron, 2021).pdf}
}

@article{viennotDecryptageCirculaireBlanquer2021,
  title = {D{\'e}cryptage : Circulaire {{Blanquer}} Contre l'{\'e}criture Dite Inclusive},
  author = {Viennot, {\'E}liane},
  year = {2021},
  journal = {Cahiers du genre},
  volume = {70},
  number = {1},
  pages = {199--214},
  doi = {10.3917/cdge.070.0199},
  keywords = {ecriture inclusive,lu}
}

@book{viennotLangageInclusifPourquoi2018,
  title = {Le Langage Inclusif : Pourquoi ? Comment ?},
  author = {Viennot, {\'E}liane},
  year = {2018},
  publisher = {{\'E}ditions iXe},
  address = {Donnemarie-Dontilly},
  urldate = {2022-11-23},
  abstract = {La violente pol{\'e}mique surgie en France {\`a} l'automne 2017 autour de << l'{\'e}criture inclusive >> a conduit {\'E}liane Viennot {\`a} {\'e}largir la question au << langage inclusif >>. L'autrice de Non, le masculin ne l'emporte pas sur le f{\'e}minin ! expose dans ce petit guide les bonnes raisons de d{\'e}barrasser la langue des normes et des r{\`e}gles masculinistes pour dire et {\'e}crire un monde o{\`u} chacun{$\cdot$}e aurait sa place, {\`a} {\'e}galit{\'e}. Les outils qui le permettent existent: il suffit de les appliquer pour red{\'e}couvrir, en toute simplicit{\'e}, les logiques du fran{\c c}ais, avec l'inventivit{\'e} que permet aussi sa souplesse. << On ne nait pas femme, on la devient. >> Telle est la phrase que Simone de Beauvoir aurait {\'e}crite si, fille de l'{\'e}cole, elle n'avait assimil{\'e} les r{\`e}gles concoct{\'e}es depuis le XVIIe{\textbackslash},si{\`e}cle pour donner au <<{\textbackslash},genre le plus noble{\textbackslash},>> la place qu'il occupe aujourd'hui dans la langue fran{\c c}aise. Contest{\'e}e d{\`e}s l'origine, longtemps n{\'e}glig{\'e}e, finalement impos{\'e}e par des institutions puissantes, cette entreprise a commenc{\'e} d'{\^e}tre d{\'e}mantel{\'e}e dans les pays francophones depuis une quarantaine d'ann{\'e}es. D'une controverse {\`a} l'autre -- et elles sont particuli{\`e}rement vives en France -- la d{\'e}masculinisation du fran{\c c}ais a d{\'e}j{\`a} fait des progr{\`e}s notables, avec l'abandon progressif des noms masculins appliqu{\'e}s aux femmes occupant des fonctions prestigieuses. Ce travail se poursuit d{\'e}sormais plus largement avec le langage inclusif, qui int{\`e}gre des exigences propres au temps pr{\'e}sent : celles de pays r{\'e}solument d{\'e}cid{\'e}s {\`a} r{\'e}aliser l'{\'e}galit{\'e} entre tous les {\^e}tres humains. Ce guide donne {\`a} la fois les bonnes raisons que nous avons d'approfondir cet effort, et les moyens simples qui sont {\`a} notre port{\'e}e pour le soutenir.},
  isbn = {979-10-90062-50-4},
  keywords = {ecriture inclusive,lu}
}

@book{viennotNonMasculinNe2014,
  title = {Non, Le Masculin Ne l'emporte Pas Sur Le F{\'e}minin ! {{Petite}} Histoire Des R{\'e}sistances de La Langue Fran{\c c}aise},
  shorttitle = {Non, Le Masculin Ne l'emporte Pas Sur Le F{\'e}minin !},
  author = {Viennot, {\'E}liane},
  year = {2014},
  publisher = {{\'E}ditions iXe},
  urldate = {2022-12-04},
  abstract = {Le long effort des grammairiens et des acad{\'e}miciens pour masculiniser le fran{\c c}ais a suscit{\'e} de vives r{\'e}sistances chez celles et ceux qui, longtemps, ont parl{\'e} et {\'e}crit cette langue sans appliquer des r{\`e}gles contraires {\`a} sa logique. <<{\textbackslash},Le genre masculin ne sera plus regard{\'e}, m{\^e}me dans la grammaire, comme le genre le plus noble, attendu que tous les genres, tous les sexes et tous les {\^e}tres doivent {\^e}tre et sont {\'e}galement nobles.{\textbackslash},>> Requ{\^e}te des dames {\`a} l'Assembl{\'e}e nationale, article 3 du Projet de d{\'e}cret adress{\'e} {\`a} la L{\'e}gislative, 1792. La domination du genre masculin sur le genre f{\'e}minin initi{\'e}e au xviie si{\`e}cle ne s'est en effet impos{\'e}e qu'{\`a} la fin du xixe avec l'instruction obligatoire. Depuis, des g{\'e}n{\'e}rations d'{\'e}coli{\`e}res et d'{\'e}coliers r{\'e}p{\`e}tent inlassablement que <<{\textbackslash},le masculin l'emporte sur le f{\'e}minin{\textbackslash},>>, se pr{\'e}parant ainsi {\`a} occuper des places diff{\'e}rentes et hi{\'e}rarchis{\'e}es dans la soci{\'e}t{\'e}. Ce livre retrace l'histoire d'une entreprise {\`a} la misogynie affirm{\'e}e ou honteuse, selon les {\'e}poques. Riche en exemples et en citations il convie {\`a} un parcours plein de surprises o{\`u} l'on en apprend de belles sur la <<{\textbackslash},virilisation{\textbackslash},>> des noms de m{\'e}tier, sur les usages qui pr{\'e}valaient en mati{\`e}re d'accords, sur l'utilisation des pronoms ou sur les op{\'e}rations <<{\textbackslash},transgenre{\textbackslash},>> subies par certains mots.},
  isbn = {979-10-90062-20-7},
  keywords = {ecriture inclusive,lu}
}

@article{viennotParentheseAuPoint2022,
  title = {De La Parenth{\`e}se Au Point M{\'e}dian. {{Des}} Nouveaux Habits de l'{\'e}criture Inclusive et de La Malhonn{\^e}tet{\'e} de Ses Opposant{$\cdot$}es},
  author = {Viennot, {\'E}liane},
  year = {2022},
  journal = {Travail, genre et soci{\'e}t{\'e}s},
  volume = {47},
  number = {1},
  pages = {165--168},
  issn = {1294-6303},
  doi = {10.3917/tgs.047.0165},
  urldate = {2022-11-03},
  keywords = {ecriture inclusive,lu}
}

@article{villoingflorenceMotsComposesVNN2002,
  title = {{Les mots compos{\'e}s [VN]N/A du fran{\c c}ais~: r{\'e}flexions {\'e}pist{\'e}mologiques et propositions d'analyse (chap. 7 et 8)}},
  author = {{Villoing, Florence}},
  year = {2002},
  pages = {110},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\QX86G75S\Villoing 2002 thèse sur les mots composés.pdf}
}

@article{violiOriginesGenreGrammatical1987,
  title = {{Les origines du genre grammatical}},
  author = {Violi, Patrizia},
  year = {1987},
  journal = {Langages},
  volume = {21},
  number = {85},
  pages = {15--34},
  issn = {0458-726X},
  doi = {10.3406/lgge.1987.1526},
  urldate = {2024-11-21},
  langid = {french},
  keywords = {read},
  file = {C:\Users\spide\Zotero\storage\JDZ6SSBW\Violi - 1987 - Les origines du genre grammatical.pdf}
}

@misc{wahdeConversationalAgentsTheory2022,
  title = {Conversational {{Agents}}: {{Theory}} and {{Applications}}},
  shorttitle = {Conversational {{Agents}}},
  author = {Wahde, Mattias and Virgolin, Marco},
  year = {2022},
  month = feb,
  eprint = {2202.03164},
  primaryclass = {cs},
  doi = {10.1142/9789811246050_0012},
  urldate = {2024-12-17},
  abstract = {In this chapter, we provide a review of conversational agents (CAs), discussing chatbots, intended for casual conversation with a user, as well as task-oriented agents that generally engage in discussions intended to reach one or several specific goals, often (but not always) within a specific domain. We also consider the concept of embodied conversational agents, briefly reviewing aspects such as character animation and speech processing. The many different approaches for representing dialogue in CAs are discussed in some detail, along with methods for evaluating such agents, emphasizing the important topics of accountability and interpretability. A brief historical overview is given, followed by an extensive overview of various applications, especially in the fields of health and education. We end the chapter by discussing benefits and potential risks regarding the societal impact of current and future CA technology.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\85NSHNKF\\Wahde and Virgolin - 2022 - Conversational Agents Theory and Applications.pdf;C\:\\Users\\spide\\Zotero\\storage\\TJ5FMHXF\\2202.html}
}

@misc{wandb,
  title = {Experiment Tracking with Weights and Biases},
  author = {Biewald, Lukas},
  year = {2020}
}

@inproceedings{wangAdversarialTextGeneration2021,
  title = {Adversarial {{Text Generation}} for {{Personality Privacy Protection}}},
  booktitle = {2021 4th {{International Conference}} on {{Data Science}} and {{Information Technology}}},
  author = {Wang, Zhe and Zheng, Kangfeng and Li, Qingbiao and Wang, Maonan and Wang, Xiujuan},
  year = {2021},
  month = jul,
  pages = {159--165},
  publisher = {ACM},
  address = {Shanghai China},
  doi = {10.1145/3478905.3478937},
  urldate = {2024-07-12},
  abstract = {Protecting the user's personality privacy can effectively interfere with or deceive the attacker's personality analysis, avoid the attacker's use of personality vulnerability, and reduce the success rate of social engineering attacks. However, the current research on personality privacy protection is at a blank stage. To solve this problem, we propose a personality privacy protection method based on adversarial text generation. This paper mainly uses gradient-based adversarial method and cosine similarity to generation adversarial text. We formed a set of replacement words to test the impact of the number of replacement words on the performance of the model. Experiments show that the method proposed in this paper has achieved good effects on model attacks (reducing the performance of the model), and can well complete the task of protecting personality privacy.},
  isbn = {978-1-4503-9024-8},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\VHSNSZ8I\Wang et al. - 2021 - Adversarial Text Generation for Personality Privac.pdf}
}

@misc{wanWhiteMenLead2024,
  title = {White {{Men Lead}}, {{Black Women Help}}? {{Benchmarking Language Agency Social Biases}} in {{LLMs}}},
  shorttitle = {White {{Men Lead}}, {{Black Women Help}}?},
  author = {Wan, Yixin and Chang, Kai-Wei},
  year = {2024},
  month = oct,
  number = {arXiv:2404.10508},
  eprint = {2404.10508},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.10508},
  urldate = {2024-12-17},
  abstract = {Social biases can manifest in language agency. While several studies approached agency-related bias in human-written language, very limited research has investigated such biases in Large Language Model (LLM)-generated content. In addition, previous works often rely on string-matching techniques to identify agentic and communal words within texts, which fall short of accurately classifying language agency. We introduce the novel Language Agency Bias Evaluation (LABE) benchmark, which comprehensively evaluates biases in LLMs by analyzing agency levels attributed to different demographic groups in model generations. LABE leverages 5,400 template-based prompts, an accurate agency classifier, and corresponding bias metrics to test for gender, racial, and intersectional language agency biases in LLMs on 3 text generation tasks: biographies, professor reviews, and reference letters. We also contribute the Language Agency Classification (LAC) dataset, consisting of 3,724 agentic and communal sentences. Using LABE, we unveil language agency social biases in 3 recent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations tend to demonstrate greater gender bias than human-written texts; (2) Models demonstrate remarkably higher levels of intersectional bias than the other bias aspects. Those who are at the intersection of gender and racial minority groups--such as Black females--are consistently described by texts with lower levels of agency, aligning with real-world social inequalities; (3) Among the 3 LLMs investigated, Llama3 demonstrates the greatest overall bias; (4) Not only does prompt-based mitigation fail to resolve language agency bias in LLMs, but it frequently leads to the exacerbation of biases in generated texts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {C\:\\Users\\spide\\Zotero\\storage\\UWIM65B8\\Wan and Chang - 2024 - White Men Lead, Black Women Help Benchmarking Language Agency Social Biases in LLMs.pdf;C\:\\Users\\spide\\Zotero\\storage\\G3F882G9\\2404.html}
}

@article{warburtonLocalizingCovidPhone2021,
  title = {Localizing a {{Covid}} Phone App: {{A}} University Class Experience},
  shorttitle = {Localizing a {{Covid}} Phone App},
  author = {Warburton, Kara and Krishnan, Kamya Bharthi},
  year = {2021},
  month = dec,
  journal = {The Journal of Internationalization and Localization},
  volume = {8},
  number = {2},
  pages = {29},
  issn = {2032-6904, 2032-6912},
  doi = {10.1075/jial.21007.war},
  urldate = {2022-07-24},
  abstract = {Abstract             In this paper we describe how a COVID-19 phone app was localized by students of the MA program in Translation and 					Interpreting at the University of Illinois at Urbana-Champaign. The project presented unique challenges including the urgency and 					short time-lines, less-than-optimal internationalization of the source code, limitations of the CAT tool, and the use of an 					open-source platform and crowd-sourced agile development model, in addition to particular linguistic aspects related to the new 					specialized subject matter, limited string context, and potentially sensitive data. On the other hand, it offered the opportunity 					to experience a real-life localization project, which had important pedagogical benefits for translation students. These and other 					aspects are described and suggestions proposed to enable similar projects to run smoothly.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\SSD7KXYZ\Localizing a Covid phone app A university class experience (Warburton and Krishnan, 2021).pdf}
}

@article{watbledLinguistiqueGenre2012,
  title = {Linguistique Du Genre},
  author = {Watbled, Jean-Philippe},
  year = {2012},
  journal = {L'Harmattan},
  series = {Genre et Dynamiques Interculturelles : La Transmission},
  pages = {167--179},
  issn = {978-2-336- 00373-3},
  urldate = {2023-01-19}
}

@incollection{weberFunctionGender2000,
  title = {On the Function of Gender},
  booktitle = {Gender in {{Grammar}} and {{Cognition}}},
  author = {Weber, Doris},
  editor = {Unterbeck, Barbara and Rissanen, Matti and Nevalainen, Terttu and Saari, Mirja},
  year = {2000},
  month = dec,
  pages = {495--510},
  publisher = {DE GRUYTER MOUTON},
  doi = {10.1515/9783110802603.495},
  urldate = {2024-12-13},
  isbn = {978-3-11-016241-7},
  file = {C:\Users\spide\Zotero\storage\8DHKALKQ\Weber - 2000 - On the function of gender.pdf}
}

@phdthesis{weigelDefisPosesPar2021,
  title = {D{\'e}fis Pos{\'e}s Par l'{\'e}criture Inclusive Au Traitement Automatique Du Fran{\c c}ais : Le Cas de La Traduction Automatique Fran{\c c}ais-Anglais},
  author = {Weigel, Sarah and Bernhard, Delphine},
  year = {2021},
  urldate = {2023-02-21},
  school = {Universit{\'e} de Strasbourg}
}

@misc{weiUnderstandingImpactAI2024,
  title = {Understanding the {{Impact}} of {{AI Generated Content}} on {{Social Media}}: {{The Pixiv Case}}},
  shorttitle = {Understanding the {{Impact}} of {{AI Generated Content}} on {{Social Media}}},
  author = {Wei, Yiluo and Tyson, Gareth},
  year = {2024},
  month = feb,
  number = {arXiv:2402.18463},
  eprint = {2402.18463},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.18463},
  urldate = {2025-01-23},
  abstract = {In the last two years, Artificial Intelligence Generated Content (AIGC) has received significant attention, leading to an anecdotal rise in the amount of AIGC being shared via social media platforms. The impact of AIGC and its implications are of key importance to social platforms, e.g., regarding the implementation of policies, community formation, and algorithmic design. Yet, to date, we know little about how the arrival of AIGC has impacted the social media ecosystem. To fill this gap, we present a comprehensive study of Pixiv, an online community for artists who wish to share and receive feedback on their illustrations. Pixiv hosts over 100 million artistic submissions and receives more than 1 billion page views per month (as of 2023). Importantly, it allows both human and AI generated content to be uploaded. Exploiting this, we perform the first analysis of the impact that AIGC has had on the social media ecosystem, through the lens of Pixiv. Based on a dataset of 15.2 million posts (including 2.4 million AI-generated images), we measure the impact of AIGC on the Pixiv community, as well as the differences between AIGC and human-generated content in terms of content creation and consumption patterns. Our results offer key insight to how AIGC is changing the dynamics of social media platforms like Pixiv.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society},
  file = {C\:\\Users\\spide\\Zotero\\storage\\WMTEVKDZ\\Wei and Tyson - 2024 - Understanding the Impact of AI Generated Content on Social Media The Pixiv Case.pdf;C\:\\Users\\spide\\Zotero\\storage\\48CUAJUT\\2402.html}
}

@article{wennerEnclavesCivilityAmidst1997,
  title = {``{{Enclaves}} of Civility amidst Clamorous Impertinence'': {{Will}} as Reflected in the Landscape of {{{\emph{Emma}}}}},
  shorttitle = {``{{Enclaves}} of Civility amidst Clamorous Impertinence''},
  author = {Wenner, Barbara Britton},
  year = {1997},
  month = feb,
  journal = {European Romantic Review},
  volume = {8},
  number = {1},
  pages = {95--115},
  issn = {1050-9585, 1740-4657},
  doi = {10.1080/10509589708570027},
  urldate = {2020-11-24},
  langid = {english},
  keywords = {jane austen},
  file = {C\:\\Users\\spide\\Documents\\PDF recherches\\Rethinking.pdf;C\:\\Users\\spide\\Documents\\PDF recherches\\Rethinking.pdf;C\:\\Users\\spide\\Documents\\PDF recherches\\wenner1997.pdf}
}

@article{whelanGEORGEKENNANHIS1959,
  title = {{{GEORGE KENNAN AND HIS INFLUENCE ON AMERICAN FOREIGN POLICY}}},
  author = {WHELAN, JOSEPH G.},
  year = {1959},
  journal = {The Virginia Quarterly Review},
  volume = {35},
  number = {2},
  eprint = {26441632},
  eprinttype = {jstor},
  pages = {24},
  issn = {0042-675X},
  urldate = {2021-12-16},
  keywords = {civi for. pol.},
  annotation = {CIVI FOR. POL.},
  file = {C:\Users\spide\Zotero\storage\7TDZAJJY\GEORGE KENNAN AND HIS INFLUENCE ON AMERICAN FOREIGN POLICY (WHELAN, 1959).pdf}
}

@article{whorfScienceLinguistics1940,
  title = {Science and {{Linguistics}}},
  author = {Whorf, Benjamin Lee},
  year = {1940},
  journal = {Technology Review},
  number = {6},
  pages = {229--231}
}

@article{WhyUkraineCrisis2022,
  title = {Why the {{Ukraine Crisis Is}} the {{West}}'s {{Fault}}: {{The Liberal Delusions That Provoked Putin}}},
  year = {2022},
  journal = {FOREIGN AFFAIRS},
  pages = {14},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\PMMLRNU9\Why the Ukraine Crisis Is the West's Fault The Liberal Delusions That Provoked Putin (, 2022).pdf}
}

@article{wilhelmSousConduiteDHermes2021,
  title = {{Sous la conduite d'Herm{\`e}s, aux carrefours de la traduction et du f{\'e}minisme}},
  author = {Wilhelm, Jane~Elisabeth},
  year = {2021},
  month = jul,
  journal = {GLAD!},
  number = {10},
  pages = {24},
  issn = {2551-0819},
  doi = {10.4000/glad.2883},
  urldate = {2022-07-24},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\UXBA6QTB\Sous la conduite d’Hermès, aux carrefours de la traduction et du féminisme (Wilhelm, 2021).pdf}
}

@article{wilhelmThreeWordClusters1975,
  title = {Three {{Word Clusters}} in {{Emma}}},
  author = {Wilhelm},
  year = {1975},
  volume = {7},
  pages = {49--60},
  keywords = {jane austen},
  file = {C:\Users\spide\Documents\PDF recherches\Wilhelm - 1975 - Three Word Clusters in Emma.pdf}
}

@article{williamheathMelvilleMarquesanEroticism1988,
  title = {Melville and {{Marquesan Eroticism}}},
  author = {{William Heath}},
  year = 1988,
  journal = {The Massachusetts Review},
  volume = {29},
  pages = {24},
  langid = {english},
  keywords = {melville},
  file = {C:\Users\spide\Zotero\storage\7W9AWFII\(GK014301AW）ARNOLD,Wayne E..pdf}
}

@article{wisniewskiBiaisGenreDans,
  title = {{Biais de genre dans un syst{\`e}me de traduction automatique neuronale : une {\'e}tude pr{\'e}liminaire}},
  author = {Wisniewski, Guillaume and Zhu, Lichao and Ballier, Nicolas and Yvon, Fran{\c c}ois},
  pages = {15},
  abstract = {This paper is a blueprint of a current study in the making on gender bias in French/English neural translation toolkits. We discuss previous research using probes for neural machine translation. We then study a minimal controlled corpus and use it to measure the intensity of such biases in the two translation directions (from and into English). Using a controlled experimental design also enables us to analyze the internal representations (attention matrices) of the translation system, and to formulate hypotheses regarding the way these biases are encoded within these representations. MOTS-CL{\'E}S : biais de genre, traduction automatique neuronale, {\'e}valuation diagnostique en TAL.},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\2ZNVXFFD\Biais de genre dans un système de traduction automatique neuronale  une étude préliminaire (Wisniewski et al., ).pdf}
}

@article{wisniewskiBiaisGenreDans2021,
  title = {{Biais de genre dans un syst{\`e}me de traduction automatique neuronale : une {\'e}tude pr{\'e}liminaire}},
  author = {Wisniewski, Guillaume and Zhu, Lichao and Ballier, Nicolas and Yvon, Fran{\c c}ois},
  year = {2021},
  abstract = {This paper is a blueprint of a current study in the making on gender bias in French/English neural translation toolkits. We discuss previous research using probes for neural machine translation. We then study a minimal controlled corpus and use it to measure the intensity of such biases in the two translation directions (from and into English). Using a controlled experimental design also enables us to analyze the internal representations (attention matrices) of the translation system, and to formulate hypotheses regarding the way these biases are encoded within these representations. MOTS-CL{\'E}S : biais de genre, traduction automatique neuronale, {\'e}valuation diagnostique en TAL.},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\JHYCY3M9\Wisniewski et al. - Biais de genre dans un système de traduction automatique neuronale  une étude préliminaire.pdf}
}

@article{wisniewskiBiaisGenreDans2021a,
  title = {{Biais de genre dans un syst{\`e}me de traduction automatique neuronale : une {\'e}tude pr{\'e}liminaire}},
  author = {Wisniewski, Guillaume and Zhu, Lichao and Ballier, Nicolas and Yvon, Fran{\c c}ois},
  year = {2021},
  journal = {Actes de la 28e Conf{\'e}rence sur le Traitement Automatique des Langues Naturelles},
  volume = {1},
  pages = {11--25},
  abstract = {This paper is a blueprint of a current study in the making on gender bias in French/English neural translation toolkits. We discuss previous research using probes for neural machine translation. We then study a minimal controlled corpus and use it to measure the intensity of such biases in the two translation directions (from and into English). Using a controlled experimental design also enables us to analyze the internal representations (attention matrices) of the translation system, and to formulate hypotheses regarding the way these biases are encoded within these representations. MOTS-CL{\'E}S : biais de genre, traduction automatique neuronale, {\'e}valuation diagnostique en TAL.},
  langid = {french},
  file = {C:\Users\spide\Zotero\storage\BIT2NRJE\Wisniewski et al. - Biais de genre dans un système de traduction automatique neuronale  une étude préliminaire.pdf}
}

@inproceedings{wisniewskiScreeningGenderTransfer2021,
  title = {Screening {{Gender Transfer}} in {{Neural Machine Translation}}},
  booktitle = {Proceedings of the {{Fourth BlackboxNLP Workshop}} on {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Wisniewski, Guillaume and Zhu, Lichao and Ballier, Nicolas and Yvon, Fran{\c c}ois},
  year = {2021},
  eprint = {2202.12568},
  primaryclass = {cs},
  pages = {311--321},
  doi = {10.18653/v1/2021.blackboxnlp-1.24},
  urldate = {2024-12-17},
  abstract = {This paper aims at identifying the information flow in state-of-the-art machine translation systems, taking as example the transfer of gender when translating from French into English. Using a controlled set of examples, we experiment several ways to investigate how gender information circulates in a encoder-decoder architecture considering both probing techniques as well as interventions on the internal representations used in the MT system. Our results show that gender information can be found in all token representations built by the encoder and the decoder and lead us to conclude that there are multiple pathways for gender transfer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\66YUHW7W\\Wisniewski et al. - 2021 - Screening Gender Transfer in Neural Machine Translation.pdf;C\:\\Users\\spide\\Zotero\\storage\\4WUMTRWM\\2202.html}
}

@article{wordenOLIVERCROMWELLPROTECTORATE2010,
  title = {{{OLIVER CROMWELL AND THE PROTECTORATE}}},
  author = {Worden, Blair},
  year = {2010},
  month = dec,
  journal = {Transactions of the Royal Historical Society},
  volume = {20},
  pages = {57--83},
  issn = {0080-4401, 1474-0648},
  doi = {10.1017/S0080440110000058},
  urldate = {2022-04-20},
  abstract = {It is often said that if Oliver Cromwell had lived longer the Puritan Revolution could have survived. The monarchical component of protectoral rule, and the protector's endeavours to broaden the base of his regime, are taken to have signalled a return towards normality and thus towards stability. That mood has been contrasted with the self-destruction of the revolution in the two years after Cromwell's death, a period of twilit anarchy which only the restoration of the Stuarts could end. That interpretation has its points but is misleadingly one-sided. The protectorate had frailties which it never overcame. It failed to live down its origins in the military coups of 1653. Those episodes affronted principles of civilian rule and parliamentary supremacy which commanded widespread support but which have been obscured by the 'revisionist' trend of parliamentary history. Though he aimed at 'healing and settling', the protector healed little and settled nothing. His attempts to woo mainstream opinion were unsuccessful. In so far as he won its compliance or tolerance, the achievement was conditional upon his readiness to submit to the principles of rule which his seizure of power had broken. It was a condition he could not or would not meet. By the end of his life, military obstruction to civilian and parliamentary rule had reduced his regime to paralysis, and had deepened the divisions between civilian and military aspirations that would soon bring down his successor and would destroy each of the fleeting regimes that followed.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\JR3M69M9\OLIVER CROMWELL AND THE PROTECTORATE (Worden, 2010).pdf}
}

@article{wrightLadiesFirstPhonology2005,
  title = {Ladies {{First}}? {{Phonology}}, {{Frequency}}, and the {{Naming Conspiracy}}},
  shorttitle = {Ladies {{First}}?},
  author = {Wright, Saundra K. and Hay, Jennifer and Bent, Tessa},
  year = {2005},
  month = jan,
  journal = {Linguistics},
  volume = {43},
  number = {3},
  issn = {0024-3949, 1613-396X},
  doi = {10.1515/ling.2005.43.3.531},
  urldate = {2023-03-25},
  abstract = {In pairs of names, male names often precede female names (e.g. Romeo and Juliet). We investigate this bias and argue that preferences for name ordering are constrained by a combination of gender, phonology, and frequency. First, various phonological constraints condition the optimal ordering of binomial pairs, and findings from our corpus investigations show that male names contain those features which lend them to be preferred in first position, while female names contain features which lend them to be preferred in second position. Thus, phonology predicts that male names are more likely to precede female names than follow them. Results from our name-ordering experiments provide further evidence that this ``gendered phonology'' plays a role in determining ordering preferences but also that an independent gender bias exists: when phonology is controlled (i.e. when two names are ``phonologically equal''), subjects prefer male names first. Finally, frequency leads to another tendency to place male names first. Further investigation shows that frequent names are ordered before less frequent names and that male names are overall more ``frequent'' than female names. Together, all of these factors conspire toward an overwhelming tendency to place male names before female names.}
}

@inproceedings{xueMT5MassivelyMultilingual2021,
  title = {{{mT5}}: {{A Massively Multilingual Pre-trained Text-to-Text Transformer}}},
  shorttitle = {{{mT5}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and {Al-Rfou}, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  year = {2021},
  pages = {483--498},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.naacl-main.41},
  urldate = {2023-03-25}
}

@article{xuPrivacyAwareTextRewriting2019,
  title = {Privacy-{{Aware Text Rewriting}}},
  author = {Xu, Qiongkai and Qu, Lizhen and Xu, Chenchen and Cui, Ran},
  year = {2019},
  journal = {Association for Computational Linguistics},
  volume = {Proceedings of the 12th International Conference on Natural Language Generation},
  pages = {247--257},
  doi = {10.18653/v1/W19-8633},
  abstract = {Biased decisions made by automatic systems have led to growing concerns in research communities. Recent work from the NLP community focuses on building systems that make fair decisions based on text. Instead of relying on unknown decision systems or human decisionmakers, we argue that a better way to protect data providers is to remove the trails of sensitive information before publishing the data. In light of this, we propose a new privacy-aware text rewriting task and explore two privacyaware back-translation methods for the task, based on adversarial training and approximate fairness risk. Our extensive experiments on three real-world datasets with varying demographical attributes show that our methods are effective in obfuscating sensitive attributes. We have also observed that the fairness risk method retains better semantics and fluency, while the adversarial training method tends to leak less sensitive information.}
}

@inproceedings{xuSESCORE2LearningText2023,
  title = {{{SESCORE2}}: {{Learning Text Generation Evaluation}} via {{Synthesizing Realistic Mistakes}}},
  shorttitle = {{{SESCORE2}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Xu, Wenda and Qian, Xian and Wang, Mingxuan and Li, Lei and Wang, William Yang},
  year = {2023},
  pages = {5166--5183},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.283},
  urldate = {2024-03-18},
  abstract = {Is it possible to train a general metric for evaluating text generation quality without humanannotated ratings? Existing learned metrics either perform unsatisfactorily across text generation tasks or require human ratings for training on specific tasks. In this paper, we propose SESCORE2, a self-supervised approach for training a model-based metric for text generation evaluation. The key concept is to synthesize realistic model mistakes by perturbing sentences retrieved from a corpus. The primary advantage of the SESCORE2 is its ease of extension to many other languages while providing reliable severity estimation. We evaluate SESCORE2 and previous methods on four text generation tasks across three languages. SESCORE2 outperforms unsupervised metric PRISM on four text generation evaluation benchmarks, with a Kendall improvement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised BLEURT and COMET on multiple text generation tasks. The code and data are available at https://github.com/ xu1998hz/SEScore21.}
}

@misc{xuWhatMeantAGI2024,
  title = {What Is {{Meant}} by {{AGI}}? {{On}} the {{Definition}} of {{Artificial General Intelligence}}},
  shorttitle = {What Is {{Meant}} by {{AGI}}?},
  author = {Xu, Bowen},
  year = {2024},
  month = apr,
  number = {arXiv:2404.10731},
  eprint = {2404.10731},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.10731},
  urldate = {2024-12-17},
  abstract = {This paper aims to establish a consensus on AGI's definition. General intelligence refers to the adaptation to open environments according to certain principles using limited resources. It emphasizes that adaptation or learning is an indispensable property of intelligence, and places the controversial part within the principles of intelligence, which can be described from different perspectives.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\spide\\Zotero\\storage\\JLFWWR6Q\\Xu - 2024 - What is Meant by AGI On the Definition of Artificial General Intelligence.pdf;C\:\\Users\\spide\\Zotero\\storage\\8RQMP3SB\\2404.html}
}

@misc{yangHotfixingLargeLanguage2024,
  title = {Hotfixing {{Large Language Models}} for {{Code}}: {{How Far Can Parameter-Efficient Fine-Tuning Go}}?},
  shorttitle = {Hotfixing {{Large Language Models}} for {{Code}}},
  author = {Yang, Zhou and Lo, David},
  year = {2024},
  month = aug,
  number = {arXiv:2408.05727},
  eprint = {2408.05727},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-28},
  abstract = {Large Language Models for Code (LLM4Code) have become an integral part of developers' workflows, assisting with tasks such as code completion and generation. However, these models are found to exhibit undesired behaviors after their release, like generating buggy code, due to their extensive training on vast amounts of source code that contain such buggy code. The training data (usually coming from open-source software) keeps evolving, e.g., developers fix the buggy code. However, adapting such evolution to mitigate LLM4Code's undesired behaviors is non-trivial, as retraining models on the updated dataset usually takes much time and resources. This motivates us to propose the concept of hotfixing LLM4Code, mitigating LLM4Code's undesired behaviors effectively and efficiently with minimal negative effects. This paper mainly focuses on hotfixing LLM4Code to make them generate less buggy code and more fixed code. We begin by demonstrating that models from the popular CodeGen family frequently generate buggy code. Then, we define three learning objectives in hotfixing and design multiple loss functions for each objective: (1) learn the desired behaviors, (2) unlearn the undesired behaviors, and (3) retain knowledge of other code. We evaluate four different fine-tuning techniques for hotfixing the models and gain the following insights. Optimizing these three learning goals together, using LoRA (low-rank adaptation), effectively influences the model's behavior. Specifically, it increases the generation of fixed code by up to 108.42\% and decreases the generation of buggy code by up to 50.47\%. Statistical tests confirm that hotfixing does not significantly affect the models' functional correctness on the HumanEval benchmark. We also show that hotfixing demonstrates strong time efficiency.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {C:\Users\spide\Zotero\storage\7QIS92BK\Yang and Lo - 2024 - Hotfixing Large Language Models for Code How Far .pdf}
}

@misc{yaoLargeLanguageModel2024,
  title = {Large {{Language Model Unlearning}}},
  author = {Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
  year = {2024},
  month = feb,
  number = {arXiv:2310.10683},
  eprint = {2310.10683},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.10683},
  urldate = {2024-12-17},
  abstract = {We study how to perform unlearning, i.e. forgetting undesirable misbehaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2\% of its computational time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\spide\\Zotero\\storage\\73EHHUJM\\Yao et al. - 2024 - Large Language Model Unlearning.pdf;C\:\\Users\\spide\\Zotero\\storage\\DT5WHL82\\2310.html}
}

@misc{yeAdversarialExamplesGeneration2021,
  title = {Adversarial {{Examples Generation}} for {{Reducing Implicit Gender Bias}} in {{Pre-trained Models}}},
  author = {Ye, Wenqian and Xu, Fei and Huang, Yaojia and Huang, Cassie and A, Ji},
  year = {2021},
  month = oct,
  number = {arXiv:2110.01094},
  eprint = {2110.01094},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-12},
  abstract = {Over the last few years, Contextualized Pretrained Neural Language Models, such as BERT, GPT, have shown significant gains in various NLP tasks. To enhance the robustness of existing pre-trained models, one way is adversarial examples generation and evaluation for conducting data augmentation or adversarial learning. In the meanwhile, gender bias embedded in the models seems to be a serious problem in practical applications. Many researches have covered the gender bias produced by word-level information(e.g. gender-stereotypical occupations), while few researchers have investigated the sentence-level cases and implicit cases. In this paper, we proposed a method to automatically generate implicit gender bias samples at sentence-level and a metric to measure gender bias. Samples generated by our method will be evaluated in terms of accuracy. The metric will be used to guide the generation of examples from Pre-trained models. Therefore, those examples could be used to impose attacks on Pre-trained Models. Finally, we discussed the evaluation efficacy of our generated examples on reducing gender bias for future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\spide\Zotero\storage\95BWNTB2\Ye et al. - 2021 - Adversarial Examples Generation for Reducing Impli.pdf}
}

@inproceedings{ylonen-2022-wiktextract,
  title = {Wiktextract: {{Wiktionary}} as Machine-Readable Structured Data},
  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  author = {Ylonen, Tatu},
  editor = {Calzolari, Nicoletta and B{\'e}chet, Fr{\'e}d{\'e}ric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, H{\'e}l{\`e}ne and Odijk, Jan and Piperidis, Stelios},
  year = {2022},
  month = jun,
  pages = {1317--1325},
  publisher = {European Language Resources Association},
  address = {Marseille, France},
  abstract = {We present a machine-readable structured data version of Wiktionary. Unlike previous Wiktionary extractions, the new extractor, Wiktextract, fully interprets and expands templates and Lua modules in Wiktionary. This enables it to perform a more complete, robust, and maintainable extraction. The extracted data is multilingual and includes lemmas, inflected forms, translations, etymology, usage examples, pronunciations (including URLs of sound files), lexical and semantic relations, and various morphological, syntactic, semantic, topical, and dialectal annotations. We extract all data from the English Wiktionary. Comparing against previous extractions from language-specific dictionaries, we find that its coverage for non-English languages often matches or exceeds the coverage in the language-specific editions, with the added benefit that all glosses are in English. The data is freely available and regularly updated, enabling anyone to add more data and correct errors by editing Wiktionary. The extracted data is in JSON format and designed to be easy to use by researchers, downstream resources, and application developers.}
}

@inproceedings{youBinaryGenderLabels2024,
  title = {Beyond {{Binary Gender Labels}}: {{Revealing Gender Bias}} in {{LLMs}} through {{Gender-Neutral Name Predictions}}},
  shorttitle = {Beyond {{Binary Gender Labels}}},
  booktitle = {Proceedings of the 5th {{Workshop}} on {{Gender Bias}} in {{Natural Language Processing}} ({{GeBNLP}})},
  author = {You, Zhiwen and Lee, HaeJin and Mishra, Shubhanshu and Jeoung, Sullam and Mishra, Apratim and Kim, Jinseok and Diesner, Jana},
  year = {2024},
  pages = {255--268},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.gebnlp-1.16},
  urldate = {2025-01-09},
  langid = {english}
}

@misc{zaranisWatchingWatchersExposing2024,
  title = {Watching the {{Watchers}}: {{Exposing Gender Disparities}} in {{Machine Translation Quality Estimation}}},
  shorttitle = {Watching the {{Watchers}}},
  author = {Zaranis, Emmanouil and Attanasio, Giuseppe and Agrawal, Sweta and Martins, Andr{\'e} F. T.},
  year = {2024},
  month = nov,
  number = {arXiv:2410.10995},
  eprint = {2410.10995},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10995},
  urldate = {2025-01-23},
  abstract = {The automatic assessment of translation quality has recently become crucial across several stages of the translation pipeline, from data curation to training and decoding. Although quality estimation (QE) metrics have been optimized to align with human judgments, no attention has been given to these metrics' potential biases, particularly in reinforcing visibility and usability for some demographic groups over others. This study is the first to investigate gender bias in QE metrics and its downstream impact on machine translation (MT). Focusing on out-of-English translations into languages with grammatical gender, we ask: Do contemporary QE metrics exhibit gender bias? Can the use of contextual information mitigate this bias? How does QE influence gender bias in MT outputs? Experiments with state-of-the-art QE metrics across multiple domains, datasets, and languages reveal significant bias. Masculine-inflected translations score higher than feminine-inflected ones, and gender-neutral translations are penalized. Moreover, context-aware QE metrics reduce errors for masculine-inflected references but fail to address feminine referents, exacerbating gender disparities. Additionally, QE metrics can perpetuate gender bias in MT systems when used in quality-aware decoding. Our findings underscore the need to address gender bias in QE metrics to ensure equitable and unbiased MT systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\92AKN9MU\\Zaranis et al. - 2024 - Watching the Watchers Exposing Gender Disparities in Machine Translation Quality Estimation.pdf;C\:\\Users\\spide\\Zotero\\storage\\F5E8EM77\\2410.html}
}

@misc{zhangDistillingTextStyle2024,
  title = {Distilling {{Text Style Transfer With Self-Explanation From LLMs}}},
  author = {Zhang, Chiyu and Cai, Honglong and Yuezhang and Li and Wu, Yuexin and Hou, Le and {Abdul-Mageed}, Muhammad},
  year = {2024},
  month = may,
  number = {arXiv:2403.01106},
  eprint = {2403.01106},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-30},
  abstract = {Text Style Transfer (TST) seeks to alter the style of text while retaining its core content. Given the constraints of limited parallel datasets for TST, we propose CoTeX, a framework that leverages large language models (LLMs) alongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills the complex rewriting and reasoning capabilities of LLMs into more streamlined models capable of working with both non-parallel and parallel data. Through experimentation across four TST datasets, CoTeX is shown to surpass traditional supervised fine-tuning and knowledge distillation methods, particularly in low-resource settings. We conduct a comprehensive evaluation, comparing CoTeX against current unsupervised, supervised, in-context learning (ICL) techniques, and instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering transparent explanations for its style transfer process.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\spide\Zotero\storage\YNMC5BVJ\Zhang et al. - 2024 - Distilling Text Style Transfer With Self-Explanati.pdf}
}

@article{zhangRiskawareArtificialIntelligence2022,
  title = {Towards Risk-Aware Artificial Intelligence and Machine Learning Systems: {{An}} Overview},
  shorttitle = {Towards Risk-Aware Artificial Intelligence and Machine Learning Systems},
  author = {Zhang, Xiaoge and Chan, Felix T.S. and Yan, Chao and Bose, Indranil},
  year = {2022},
  month = aug,
  journal = {Decision Support Systems},
  volume = {159},
  pages = {113800},
  issn = {01679236},
  doi = {10.1016/j.dss.2022.113800},
  urldate = {2024-12-17},
  langid = {english}
}

@misc{zhangSirensSongAI2023,
  title = {Siren's {{Song}} in the {{AI Ocean}}: {{A Survey}} on {{Hallucination}} in {{Large Language Models}}},
  shorttitle = {Siren's {{Song}} in the {{AI Ocean}}},
  author = {Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
  year = {2023},
  month = sep,
  eprint = {2309.01219},
  urldate = {2024-03-18},
  abstract = {While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2309.01219},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@misc{zhaoGenderBiasCoreference2018,
  title = {Gender {{Bias}} in {{Coreference Resolution}}: {{Evaluation}} and {{Debiasing Methods}}},
  shorttitle = {Gender {{Bias}} in {{Coreference Resolution}}},
  author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  year = {2018},
  month = apr,
  number = {arXiv:1804.06876},
  eprint = {1804.06876},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1804.06876},
  urldate = {2024-12-17},
  abstract = {We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are available at http://winobias.org.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\SYBMYCYP\\Zhao et al. - 2018 - Gender Bias in Coreference Resolution Evaluation and Debiasing Methods.pdf;C\:\\Users\\spide\\Zotero\\storage\\25NG5D7R\\1804.html}
}

@misc{zhaoGenderBiasLarge2024,
  title = {Gender {{Bias}} in {{Large Language Models}} across {{Multiple Languages}}},
  author = {Zhao, Jinman and Ding, Yitian and Jia, Chen and Wang, Yining and Qian, Zifan},
  year = {2024},
  month = mar,
  number = {arXiv:2403.00277},
  eprint = {2403.00277},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.00277},
  urldate = {2024-12-17},
  abstract = {With the growing deployment of large language models (LLMs) across various applications, assessing the influence of gender biases embedded in LLMs becomes crucial. The topic of gender bias within the realm of natural language processing (NLP) has gained considerable focus, particularly in the context of English. Nonetheless, the investigation of gender bias in languages other than English is still relatively under-explored and insufficiently analyzed. In this work, We examine gender bias in LLMs-generated outputs for different languages. We use three measurements: 1) gender bias in selecting descriptive words given the gender-related context. 2) gender bias in selecting gender-related pronouns (she/he) given the descriptive words. 3) gender bias in the topics of LLM-generated dialogues. We investigate the outputs of the GPT series of LLMs in various languages using our three measurement methods. Our findings revealed significant gender biases across all the languages we examined.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\RBH2W4ME\\Zhao et al. - 2024 - Gender Bias in Large Language Models across Multiple Languages.pdf;C\:\\Users\\spide\\Zotero\\storage\\IUWLZM4D\\2403.html}
}

@article{zhouEvaluatingMitigatingGender2024,
  title = {Evaluating and {{Mitigating Gender Bias}} in {{Generative Large Language Models}}},
  author = {Zhou, Hanqing and Inkpen, Diana and Kantarci, Burak},
  year = {2024},
  month = nov,
  journal = {INTERNATIONAL JOURNAL OF COMPUTERS  COMMUNICATIONS \& CONTROL},
  volume = {19},
  number = {6},
  issn = {1841-9844, 1841-9836},
  doi = {10.15837/ijccc.2024.6.6853},
  urldate = {2024-11-19},
  abstract = {The examination of gender bias, alongside other demographic biases like race, nationality, and religion, within generative large language models (LLMs), is increasingly capturing the attention of both the scientific community and industry stakeholders. These biases often affect generative LLMs, influencing popular products and potentially compromising user experiences. A growing body of research is dedicated to enhancing gender representations in natural language processing (NLP) across a spectrum of generative LLMs. This paper explores the current research focused on identifying and evaluating gender bias in generative LLMs. A comprehensive investigation is conducted to evaluate and mitigate gender bias across five distinct generative LLMs. The mitigation strategies implemented yield significant improvements in gender bias scores, with performance enhancements of up to 46\% compared to zero-shot text generation approaches. Additionally, we explore how different levels of LLM precision and quantization impact gender bias, providing insights into how technical factors influence bias mitigation strategies. By tackling these challenges and suggesting areas for future research, we aim to contribute to the ongoing discussion about gender bias in language technologies, promoting more equitable and inclusive NLP systems.},
  copyright = {http://creativecommons.org/licenses/by-nc/4.0},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\6VYRE4GJ\Zhou et al. - 2024 - Evaluating and Mitigating Gender Bias in Generative Large Language Models.pdf}
}

@inproceedings{zhouExaminingGenderBias2019,
  title = {Examining Gender Bias in Languages with Grammatical Gender},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing},
  author = {Zhou, Pei and Shi, Weijia and Zhao, Jieyu and Huang, Kuan-Hao and Chen, Muhao and Cotterell, Ryan and Chang, Kai-Wei},
  year = {2019},
  month = nov,
  pages = {5276--5284},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  abstract = {Recent studies have shown that word embeddings exhibit gender bias inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such bias only in English. These analyses cannot be directly extended to languages that exhibit morphological agreement on gender, such as Spanish and French. In this paper, we propose new metrics for evaluating gender bias in word embeddings of these languages and further demonstrate evidence of gender bias in bilingual embeddings which align these languages with English. Finally, we extend an existing approach to mitigate gender bias in word embedding of these languages under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches can effectively reduce the gender bias while preserving the utility of the original embeddings.}
}

@misc{zhuMultilingualMachineTranslation2024,
  title = {Multilingual {{Machine Translation}} with {{Large Language Models}}: {{Empirical Results}} and {{Analysis}}},
  shorttitle = {Multilingual {{Machine Translation}} with {{Large Language Models}}},
  author = {Zhu, Wenhao and Liu, Hongyi and Dong, Qingxiu and Xu, Jingjing and Huang, Shujian and Kong, Lingpeng and Chen, Jiajun and Li, Lei},
  year = {2024},
  month = jun,
  number = {arXiv:2304.04675},
  eprint = {2304.04675},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.04675},
  urldate = {2025-01-09},
  abstract = {Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating massive languages? 2) Which factors affect LLMs' performance in translation? We thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our empirical results show that translation capabilities of LLMs are continually involving. GPT-4 has beat the strong supervised baseline NLLB in 40.91\% of translation directions but still faces a large gap towards the commercial translation system like Google Translate, especially on low-resource languages. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, LLM can acquire translation ability in a resource-efficient way and generate moderate translation even on zero-resource languages. Second, instruction semantics can surprisingly be ignored when given in-context exemplars. Third, cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs. Code will be released at: https://github.com/NJUNLP/MMT-LLM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\EEU3FRNM\\Zhu et al. - 2024 - Multilingual Machine Translation with Large Language Models Empirical Results and Analysis.pdf;C\:\\Users\\spide\\Zotero\\storage\\ZVHK4PPT\\2304.html}
}

@misc{zhuoExploringAIEthics2023,
  title = {Exploring {{AI Ethics}} of {{ChatGPT}}: {{A Diagnostic Analysis}}},
  shorttitle = {Exploring {{AI Ethics}} of {{ChatGPT}}},
  author = {Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Xing, Zhenchang},
  year = {2023},
  month = feb,
  eprint = {2301.12867},
  urldate = {2023-03-25},
  abstract = {Recent breakthroughs in natural language processing (NLP) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. The large language-model (LLM) has significantly impacted businesses such as report summarization softwares and copywriters. Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. Large-scale benchmarks for accountable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical difficulties in advanced LLMs, there is no systematic examination and user study of the ethics of current LLMs use. To further educate future efforts on constructing ethical LLMs responsibly, we perform a qualitative research method on OpenAI's ChatGPT1 to better understand the practical features of ethical dangers in recent LLMs. We analyze ChatGPT comprehensively from four perspectives: 1) Bias 2) Reliability 3) Robustness 4) Toxicity. In accordance with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies. In addition, we examine the implications of our findings on the AI ethics of ChatGPT, as well as future problems and practical design considerations for LLMs. We believe that our findings may give light on future efforts to determine and mitigate the ethical hazards posed by machines in LLM applications.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2301.12867},
  keywords = {Computer Science - Computation and Language,Computer Science - Software Engineering}
}

@article{zimmermanCatalogueSelectedRhetorical2021,
  title = {A {{Catalogue}} of {{Selected Rhetorical Devices Used}} in the {{Works}} of {{Edgar Allan Poe}}},
  author = {Zimmerman, Brett},
  year = {2021},
  pages = {22},
  langid = {english},
  keywords = {poe},
  file = {C:\Users\spide\Zotero\storage\IFJFGQ2C\A Catalogue of Selected Rhetorical Devices Used in the Works of Edgar Allan Poe (Zimmerman, 2021).pdf}
}

@inproceedings{zmigrodCounterfactualDataAugmentation2019,
  title = {Counterfactual {{Data Augmentation}} for {{Mitigating Gender Stereotypes}} in {{Languages}} with {{Rich Morphology}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Zmigrod, Ran and Mielke, Sebastian J. and Wallach, Hanna and Cotterell, Ryan},
  year = {2019},
  pages = {1651--1661},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1161},
  urldate = {2023-02-21}
}

@phdthesis{zotero-198,
  type = {Phdthesis}
}

@misc{zuoContextAwareQueryRewriting2022,
  title = {Context-{{Aware Query Rewriting}} for {{Improving Users}}' {{Search Experience}} on {{E-commerce Websites}}},
  author = {Zuo, Simiao and Yin, Qingyu and Jiang, Haoming and Xi, Shaohui and Yin, Bing and Zhang, Chao and Zhao, Tuo},
  year = {2022},
  month = sep,
  eprint = {2209.07584},
  urldate = {2023-03-19},
  abstract = {E-commerce queries are often short and ambiguous. Consequently, query understanding often uses query rewriting to disambiguate user-input queries. While using e-commerce search tools, users tend to enter multiple searches, which we call context, before purchasing. These history searches contain contextual insights about users' true shopping intents. Therefore, modeling such contextual information is critical to a better query rewriting model. However, existing query rewriting models ignore users' history behaviors and consider only the instant search query, which is often a short string offering limited information about the true shopping intent.},
  archiveprefix = {arXiv},
  howpublished = {http://arxiv.org/abs/2209.07584},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning}
}

@misc{openaiGPT4oMiniAdvancing2024,
  title = {{{GPT-4o}} Mini: Advancing Cost-Efficient Intelligence},
  shorttitle = {{{GPT-4o}} Mini},
  author = {{OpenAI}},
  year = {2024},
  urldate = {2025-01-24},
  abstract = {Introducing the most cost-efficient small model in the market},
  howpublished = {https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/},
  langid = {american},
  file = {C:\Users\spide\Zotero\storage\6ED9D2ZC\gpt-4o-mini-advancing-cost-efficient-intelligence.html}
}

@misc{lardelliGenderFairPostEditingCase2023,
  title = {Gender-{{Fair Post-Editing}}: {{A Case Study Beyond}} the {{Binary}}},
  shorttitle = {Gender-{{Fair Post-Editing}}},
  author = {Lardelli, Manuel and Gromann, Dagmar},
  year = {2023},
  month = may,
  publisher = {Zenodo},
  doi = {10.5281/ZENODO.7898328},
  urldate = {2025-02-12},
  abstract = {Machine Translation (MT) models are well-known to suffer from gender bias, especially for gender beyond a binary conception. Due to the multiplicity of language-specific strategies for gender representation beyond the binary, debiasing MT is extremely challenging. As an alternative, we propose a case study on genderfair post-editing. In this study, six professional translators each post-edited three English to German machine translations. For each translation, participants were instructed to use a different gender-fair language approach, that is, gender-neutral rewording, gender-inclusive characters, and a neosystem. The focus of this study is not on translation quality but rather on the ease of integrating gender-fair language into the post-editing process. Findings from non-participant observation and interviews show clear differences in temporal and cognitive effort between participants and GFL approach as well as in the success of using gender-fair language.},
  archiveprefix = {Zenodo},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  langid = {english},
  keywords = {gender-fair language,machine translation,non-binary,post-editing},
  file = {C:\Users\spide\Zotero\storage\ZXL5U3ZX\Lardelli and Gromann - 2023 - Gender-Fair Post-Editing A Case Study Beyond the Binary.pdf}
}

@article{piergentiliGenderNeutralizationInclusive2023,
  title = {Gender {{Neutralization}} for an {{Inclusive Machine Translation}}: From {{Theoretical Foundations}} to {{Open Challenges}}},
  author = {Piergentili, Andrea and Fucci, Dennis and Savoldi, Beatrice and Negri, Matteo and Bentivogli, Luisa},
  year = {2023},
  abstract = {Gender inclusivity in language technologies has become a prominent research topic. In this study, we explore genderneutral translation (GNT) as a form of gender inclusivity and a goal to be achieved by machine translation (MT) models, which have been found to perpetuate gender bias and discrimination. Specifically, we focus on translation from English into Italian, a language pair representative of salient gender-related linguistic transfer problems. To define GNT, we review a selection of relevant institutional guidelines for gender-inclusive language, discuss its scenarios of use, and examine the technical challenges of performing GNT in MT, concluding with a discussion of potential solutions to encourage advancements toward greater inclusivity in MT.},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\48JH5AWA\Piergentili et al. - Gender Neutralization for an Inclusive Machine Translation from Theoretical Foundations to Open Cha.pdf}
}

@article{tibblinMaleBiasCan2023,
  title = {The Male Bias Can Be Attenuated in Reading: On the Resolution of Anaphoric Expressions Following Gender-Fair Forms in {{French}}},
  shorttitle = {The Male Bias Can Be Attenuated in Reading},
  author = {Tibblin, Julia and Granfeldt, Jonas and Van De Weijer, Joost and Gygax, Pascal},
  year = {2023},
  month = jul,
  journal = {Glossa Psycholinguistics},
  volume = {2},
  number = {1},
  issn = {2767-0279},
  doi = {10.5070/G60111267},
  urldate = {2025-02-12},
  abstract = {Despite the increased use of different types of gender-fair forms in French, studies investigating how they are interpreted when presented in a sentence remain few. To fill this gap, we conducted a pre-registered study using a timed sentence evaluation task to examine the possibility of speakers' establishing an anaphoric relationship between a gendered anaphoric expression (femmes `women' or hommes `men') and non-stereotyped role nouns as antecedents. The antecedents were presented in their masculine form or in one out of three different gender-fair forms (complete double forms: les voisines et voisins `the neighbours.FEM and neighbours.MASC', contracted double forms: les voisin{$\cdot$}es `the neighbours.MASC{$\cdot$}FEM', or gender-neutral forms: le voisinage `the neighbourhood'). In line with previous findings, the masculine form led to a male bias in the participants' mental representations of gender. All three examined gender-fair forms resolved this bias, but comparisons of the different forms to each other revealed differences between them. The results show that complete double forms lead to equally balanced mental representations of gender while contracted double forms slightly favour representation of women. Finally, gender-neutral forms result in a small male bias, although significantly smaller than the one produced by the masculine form. The results are discussed in relation to the mental models theory and provide new and important insights on how gender-fair forms in French are interpreted.\&nbsp;},
  langid = {english},
  file = {C:\Users\spide\Zotero\storage\855GPH8E\Tibblin et al. - 2023 - The male bias can be attenuated in reading on the resolution of anaphoric expressions following gen.pdf}
}

@online{bartlShowgirlsPerformersFinetuning2024,
  title = {From '{{Showgirls}}' to '{{Performers}}': {{Fine-tuning}} with {{Gender-inclusive Language}} for {{Bias Reduction}} in {{LLMs}}},
  shorttitle = {From '{{Showgirls}}' to '{{Performers}}'},
  author = {Bartl, Marion and Leavy, Susan},
  year = {2024},
  eprint = {2407.04434},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2407.04434},
  url = {http://arxiv.org/abs/2407.04434},
  urldate = {2025-02-17},
  abstract = {Gender bias is not only prevalent in Large Language Models (LLMs) and their training data, but also firmly ingrained into the structural aspects of language itself. Therefore, adapting linguistic structures within LLM training data to promote gender-inclusivity can make gender representations within the model more inclusive. The focus of our work are gender-exclusive affixes in English, such as in 'show-girl' or 'man-cave', which can perpetuate gender stereotypes and binary conceptions of gender. We use an LLM training dataset to compile a catalogue of 692 gender-exclusive terms along with gender-neutral variants and from this, develop a gender-inclusive fine-tuning dataset, the 'Tiny Heap'. Fine-tuning three different LLMs with this dataset, we observe an overall reduction in gender-stereotyping tendencies across the models. Our approach provides a practical method for enhancing gender inclusivity in LLM training data and contributes to incorporating queer-feminist linguistic activism in bias mitigation research in NLP.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\M7FVI4YT\\Bartl and Leavy - 2024 - From 'Showgirls' to 'Performers' Fine-tuning with Gender-inclusive Language for Bias Reduction in L.pdf;C\:\\Users\\spide\\Zotero\\storage\\J9JITDJK\\2407.html}
}

@inproceedings{nunziatini-diego-2024-implementing,
    title = "Implementing Gender-Inclusivity in {MT} Output using Automatic Post-Editing with {LLM}s",
    author = "Nunziatini, Mara  and
      Diego, Sara",
    editor = "Scarton, Carolina  and
      Prescott, Charlotte  and
      Bayliss, Chris  and
      Oakley, Chris  and
      Wright, Joanna  and
      Wrigley, Stuart  and
      Song, Xingyi  and
      Gow-Smith, Edward  and
      Bawden, Rachel  and
      S{\'a}nchez-Cartagena, V{\'i}ctor M  and
      Cadwell, Patrick  and
      Lapshinova-Koltunski, Ekaterina  and
      Cabarr{\~a}o, Vera  and
      Chatzitheodorou, Konstantinos  and
      Nurminen, Mary  and
      Kanojia, Diptesh  and
      Moniz, Helena",
    booktitle = "Proceedings of the 25th Annual Conference of the European Association for Machine Translation (Volume 1)",
    month = jun,
    year = "2024",
    address = "Sheffield, UK",
    publisher = "European Association for Machine Translation (EAMT)",
    url = "https://aclanthology.org/2024.eamt-1.48/",
    pages = "580--589",
    abstract = "This paper investigates the effectiveness of combining machine translation (MT) systems and large language models (LLMs) to produce gender-inclusive translations from English to Spanish. The study uses a multi-step approach where a translation is first generated by an MT engine and then reviewed by an LLM. The results suggest that while LLMs, particularly GPT-4, are successful in generating gender-inclusive post-edited translations and show potential in enhancing fluency, they often introduce unnecessary changes and inconsistencies. The findings underscore the continued necessity for human review in the translation process, highlighting the current limitations of AI systems in handling nuanced tasks like gender-inclusive translation. Also, the study highlights that while the combined approach can improve translation fluency, the effectiveness and reliability of the post-edited translations can vary based on the language of the prompts used."
}

@online{jourdanFairTranslateEnglishFrenchDataset2025,
  title = {{{FairTranslate}}: {{An English-French Dataset}} for {{Gender Bias Evaluation}} in {{Machine Translation}} by {{Overcoming Gender Binarity}}},
  shorttitle = {{{FairTranslate}}},
  author = {Jourdan, Fanny and Chevalier, Yannick and Favre, Cécile},
  year = {2025},
  eprint = {2504.15941},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.15941},
  url = {http://arxiv.org/abs/2504.15941},
  urldate = {2025-04-23},
  abstract = {Large Language Models (LLMs) are increasingly leveraged for translation tasks but often fall short when translating inclusive language -- such as texts containing the singular 'they' pronoun or otherwise reflecting fair linguistic protocols. Because these challenges span both computational and societal domains, it is imperative to critically evaluate how well LLMs handle inclusive translation with a well-founded framework. This paper presents FairTranslate, a novel, fully human-annotated dataset designed to evaluate non-binary gender biases in machine translation systems from English to French. FairTranslate includes 2418 English-French sentence pairs related to occupations, annotated with rich metadata such as the stereotypical alignment of the occupation, grammatical gender indicator ambiguity, and the ground-truth gender label (male, female, or inclusive). We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B, Llama3.3-70B) on this dataset under different prompting procedures. Our results reveal substantial biases in gender representation across LLMs, highlighting persistent challenges in achieving equitable outcomes in machine translation. These findings underscore the need for focused strategies and interventions aimed at ensuring fair and inclusive language usage in LLM-based translation systems. We make the FairTranslate dataset publicly available on Hugging Face, and disclose the code for all experiments on GitHub.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\55MWPJHM\\Jourdan et al. - 2025 - FairTranslate An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overco.pdf;C\:\\Users\\spide\\Zotero\\storage\\YW8BE6QQ\\2504.html}
}

@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}

@article{kilgarriff2014sketch,
  title={The Sketch Engine: ten years on},
  author={Kilgarriff, Adam and Baisa, Vít and Bušta, Jan and Jakubíček, Miloš and Kovář, Vojtěch and Michelfeit, Jan and Rychlý, Pavel and Suchomel, Vít},
  journal={Lexicography},
  year={2014},
  volume={1},
  pages={7--36},
  publisher={Springer}
}

@online{doyenManMadeLanguage2025,
  title = {Man {{Made Language Models}}? {{Evaluating LLMs}}' {{Perpetuation}} of {{Masculine Generics Bias}}},
  shorttitle = {Man {{Made Language Models}}?},
  author = {Doyen, Enzo and Todirascu, Amalia},
  date = {2025-02-14},
  eprint = {2502.10577},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.10577},
  url = {http://arxiv.org/abs/2502.10577},
  urldate = {2025-04-11},
  abstract = {Large language models (LLMs) have been shown to propagate and even amplify gender bias, in English and other languages, in specific or constrained contexts. However, no studies so far have focused on gender biases conveyed by LLMs' responses to generic instructions, especially with regard to masculine generics (MG). MG are a linguistic feature found in many gender-marked languages, denoting the use of the masculine gender as a "default" or supposedly neutral gender to refer to mixed group of men and women, or of a person whose gender is irrelevant or unknown. Numerous psycholinguistics studies have shown that MG are not neutral and induce gender bias. This work aims to analyze the use of MG by both proprietary and local LLMs in responses to generic instructions and evaluate their MG bias rate. We focus on French and create a human noun database from existing lexical resources. We filter existing French instruction datasets to retrieve generic instructions and analyze the responses of 6 different LLMs. Overall, we find that \$\textbackslash approx\$39.5\textbackslash\% of LLMs' responses to generic instructions are MG-biased (\$\textbackslash approx\$73.1\textbackslash\% across responses with human nouns). Our findings also reveal that LLMs are reluctant to using gender-fair language spontaneously.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\spide\\Zotero\\storage\\HYFDXGT3\\Doyen and Todirascu - 2025 - Man Made Language Models Evaluating LLMs' Perpetuation of Masculine Generics Bias.pdf;C\:\\Users\\spide\\Zotero\\storage\\FPG9W5M2\\2502.html}
}

@article{pratesAssessingGenderBias2020,
  title = {Assessing Gender Bias in Machine Translation: A Case Study with {{Google Translate}}},
  shorttitle = {Assessing Gender Bias in Machine Translation},
  author = {Prates, Marcelo O. R. and Avelar, Pedro H. and Lamb, Luís C.},
  date = {2020-05},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput \& Applic},
  volume = {32},
  number = {10},
  pages = {6363--6381},
  issn = {0941-0643, 1433-3058},
  doi = {10.1007/s00521-019-04144-6},
  url = {http://link.springer.com/10.1007/s00521-019-04144-6},
  urldate = {2025-06-05},
  langid = {english}
}
